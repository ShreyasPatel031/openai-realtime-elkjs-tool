Cloud,Group,Subgroup,Source,Description,Architecture
gcp,ai-ml,gen-ai,https://cloud.google.com/architecture/ai-ml/generative-ai-knowledge-base,"Generative AI Knowledge Base This solution showcases how to extract question & answer pairs out of documents using Generative AI. It provides an end-to-end demonstration of QA extraction and fine-tuning of a large language model (LLM) on Vertex AI. Along the way, the solution utilizes Document AI Character Recognition (OCR), Firestore, Vector Search, Vertex AI Studio, and Cloud Functions. You can choose whether to deploy your Jump Start Solution through the console directly or download as Terraform on GitHub  to deploy later. Products used in this solution Product name Description Category Cloud Run functions        Event-driven serverless functions        Serverless Cloud Storage        Enterprise-ready object storage        Storage Document AI        Document analysis, classification, and searches        Artificial Intelligence Eventarc        Modern event delivery        Integration Services Firestore        Serverless, JSON & MongoDB compatible database        Databases Vertex AI        One AI platform, every ML tool you need        Artificial Intelligence","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud Platform"" } ], ""children"": [ { ""id"": ""application_layers"", ""labels"": [ { ""text"": ""application_layers"" } ], ""children"": [ { ""id"": ""ai_services"", ""labels"": [ { ""text"": ""ai_services"" } ], ""children"": [ { ""id"": ""document_ai"", ""labels"": [ { ""text"": ""Document AI"" } ], ""children"": [], ""data"": { ""label"": ""Document AI"", ""icon"": ""gcp_document_ai"" } }, { ""id"": ""vertex_search"", ""labels"": [ { ""text"": ""gcp_recommendations_ai"" } ], ""children"": [], ""data"": { ""label"": ""gcp_recommendations_ai"", ""icon"": ""gcp_recommendations_ai"" } }, { ""id"": ""vertex_ai_llm"", ""labels"": [ { ""text"": ""Vertex AI (LLM)"" } ], ""children"": [], ""data"": { ""label"": ""Vertex AI (LLM)"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""vertex_ai_finetune"", ""labels"": [ { ""text"": ""Vertex AI Fine-tune"" } ], ""children"": [], ""data"": { ""label"": ""Vertex AI Fine-tune"", ""icon"": ""gcp_vertexai"" } } ], ""data"": { ""label"": ""ai_services"" }, ""edges"": [ { ""id"": ""e_finetune_llm"", ""sources"": [ ""vertex_ai_finetune"" ], ""targets"": [ ""vertex_ai_llm"" ], ""labels"": [ { ""text"": ""update model"" } ] } ] }, { ""id"": ""data_storage"", ""labels"": [ { ""text"": ""data_storage"" } ], ""children"": [ { ""id"": ""firestore_ds"", ""labels"": [ { ""text"": ""Cloud Firestore"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Firestore"", ""icon"": ""gcp_firestore"" } }, { ""id"": ""storage_tuning"", ""labels"": [ { ""text"": ""Cloud Storage (Tuning)"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Storage (Tuning)"", ""icon"": ""gcp_cloud_storage"" } } ], ""data"": { ""label"": ""data_storage"" } }, { ""id"": ""middle_tier"", ""labels"": [ { ""text"": ""middle_tier"" } ], ""children"": [ { ""id"": ""eventarc_trigger"", ""labels"": [ { ""text"": ""Eventarc Trigger"" } ], ""children"": [], ""data"": { ""label"": ""Eventarc Trigger"", ""icon"": ""gcp_eventarc"" } }, { ""id"": ""cloud_functions"", ""labels"": [ { ""text"": ""Cloud Functions"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Functions"", ""icon"": ""gcp_cloud_functions"" } } ], ""data"": { ""label"": ""middle_tier"" }, ""edges"": [ { ""id"": ""e_eventarc_cf"", ""sources"": [ ""eventarc_trigger"" ], ""targets"": [ ""cloud_functions"" ], ""labels"": [ { ""text"": ""event"" } ] } ] }, { ""id"": ""frontend"", ""labels"": [ { ""text"": ""frontend"" } ], ""children"": [ { ""id"": ""storage_docs"", ""labels"": [ { ""text"": ""Cloud Storage (Docs)"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Storage (Docs)"", ""icon"": ""gcp_cloud_storage"" } } ], ""data"": { ""label"": ""frontend"" } } ], ""data"": { ""label"": ""application_layers"" }, ""edges"": [ { ""id"": ""e_cf_docai"", ""sources"": [ ""cloud_functions"" ], ""targets"": [ ""document_ai"" ], ""labels"": [ { ""text"": ""gRPC"" } ] }, { ""id"": ""e_cf_search"", ""sources"": [ ""cloud_functions"" ], ""targets"": [ ""vertex_search"" ], ""labels"": [ { ""text"": ""gRPC"" } ] }, { ""id"": ""e_firestore_search"", ""sources"": [ ""firestore_ds"" ], ""targets"": [ ""vertex_search"" ], ""labels"": [ { ""text"": ""index"" } ] }, { ""id"": ""e_cf_llm"", ""sources"": [ ""cloud_functions"" ], ""targets"": [ ""vertex_ai_llm"" ], ""labels"": [ { ""text"": ""gRPC"" } ] }, { ""id"": ""e_storage_finetune"", ""sources"": [ ""storage_tuning"" ], ""targets"": [ ""vertex_ai_finetune"" ], ""labels"": [ { ""text"": ""tuning data"" } ] }, { ""id"": ""e_cf_firestore"", ""sources"": [ ""cloud_functions"" ], ""targets"": [ ""firestore_ds"" ], ""labels"": [ { ""text"": ""write"" } ] }, { ""id"": ""e_cf_storage_tune"", ""sources"": [ ""cloud_functions"" ], ""targets"": [ ""storage_tuning"" ], ""labels"": [ { ""text"": ""store"" } ] }, { ""id"": ""e_docs_eventarc"", ""sources"": [ ""storage_docs"" ], ""targets"": [ ""eventarc_trigger"" ], ""labels"": [ { ""text"": ""event"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud Platform"", ""icon"": ""gcp_logo"" } }, { ""id"": ""clients"", ""labels"": [ { ""text"": ""clients"" } ], ""children"": [ { ""id"": ""developer_client"", ""labels"": [ { ""text"": ""Developer"" } ], ""children"": [], ""data"": { ""label"": ""Developer"", ""icon"": ""browser_client"" } } ], ""data"": { ""label"": ""clients"" } } ], ""edges"": [ { ""id"": ""e_dev_upload"", ""sources"": [ ""developer_client"" ], ""targets"": [ ""storage_docs"" ], ""labels"": [ { ""text"": ""upload"" } ] } ] }"
gcp,ai-ml,gen-ai,https://cloud.google.com/architecture/ai-ml/generative-ai-rag,"Generative AI RAG with Cloud SQL Deploys a sample application using Cloud Run that uses retrieval augmented generation (RAG) based on embeddings created using Vertex AI, stored in Cloud SQL to provide a chat based experience to search for recommendations and find similar items. You can choose whether to deploy your Jump Start Solution through the console directly or download as Terraform on GitHub  to deploy later. Products used in this solution Product name Description Category Cloud Run	Fully managed application platform	Serverless Cloud SQL	Managed MySQL, PostgreSQL, SQL Server	Databases Vertex AI	One AI platform, every ML tool you need	Artificial Intelligence","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud Platform"" } ], ""children"": [ { ""id"": ""data_processing"", ""labels"": [ { ""text"": ""data_processing"" } ], ""children"": [ { ""id"": ""staging_bucket"", ""labels"": [ { ""text"": ""Staging bucket"" } ], ""children"": [], ""data"": { ""label"": ""Staging bucket"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""pgvector_db"", ""labels"": [ { ""text"": ""Cloud SQL (pgvector)"" } ], ""children"": [], ""data"": { ""label"": ""Cloud SQL (pgvector)"", ""icon"": ""gcp_cloud_sql"" } }, { ""id"": ""vector_embeddings"", ""labels"": [ { ""text"": ""Vector embeddings"" } ], ""children"": [], ""data"": { ""label"": ""Vector embeddings"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""e_stage_sql"", ""sources"": [ ""staging_bucket"" ], ""targets"": [ ""pgvector_db"" ], ""labels"": [ { ""text"": ""load"" } ] }, { ""id"": ""e_sql_vectors"", ""sources"": [ ""pgvector_db"" ], ""targets"": [ ""vector_embeddings"" ], ""labels"": [ { ""text"": ""index"" } ] } ] }, { ""id"": ""app"", ""labels"": [ { ""text"": ""app"" } ], ""children"": [ { ""id"": ""python_frontend"", ""labels"": [ { ""text"": ""Python frontend"" } ], ""children"": [], ""data"": { ""label"": ""Python frontend"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""python_backend"", ""labels"": [ { ""text"": ""Python backend"" } ], ""children"": [], ""data"": { ""label"": ""Python backend"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""llm_endpoint"", ""labels"": [ { ""text"": ""LLM endpoint"" } ], ""children"": [], ""data"": { ""label"": ""LLM endpoint"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""e_front_back"", ""sources"": [ ""python_frontend"" ], ""targets"": [ ""python_backend"" ], ""labels"": [ { ""text"": ""API"" } ] }, { ""id"": ""e_back_llm"", ""sources"": [ ""python_backend"" ], ""targets"": [ ""llm_endpoint"" ], ""labels"": [ { ""text"": ""predict"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud Platform"", ""icon"": ""gcp_logo"" }, ""edges"": [ { ""id"": ""e_back_sql"", ""sources"": [ ""python_backend"" ], ""targets"": [ ""pgvector_db"" ], ""labels"": [ { ""text"": ""query"" } ] } ] }, { ""id"": ""data_sources"", ""labels"": [ { ""text"": ""data_sources"" } ], ""children"": [ { ""id"": ""source_data"", ""labels"": [ { ""text"": ""Source data"" } ], ""children"": [], ""data"": { ""label"": ""Source data"", ""icon"": ""database"" } } ], ""edges"": [] }, { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""app_user"", ""labels"": [ { ""text"": ""User"" } ], ""children"": [], ""data"": { ""label"": ""User"", ""icon"": ""browser_client"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_src_stage"", ""sources"": [ ""source_data"" ], ""targets"": [ ""staging_bucket"" ], ""labels"": [ { ""text"": ""ingest"" } ] }, { ""id"": ""e_user_front"", ""sources"": [ ""app_user"" ], ""targets"": [ ""python_frontend"" ], ""labels"": [ { ""text"": ""HTTP"" } ] } ] }"
gcp,ai-ml,gen-ai,https://cloud.google.com/architecture/gen-ai-rag-vertex-ai-vector-search,"Infrastructure for a RAG-capable generative AI application using Vertex AI and Vector Search bookmark_border Release Notes Last reviewed 2025-03-07 UTC This document provides a reference architecture that you can use to design the infrastructure for a generative AI application with retrieval-augmented generation (RAG) by using Vector Search. Vector Search is a fully managed Google Cloud service that provides optimized serving infrastructure for very large-scale vector-similarity matching. The intended audience for this document includes architects, developers, and administrators of generative AI applications. The document assumes a basic understanding of AI, machine learning (ML), and large language model (LLM) concepts. This document doesn't provide guidance about how to design and develop a generative AI application.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""vertex_ai"", ""labels"": [ { ""text"": ""vertex_ai"" } ], ""children"": [ { ""id"": ""embeddings_api"", ""labels"": [ { ""text"": ""Embeddings API"" } ], ""children"": [], ""data"": { ""label"": ""Embeddings API"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""vector_search"", ""labels"": [ { ""text"": ""Vector Search"" } ], ""children"": [], ""data"": { ""label"": ""Vector Search"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""llm"", ""labels"": [ { ""text"": ""LLM (Gemini)"" } ], ""children"": [], ""data"": { ""label"": ""LLM (Gemini)"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""responsible_ai"", ""labels"": [ { ""text"": ""Responsible AI filters"" } ], ""children"": [], ""data"": { ""label"": ""Responsible AI filters"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""prompt_optimizer"", ""labels"": [ { ""text"": ""Prompt optimizer"" } ], ""children"": [], ""data"": { ""label"": ""Prompt optimizer"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""e_llm_ra"", ""sources"": [ ""llm"" ], ""targets"": [ ""responsible_ai"" ], ""labels"": [ { ""text"": ""filter"" } ] }, { ""id"": ""e_po_llm"", ""sources"": [ ""prompt_optimizer"" ], ""targets"": [ ""llm"" ], ""labels"": [ { ""text"": ""optimize"" } ] } ] }, { ""id"": ""observability"", ""labels"": [ { ""text"": ""observability"" } ], ""children"": [ { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Logging"", ""icon"": ""gcp_cloud_logging"" } }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Monitoring"", ""icon"": ""gcp_cloud_monitoring"" } }, { ""id"": ""bigquery"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery"", ""icon"": ""gcp_bigquery"" } } ], ""edges"": [ { ""id"": ""e_log_mon"", ""sources"": [ ""cloud_logging"" ], ""targets"": [ ""cloud_monitoring"" ], ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""e_mon_bq"", ""sources"": [ ""cloud_monitoring"" ], ""targets"": [ ""bigquery"" ], ""labels"": [ { ""text"": ""export"" } ] } ] }, { ""id"": ""data_ingestion"", ""labels"": [ { ""text"": ""data_ingestion"" } ], ""children"": [ { ""id"": ""staging_bucket"", ""labels"": [ { ""text"": ""Staging bucket"" } ], ""children"": [], ""data"": { ""label"": ""Staging bucket"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""ingest_pubsub"", ""labels"": [ { ""text"": ""Pub/Sub topic"" } ], ""children"": [], ""data"": { ""label"": ""Pub/Sub topic"", ""icon"": ""gcp_pubsub"" } }, { ""id"": ""data_proc_fn"", ""labels"": [ { ""text"": ""Cloud Run Fn"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Run Fn"", ""icon"": ""gcp_cloud_run"" } } ], ""edges"": [ { ""id"": ""e_bucket_pubsub"", ""sources"": [ ""staging_bucket"" ], ""targets"": [ ""ingest_pubsub"" ], ""labels"": [ { ""text"": ""message"" } ] }, { ""id"": ""e_pubsub_fn"", ""sources"": [ ""ingest_pubsub"" ], ""targets"": [ ""data_proc_fn"" ], ""labels"": [ { ""text"": ""trigger"" } ] } ] }, { ""id"": ""serving"", ""labels"": [ { ""text"": ""serving"" } ], ""children"": [ { ""id"": ""chat_frontend"", ""labels"": [ { ""text"": ""Cloud Run Frontend"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Run Frontend"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""rag_backend"", ""labels"": [ { ""text"": ""Cloud Run Backend"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Run Backend"", ""icon"": ""gcp_cloud_run"" } } ], ""edges"": [ { ""id"": ""e_front_back"", ""sources"": [ ""chat_frontend"" ], ""targets"": [ ""rag_backend"" ], ""labels"": [ { ""text"": ""API"" } ] }, { ""id"": ""e_back_front_resp"", ""sources"": [ ""rag_backend"" ], ""targets"": [ ""chat_frontend"" ], ""labels"": [ { ""text"": ""response"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" }, ""edges"": [ { ""id"": ""e_fn_embed"", ""sources"": [ ""data_proc_fn"" ], ""targets"": [ ""embeddings_api"" ], ""labels"": [ { ""text"": ""embed"" } ] }, { ""id"": ""e_fn_index"", ""sources"": [ ""data_proc_fn"" ], ""targets"": [ ""vector_search"" ], ""labels"": [ { ""text"": ""update index"" } ] }, { ""id"": ""e_back_embed"", ""sources"": [ ""rag_backend"" ], ""targets"": [ ""embeddings_api"" ], ""labels"": [ { ""text"": ""create embedding"" } ] }, { ""id"": ""e_back_vs"", ""sources"": [ ""rag_backend"" ], ""targets"": [ ""vector_search"" ], ""labels"": [ { ""text"": ""similarity search"" } ] }, { ""id"": ""e_back_llm"", ""sources"": [ ""rag_backend"" ], ""targets"": [ ""llm"" ], ""labels"": [ { ""text"": ""prompt"" } ] }, { ""id"": ""e_back_ra"", ""sources"": [ ""rag_backend"" ], ""targets"": [ ""responsible_ai"" ], ""labels"": [ { ""text"": ""safety check"" } ] }, { ""id"": ""e_serving_to_log"", ""sources"": [ ""serving"" ], ""targets"": [ ""cloud_logging"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""e_serving_to_mon"", ""sources"": [ ""serving"" ], ""targets"": [ ""cloud_monitoring"" ], ""labels"": [ { ""text"": ""metrics"" } ] } ] }, { ""id"": ""genai_devs"", ""labels"": [ { ""text"": ""genai_devs"" } ], ""children"": [ { ""id"": ""genai_dev"", ""labels"": [ { ""text"": ""Gen-AI Dev"" } ], ""children"": [], ""data"": { ""label"": ""Gen-AI Dev"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""app_users"", ""labels"": [ { ""text"": ""app_users"" } ], ""children"": [ { ""id"": ""app_user"", ""labels"": [ { ""text"": ""App User"" } ], ""children"": [], ""data"": { ""label"": ""App User"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""data_engineers"", ""labels"": [ { ""text"": ""data_engineers"" } ], ""children"": [ { ""id"": ""data_engineer"", ""labels"": [ { ""text"": ""Data Engineer"" } ], ""children"": [], ""data"": { ""label"": ""Data Engineer"", ""icon"": ""browser_client"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_dev_prompts"", ""sources"": [ ""genai_dev"" ], ""targets"": [ ""prompt_optimizer"" ], ""labels"": [ { ""text"": ""sample prompts"" } ] }, { ""id"": ""e_user_query"", ""sources"": [ ""app_user"" ], ""targets"": [ ""chat_frontend"" ], ""labels"": [ { ""text"": ""query"" } ] }, { ""id"": ""e_engineer_upload"", ""sources"": [ ""data_engineer"" ], ""targets"": [ ""staging_bucket"" ], ""labels"": [ { ""text"": ""upload"" } ] }, { ""id"": ""e_po_genai_dev"", ""sources"": [ ""prompt_optimizer"" ], ""targets"": [ ""genai_dev"" ], ""labels"": [ { ""text"": ""evaluation scores"" } ] } ] }"
gcp,ai-ml,gen-ai,https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-vertex-ai,"Infrastructure for a RAG-capable generative AI application using Vertex AI and AlloyDB for PostgreSQL bookmark_border Release Notes Last reviewed 2024-12-11 UTC This document provides a reference architecture that you can use to design the infrastructure to run a generative artificial intelligence (AI) application with retrieval-augmented generation (RAG). The intended audience for this document includes developers and administrators of generative AI applications and cloud architects. The document assumes a basic understanding of AI, machine learning (ML), and large language model (LLM) concepts. This document doesn't provide guidance about how to design and develop a generative AI application.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""app_user"", ""labels"": [ { ""text"": ""Application user"" } ], ""children"": [], ""data"": { ""label"": ""Application user"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""quality_evaluation"", ""labels"": [ { ""text"": ""quality_evaluation"" } ], ""children"": [ { ""id"": ""pubsub_eval"", ""labels"": [ { ""text"": ""Pub/Sub (Eval trigger)"" } ], ""children"": [], ""data"": { ""label"": ""Pub/Sub (Eval trigger)"", ""icon"": ""gcp_pubsub"" } }, { ""id"": ""run_evaluator"", ""labels"": [ { ""text"": ""Cloud Run – Evaluator"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Run – Evaluator"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""bigquery_eval"", ""labels"": [ { ""text"": ""BigQuery (Eval results)"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery (Eval results)"", ""icon"": ""gcp_bigquery"" } } ], ""edges"": [ { ""id"": ""e_pubsub_to_eval"", ""sources"": [ ""pubsub_eval"" ], ""targets"": [ ""run_evaluator"" ], ""labels"": [ { ""text"": ""trigger"" } ] }, { ""id"": ""e_eval_to_bq"", ""sources"": [ ""run_evaluator"" ], ""targets"": [ ""bigquery_eval"" ], ""labels"": [ { ""text"": ""store scores"" } ] } ] }, { ""id"": ""database"", ""labels"": [ { ""text"": ""database"" } ], ""children"": [ { ""id"": ""alloydb"", ""labels"": [ { ""text"": ""AlloyDB"" } ], ""children"": [], ""data"": { ""label"": ""AlloyDB"", ""icon"": ""gcp_cloud_sql"" } } ], ""edges"": [] }, { ""id"": ""ingestion"", ""labels"": [ { ""text"": ""ingestion"" } ], ""children"": [ { ""id"": ""storage_bucket"", ""labels"": [ { ""text"": ""Cloud Storage Bucket"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Storage Bucket"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""pubsub_ingest"", ""labels"": [ { ""text"": ""Pub/Sub (Ingest)"" } ], ""children"": [], ""data"": { ""label"": ""Pub/Sub (Ingest)"", ""icon"": ""gcp_pubsub"" } }, { ""id"": ""run_file_processor"", ""labels"": [ { ""text"": ""Cloud Run – File processor"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Run – File processor"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""document_ai"", ""labels"": [ { ""text"": ""Document AI"" } ], ""children"": [], ""data"": { ""label"": ""Document AI"", ""icon"": ""gcp_document_ai"" } }, { ""id"": ""vertex_embeddings"", ""labels"": [ { ""text"": ""Vertex AI – Embeddings"" } ], ""children"": [], ""data"": { ""label"": ""Vertex AI – Embeddings"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""e_bucket_pubsub"", ""sources"": [ ""storage_bucket"" ], ""targets"": [ ""pubsub_ingest"" ], ""labels"": [ { ""text"": ""message"" } ] }, { ""id"": ""e_pubsub_processor"", ""sources"": [ ""pubsub_ingest"" ], ""targets"": [ ""run_file_processor"" ], ""labels"": [ { ""text"": ""trigger"" } ] }, { ""id"": ""e_processor_docai"", ""sources"": [ ""run_file_processor"" ], ""targets"": [ ""document_ai"" ], ""labels"": [ { ""text"": ""parse/convert"" } ] }, { ""id"": ""e_docai_embeddings"", ""sources"": [ ""document_ai"" ], ""targets"": [ ""vertex_embeddings"" ], ""labels"": [ { ""text"": ""generate"" } ] }, { ""id"": ""e_processor_embeddings"", ""sources"": [ ""run_file_processor"" ], ""targets"": [ ""vertex_embeddings"" ], ""labels"": [ { ""text"": ""embed"" } ] } ] }, { ""id"": ""serving"", ""labels"": [ { ""text"": ""serving"" } ], ""children"": [ { ""id"": ""llm_vertex"", ""labels"": [ { ""text"": ""Vertex AI – LLM"" } ], ""children"": [], ""data"": { ""label"": ""Vertex AI – LLM"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""responsible_ai"", ""labels"": [ { ""text"": ""Responsible AI"" } ], ""children"": [], ""data"": { ""label"": ""Responsible AI"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Logging"", ""icon"": ""gcp_cloud_logging"" } }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Monitoring"", ""icon"": ""gcp_cloud_monitoring"" } }, { ""id"": ""bigquery_analytics"", ""labels"": [ { ""text"": ""BigQuery (Analytics)"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery (Analytics)"", ""icon"": ""gcp_bigquery"" } }, { ""id"": ""generative_ai_app"", ""labels"": [ { ""text"": ""generative_ai_app"" } ], ""children"": [ { ""id"": ""frontend"", ""labels"": [ { ""text"": ""Frontend (chatbot)"" } ], ""children"": [], ""data"": { ""label"": ""Frontend (chatbot)"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""backend"", ""labels"": [ { ""text"": ""Backend"" } ], ""children"": [], ""data"": { ""label"": ""Backend"", ""icon"": ""gcp_cloud_run"" } } ], ""edges"": [ { ""id"": ""e_backend_frontend_resp"", ""sources"": [ ""backend"" ], ""targets"": [ ""frontend"" ], ""labels"": [ { ""text"": ""response"" } ] }, { ""id"": ""e_frontend_backend"", ""sources"": [ ""frontend"" ], ""targets"": [ ""backend"" ], ""labels"": [ { ""text"": ""API"" } ] } ] } ], ""edges"": [ { ""id"": ""e_llm_responsible"", ""sources"": [ ""llm_vertex"" ], ""targets"": [ ""responsible_ai"" ], ""labels"": [ { ""text"": ""safety filter"" } ] }, { ""id"": ""e_metrics_bq"", ""sources"": [ ""cloud_monitoring"" ], ""targets"": [ ""bigquery_analytics"" ], ""labels"": [ { ""text"": ""export"" } ] }, { ""id"": ""e_backend_llm"", ""sources"": [ ""backend"" ], ""targets"": [ ""llm_vertex"" ], ""labels"": [ { ""text"": ""inference request"" } ] }, { ""id"": ""e_back_logs"", ""sources"": [ ""backend"" ], ""targets"": [ ""cloud_logging"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""e_back_metrics"", ""sources"": [ ""backend"" ], ""targets"": [ ""cloud_monitoring"" ], ""labels"": [ { ""text"": ""metrics"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" }, ""edges"": [ { ""id"": ""e_db_to_pubsub"", ""sources"": [ ""alloydb"" ], ""targets"": [ ""pubsub_eval"" ], ""labels"": [ { ""text"": ""eval trigger"" } ] }, { ""id"": ""e_embeddings_to_db"", ""sources"": [ ""vertex_embeddings"" ], ""targets"": [ ""alloydb"" ], ""labels"": [ { ""text"": ""store chunks"" } ] }, { ""id"": ""e_backend_semsearch"", ""sources"": [ ""backend"" ], ""targets"": [ ""vertex_embeddings"" ], ""labels"": [ { ""text"": ""semantic search"" } ] }, { ""id"": ""e_cloud_run_frontend"", ""sources"": [ ""run_evaluator"" ], ""targets"": [ ""frontend"" ], ""labels"": [ { ""text"": ""invoke"" } ] }, { ""id"": ""e_alloydb_backend_app"", ""sources"": [ ""alloydb"" ], ""targets"": [ ""backend"" ], ""labels"": [ { ""text"": ""construct prompt"" } ] } ] }, { ""id"": ""data_engineer"", ""labels"": [ { ""text"": ""data_engineer"" } ], ""children"": [ { ""id"": ""data_upload"", ""labels"": [ { ""text"": ""Data Upload"" } ], ""children"": [], ""data"": { ""label"": ""Data Upload"", ""icon"": ""gcp_cloud_storage"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_user_frontend"", ""sources"": [ ""app_user"" ], ""targets"": [ ""frontend"" ], ""labels"": [ { ""text"": ""request"" } ] }, { ""id"": ""e_data_upload_bucket"", ""sources"": [ ""data_upload"" ], ""targets"": [ ""storage_bucket"" ], ""labels"": [ { ""text"": ""upload"" } ] } ] }"
gcp,ai-ml,gen-ai,https://cloud.google.com/architecture/rag-capable-gen-ai-app-using-gke,"Infrastructure for a RAG-capable generative AI application using GKE and Cloud SQL bookmark_border Release Notes Last reviewed 2024-12-11 UTC This document provides a reference architecture that you can use to design the infrastructure to run a generative AI application with retrieval-augmented generation (RAG) using Google Kubernetes Engine (GKE), Cloud SQL, and open source tools like Ray, Hugging Face, and LangChain. To help you experiment with this reference architecture, a sample application and Terraform configuration are provided in GitHub. This document is for developers who want to rapidly build and deploy RAG-capable generative AI applications by using open source tools and models. It assumes that you have experience with using GKE and Cloud SQL and that you have a conceptual understanding of AI, machine learning (ML), and large language models (LLMs). This document doesn't provide guidance about how to design and develop a generative AI application.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""app_user"", ""labels"": [ { ""text"": ""Application user"" } ], ""children"": [], ""data"": { ""label"": ""Application user"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""external_sources"", ""labels"": [ { ""text"": ""external_sources"" } ], ""children"": [ { ""id"": ""raw_data"", ""labels"": [ { ""text"": ""Raw data source"" } ], ""children"": [], ""data"": { ""label"": ""Raw data source"", ""icon"": ""database"" } } ], ""edges"": [] }, { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""application"", ""labels"": [ { ""text"": ""application"" } ], ""children"": [ { ""id"": ""serving"", ""labels"": [ { ""text"": ""serving"" } ], ""children"": [ { ""id"": ""iap"", ""labels"": [ { ""text"": ""Identity-Aware Proxy"" } ], ""children"": [], ""data"": { ""label"": ""Identity-Aware Proxy"", ""icon"": ""gcp_identity-aware_proxy"" } }, { ""id"": ""frontend_server"", ""labels"": [ { ""text"": ""Frontend server"" } ], ""children"": [], ""data"": { ""label"": ""Frontend server"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""chat_interface"", ""labels"": [ { ""text"": ""Chat interface"" } ], ""children"": [], ""data"": { ""label"": ""Chat interface"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""inference_server"", ""labels"": [ { ""text"": ""Inference server (TGI)"" } ], ""children"": [], ""data"": { ""label"": ""Inference server (TGI)"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""responsible_ai"", ""labels"": [ { ""text"": ""Responsible AI"" } ], ""children"": [], ""data"": { ""label"": ""Responsible AI"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""e_iap_frontend"", ""sources"": [ ""iap"" ], ""targets"": [ ""frontend_server"" ], ""labels"": [ { ""text"": ""auth pass"" } ] }, { ""id"": ""e_front_chat"", ""sources"": [ ""frontend_server"" ], ""targets"": [ ""chat_interface"" ], ""labels"": [ { ""text"": ""UI events"" } ] }, { ""id"": ""e_front_infer"", ""sources"": [ ""frontend_server"" ], ""targets"": [ ""inference_server"" ], ""labels"": [ { ""text"": ""LLM prompt"" } ] }, { ""id"": ""e_infer_resp"", ""sources"": [ ""inference_server"" ], ""targets"": [ ""frontend_server"" ], ""labels"": [ { ""text"": ""LLM response"" } ] }, { ""id"": ""e_front_resp_ai"", ""sources"": [ ""frontend_server"" ], ""targets"": [ ""responsible_ai"" ], ""labels"": [ { ""text"": ""safety check"" } ] } ] }, { ""id"": ""embedding"", ""labels"": [ { ""text"": ""embedding"" } ], ""children"": [ { ""id"": ""embedding_service"", ""labels"": [ { ""text"": ""Embedding service (Ray)"" } ], ""children"": [], ""data"": { ""label"": ""Embedding service (Ray)"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""storage_bucket"", ""labels"": [ { ""text"": ""Cloud Storage Bucket"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Storage Bucket"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""pubsub_ingest"", ""labels"": [ { ""text"": ""Pub/Sub (ingest)"" } ], ""children"": [], ""data"": { ""label"": ""Pub/Sub (ingest)"", ""icon"": ""gcp_pubsub"" } } ], ""edges"": [ { ""id"": ""e_bucket_pubsub"", ""sources"": [ ""storage_bucket"" ], ""targets"": [ ""pubsub_ingest"" ], ""labels"": [ { ""text"": ""notify"" } ] }, { ""id"": ""e_pubsub_embed"", ""sources"": [ ""pubsub_ingest"" ], ""targets"": [ ""embedding_service"" ], ""labels"": [ { ""text"": ""trigger"" } ] } ] }, { ""id"": ""database"", ""labels"": [ { ""text"": ""database"" } ], ""children"": [ { ""id"": ""cloud_sql_pgvector"", ""labels"": [ { ""text"": ""Cloud SQL (pgvector)"" } ], ""children"": [], ""data"": { ""label"": ""Cloud SQL (pgvector)"", ""icon"": ""gcp_cloud_sql"" } } ], ""edges"": [] }, { ""id"": ""observability"", ""labels"": [ { ""text"": ""observability"" } ], ""children"": [ { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Logging"", ""icon"": ""gcp_cloud_logging"" } }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Monitoring"", ""icon"": ""gcp_cloud_monitoring"" } }, { ""id"": ""bigquery_analytics"", ""labels"": [ { ""text"": ""BigQuery (Analytics)"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery (Analytics)"", ""icon"": ""gcp_bigquery"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_front_bq"", ""sources"": [ ""frontend_server"" ], ""targets"": [ ""bigquery_analytics"" ], ""labels"": [ { ""text"": ""analytics"" } ] }, { ""id"": ""e_front_mon"", ""sources"": [ ""frontend_server"" ], ""targets"": [ ""cloud_monitoring"" ], ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""e_front_log"", ""sources"": [ ""frontend_server"" ], ""targets"": [ ""cloud_logging"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""e_front_sem_search"", ""sources"": [ ""frontend_server"" ], ""targets"": [ ""cloud_sql_pgvector"" ], ""labels"": [ { ""text"": ""semantic search"" } ] }, { ""id"": ""e_embed_to_db"", ""sources"": [ ""embedding_service"" ], ""targets"": [ ""cloud_sql_pgvector"" ], ""labels"": [ { ""text"": ""store embeddings"" } ] } ] }, { ""id"": ""internal_sources"", ""labels"": [ { ""text"": ""internal_sources"" } ], ""children"": [ { ""id"": ""internal_data"", ""labels"": [ { ""text"": ""Internal data source"" } ], ""children"": [], ""data"": { ""label"": ""Internal data source"", ""icon"": ""database"" } } ], ""edges"": [] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" }, ""edges"": [ { ""id"": ""e_internal_to_bucket_embed"", ""sources"": [ ""internal_sources"" ], ""targets"": [ ""storage_bucket"" ], ""labels"": [ { ""text"": ""store"" } ] } ] } ], ""edges"": [ { ""id"": ""e_raw_to_bucket"", ""sources"": [ ""raw_data"" ], ""targets"": [ ""storage_bucket"" ], ""labels"": [ { ""text"": ""raw data"" } ] }, { ""id"": ""e_user_iap"", ""sources"": [ ""app_user"" ], ""targets"": [ ""iap"" ], ""labels"": [ { ""text"": ""request"" } ] } ] }"
gcp,ai-ml,gen-ai,https://cloud.google.com/architecture/gen-ai-graphrag-spanner,"GraphRAG infrastructure for generative AI using Vertex AI and Spanner Graph bookmark_border Release Notes Last reviewed 2025-07-01 UTC This document provides a reference architecture to help you design infrastructure for GraphRAG generative AI applications in Google Cloud. The intended audience includes architects, developers, and administrators who build and manage intelligent information retrieval systems. The document assumes a foundational understanding of AI, graph data management, and knowledge graph concepts. This document doesn't provide specific guidance for designing and developing GraphRAG applications. GraphRAG is a graph-based approach to retrieval augmented generation (RAG). RAG helps to ground AI-generated responses by augmenting prompts with contextually relevant data that's retrieved using vector search. GraphRAG combines vector search with a knowledge-graph query to retrieve contextual data that better reflects the interconnectedness of data from diverse sources. Prompts that are augmented using GraphRAG can generate more detailed and relevant AI responses.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""app_user"", ""labels"": [ { ""text"": ""Application user"" } ], ""children"": [], ""data"": { ""label"": ""Application user"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""data_engineers"", ""labels"": [ { ""text"": ""data_engineers"" } ], ""children"": [ { ""id"": ""data_engineer"", ""labels"": [ { ""text"": ""Data engineer"" } ], ""children"": [], ""data"": { ""label"": ""Data engineer"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""application"", ""labels"": [ { ""text"": ""application"" } ], ""children"": [ { ""id"": ""serving"", ""labels"": [ { ""text"": ""serving"" } ], ""children"": [ { ""id"": ""agent"", ""labels"": [ { ""text"": ""Agent"" } ], ""children"": [], ""data"": { ""label"": ""Agent"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""agent_eng"", ""labels"": [ { ""text"": ""Vertex AI Agent Engine"" } ], ""children"": [], ""data"": { ""label"": ""Vertex AI Agent Engine"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""rank_api"", ""labels"": [ { ""text"": ""AI Apps Ranking API"" } ], ""children"": [], ""data"": { ""label"": ""AI Apps Ranking API"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""s_2d_rank"", ""sources"": [ ""agent_eng"" ], ""targets"": [ ""rank_api"" ], ""labels"": [ { ""text"": ""rank"" } ] }, { ""id"": ""s_2e_sum"", ""sources"": [ ""agent_eng"" ], ""targets"": [ ""agent"" ], ""labels"": [ { ""text"": ""summary"" } ] } ] }, { ""id"": ""ingestion"", ""labels"": [ { ""text"": ""ingestion"" } ], ""children"": [ { ""id"": ""bucket"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Storage"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""pubsub_ing"", ""labels"": [ { ""text"": ""Pub/Sub queue"" } ], ""children"": [], ""data"": { ""label"": ""Pub/Sub queue"", ""icon"": ""gcp_pubsub"" } }, { ""id"": ""file_fn"", ""labels"": [ { ""text"": ""Cloud Run function"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Run function"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""embed_api"", ""labels"": [ { ""text"": ""Vertex AI Embeddings"" } ], ""children"": [], ""data"": { ""label"": ""Vertex AI Embeddings"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""gemini_api"", ""labels"": [ { ""text"": ""Vertex AI Gemini"" } ], ""children"": [], ""data"": { ""label"": ""Vertex AI Gemini"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""spanner"", ""labels"": [ { ""text"": ""Spanner (KG + vec)"" } ], ""children"": [], ""data"": { ""label"": ""Spanner (KG + vec)"", ""icon"": ""gcp_spanner"" } } ], ""edges"": [ { ""id"": ""b_bucket_msg"", ""sources"": [ ""bucket"" ], ""targets"": [ ""pubsub_ing"" ], ""labels"": [ { ""text"": ""message"" } ] }, { ""id"": ""c_trigger_fn"", ""sources"": [ ""pubsub_ing"" ], ""targets"": [ ""file_fn"" ], ""labels"": [ { ""text"": ""trigger"" } ] }, { ""id"": ""d_fn_gemini"", ""sources"": [ ""file_fn"" ], ""targets"": [ ""gemini_api"" ], ""labels"": [ { ""text"": ""build KG"" } ] }, { ""id"": ""e_fn_spanner"", ""sources"": [ ""file_fn"" ], ""targets"": [ ""spanner"" ], ""labels"": [ { ""text"": ""store KG"" } ] }, { ""id"": ""f_fn_embed"", ""sources"": [ ""file_fn"" ], ""targets"": [ ""embed_api"" ], ""labels"": [ { ""text"": ""embeddings"" } ] }, { ""id"": ""g_embed_back"", ""sources"": [ ""embed_api"" ], ""targets"": [ ""file_fn"" ], ""labels"": [ { ""text"": ""vectors"" } ] }, { ""id"": ""h_fn_spanner"", ""sources"": [ ""file_fn"" ], ""targets"": [ ""spanner"" ], ""labels"": [ { ""text"": ""store vectors"" } ] } ] }, { ""id"": ""observability"", ""labels"": [ { ""text"": ""observability"" } ], ""children"": [ { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Logging"", ""icon"": ""gcp_cloud_logging"" } }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Monitoring"", ""icon"": ""gcp_cloud_monitoring"" } }, { ""id"": ""bq_analytics"", ""labels"": [ { ""text"": ""BigQuery Analytics"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery Analytics"", ""icon"": ""gcp_bigquery"" } } ], ""edges"": [] }, { ""id"": ""iam_group"", ""labels"": [ { ""text"": ""iam_group"" } ], ""children"": [ { ""id"": ""iam"", ""labels"": [ { ""text"": ""Identity and Access Management"" } ], ""children"": [], ""data"": { ""label"": ""Identity and Access Management"", ""icon"": ""gcp_identity_and_access_management"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""bq"", ""sources"": [ ""agent"" ], ""targets"": [ ""bq_analytics"" ], ""labels"": [ { ""text"": ""analytics"" } ] }, { ""id"": ""mon"", ""sources"": [ ""agent"" ], ""targets"": [ ""cloud_monitoring"" ], ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""log"", ""sources"": [ ""agent"" ], ""targets"": [ ""cloud_logging"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""s_2c"", ""sources"": [ ""agent_eng"" ], ""targets"": [ ""spanner"" ], ""labels"": [ { ""text"": ""graph data"" } ] }, { ""id"": ""s_2b"", ""sources"": [ ""agent_eng"" ], ""targets"": [ ""spanner"" ], ""labels"": [ { ""text"": ""vector search"" } ] }, { ""id"": ""s_2a"", ""sources"": [ ""agent_eng"" ], ""targets"": [ ""embed_api"" ], ""labels"": [ { ""text"": ""embed"" } ] }, { ""id"": ""edge_iam_group_to_bucket"", ""sources"": [ ""iam_group"" ], ""targets"": [ ""bucket"" ], ""labels"": [ { ""text"": ""access"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" }, ""edges"": [] } ], ""edges"": [ { ""id"": ""req"", ""sources"": [ ""app_user"" ], ""targets"": [ ""agent"" ], ""labels"": [ { ""text"": ""request"" } ] }, { ""id"": ""resp"", ""sources"": [ ""agent"" ], ""targets"": [ ""app_user"" ], ""labels"": [ { ""text"": ""response"" } ] }, { ""id"": ""edge_data_engineers_to_iam_group"", ""sources"": [ ""data_engineers"" ], ""targets"": [ ""iam_group"" ], ""labels"": [ { ""text"": ""manage"" } ] } ] }"
gcp,ai-ml,gen-ai,https://cloud.google.com/architecture/use-generative-ai-utilization-management,"Use generative AI for utilization management bookmark_border Release Notes Last reviewed 2024-08-19 UTC This document describes a reference architecture for health insurance companies who want to automate prior authorization (PA) request processing and improve their utilization review (UR) processes by using Google Cloud. It's intended for software developers and program administrators in these organizations. This architecture helps to enable health plan providers to reduce administrative overhead, increase efficiency, and enhance decision-making by automating data ingestion and the extraction of insights from clinical forms. It also allows them to use AI models for prompt generation and recommendations. Architecture The following diagram describes an architecture and an approach for automating the data ingestion workflow and optimizing the utilization management (UM) review process. This approach uses data and AI services in Google Cloud.","{ ""id"": ""root"", ""children"": [ { ""id"": ""pa_case_managers"", ""labels"": [ { ""text"": ""pa_case_managers"" } ], ""children"": [ { ""id"": ""pa_manager"", ""labels"": [ { ""text"": ""PA case manager"" } ], ""children"": [], ""data"": { ""label"": ""PA case manager"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""ur_specialists"", ""labels"": [ { ""text"": ""ur_specialists"" } ], ""children"": [ { ""id"": ""ur_specialist"", ""labels"": [ { ""text"": ""UR specialist"" } ], ""children"": [], ""data"": { ""label"": ""UR specialist"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""data_science_team"", ""labels"": [ { ""text"": ""data_science_team"" } ], ""children"": [ { ""id"": ""data_scientist"", ""labels"": [ { ""text"": ""Data-science team"" } ], ""children"": [], ""data"": { ""label"": ""Data-science team"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""application"", ""labels"": [ { ""text"": ""application"" } ], ""children"": [ { ""id"": ""claims_activator"", ""labels"": [ { ""text"": ""claims_activator"" } ], ""children"": [ { ""id"": ""pa_forms_bkt"", ""labels"": [ { ""text"": ""pa_forms_bkt"" } ], ""children"": [], ""data"": { ""label"": ""pa_forms_bkt"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""ingest_svc"", ""labels"": [ { ""text"": ""ingestion_service"" } ], ""children"": [], ""data"": { ""label"": ""ingestion_service"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""doc_ai"", ""labels"": [ { ""text"": ""form_processors (Doc AI)"" } ], ""children"": [], ""data"": { ""label"": ""form_processors (Doc AI)"", ""icon"": ""gcp_document_ai"" } }, { ""id"": ""pa_forms_db"", ""labels"": [ { ""text"": ""pa_form_collection (Firestore)"" } ], ""children"": [], ""data"": { ""label"": ""pa_form_collection (Firestore)"", ""icon"": ""gcp_firestore"" } }, { ""id"": ""hitl_app"", ""labels"": [ { ""text"": ""hitl_app"" } ], ""children"": [], ""data"": { ""label"": ""hitl_app"", ""icon"": ""gcp_cloud_run"" } } ], ""edges"": [ { ""id"": ""c2_event"", ""sources"": [ ""pa_forms_bkt"" ], ""targets"": [ ""ingest_svc"" ], ""labels"": [ { ""text"": ""event"" } ] }, { ""id"": ""c3_parse"", ""sources"": [ ""ingest_svc"" ], ""targets"": [ ""doc_ai"" ], ""labels"": [ { ""text"": ""parse"" } ] }, { ""id"": ""c3_store"", ""sources"": [ ""ingest_svc"" ], ""targets"": [ ""pa_forms_db"" ], ""labels"": [ { ""text"": ""store"" } ] }, { ""id"": ""c4_review"", ""sources"": [ ""hitl_app"" ], ""targets"": [ ""pa_forms_db"" ], ""labels"": [ { ""text"": ""review"" } ] } ] }, { ""id"": ""ur_service"", ""labels"": [ { ""text"": ""ur_service"" } ], ""children"": [ { ""id"": ""ur_app"", ""labels"": [ { ""text"": ""ur_app"" } ], ""children"": [], ""data"": { ""label"": ""ur_app"", ""icon"": ""gcp_cloud_run"" } }, { ""id"": ""prompt_model"", ""labels"": [ { ""text"": ""prompt_model & ur_model"" } ], ""children"": [], ""data"": { ""label"": ""prompt_model & ur_model"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""ur_search"", ""labels"": [ { ""text"": ""ur_search_app (Agent Builder)"" } ], ""children"": [], ""data"": { ""label"": ""ur_search_app (Agent Builder)"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""r2"", ""sources"": [ ""ur_app"" ], ""targets"": [ ""prompt_model"" ], ""labels"": [ { ""text"": ""R2"" } ] }, { ""id"": ""r3"", ""sources"": [ ""prompt_model"" ], ""targets"": [ ""ur_app"" ], ""labels"": [ { ""text"": ""R3"" } ] } ] }, { ""id"": ""train_deploy_doc"", ""labels"": [ { ""text"": ""train_deploy_doc"" } ], ""children"": [ { ""id"": ""training_forms_bkt"", ""labels"": [ { ""text"": ""training_forms"" } ], ""children"": [], ""data"": { ""label"": ""training_forms"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""eval_forms_bkt"", ""labels"": [ { ""text"": ""eval_forms"" } ], ""children"": [], ""data"": { ""label"": ""eval_forms"", ""icon"": ""gcp_cloud_storage"" } } ], ""edges"": [] }, { ""id"": ""tune_deploy_llm"", ""labels"": [ { ""text"": ""tune_deploy_llm"" } ], ""children"": [ { ""id"": ""tuning_dataset_bkt"", ""labels"": [ { ""text"": ""tuning_dataset"" } ], ""children"": [], ""data"": { ""label"": ""tuning_dataset"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""eval_dataset_bkt"", ""labels"": [ { ""text"": ""eval_dataset"" } ], ""children"": [], ""data"": { ""label"": ""eval_dataset"", ""icon"": ""gcp_cloud_storage"" } } ], ""edges"": [] }, { ""id"": ""knowledge_docs"", ""labels"": [ { ""text"": ""knowledge_docs"" } ], ""children"": [ { ""id"": ""clinical_docs_bkt"", ""labels"": [ { ""text"": ""clinical_docs"" } ], ""children"": [], ""data"": { ""label"": ""clinical_docs"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""um_policies_bkt"", ""labels"": [ { ""text"": ""UM_policies"" } ], ""children"": [], ""data"": { ""label"": ""UM_policies"", ""icon"": ""gcp_cloud_storage"" } } ], ""edges"": [] }, { ""id"": ""observability"", ""labels"": [ { ""text"": ""observability"" } ], ""children"": [ { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Logging"", ""icon"": ""gcp_cloud_logging"" } }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Monitoring"", ""icon"": ""gcp_cloud_monitoring"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""ing_metrics"", ""sources"": [ ""ingest_svc"" ], ""targets"": [ ""cloud_monitoring"" ], ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""ing_logs"", ""sources"": [ ""ingest_svc"" ], ""targets"": [ ""cloud_logging"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""evalform_to_docai"", ""sources"": [ ""eval_forms_bkt"" ], ""targets"": [ ""doc_ai"" ], ""labels"": [ { ""text"": ""parse"" } ] }, { ""id"": ""train_to_docai"", ""sources"": [ ""training_forms_bkt"" ], ""targets"": [ ""doc_ai"" ], ""labels"": [ { ""text"": ""parse"" } ] }, { ""id"": ""r5"", ""sources"": [ ""ur_app"" ], ""targets"": [ ""pa_forms_db"" ], ""labels"": [ { ""text"": ""R5"" } ] }, { ""id"": ""ur_metrics"", ""sources"": [ ""ur_app"" ], ""targets"": [ ""cloud_monitoring"" ], ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""ur_logs"", ""sources"": [ ""ur_app"" ], ""targets"": [ ""cloud_logging"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""eval_ds_to_model"", ""sources"": [ ""eval_dataset_bkt"" ], ""targets"": [ ""prompt_model"" ], ""labels"": [ { ""text"": ""eval"" } ] }, { ""id"": ""tune_to_model"", ""sources"": [ ""tuning_dataset_bkt"" ], ""targets"": [ ""prompt_model"" ], ""labels"": [ { ""text"": ""fine-tune"" } ] }, { ""id"": ""pol_to_search"", ""sources"": [ ""um_policies_bkt"" ], ""targets"": [ ""ur_search"" ], ""labels"": [ { ""text"": ""index policies"" } ] }, { ""id"": ""docs_to_search"", ""sources"": [ ""clinical_docs_bkt"" ], ""targets"": [ ""ur_search"" ], ""labels"": [ { ""text"": ""index docs"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" }, ""edges"": [] } ], ""edges"": [ { ""id"": ""upload_train_forms"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""training_forms_bkt"" ], ""labels"": [ { ""text"": ""upload"" } ] }, { ""id"": ""upload_eval_forms"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""eval_forms_bkt"" ], ""labels"": [ { ""text"": ""upload"" } ] }, { ""id"": ""upload_tune_ds"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""tuning_dataset_bkt"" ], ""labels"": [ { ""text"": ""upload"" } ] }, { ""id"": ""upload_eval_ds"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""eval_dataset_bkt"" ], ""labels"": [ { ""text"": ""upload"" } ] }, { ""id"": ""upload_clinical_docs"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""clinical_docs_bkt"" ], ""labels"": [ { ""text"": ""upload"" } ] }, { ""id"": ""upload_um_policies"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""um_policies_bkt"" ], ""labels"": [ { ""text"": ""upload"" } ] }, { ""id"": ""c1_pa_upload"", ""sources"": [ ""pa_manager"" ], ""targets"": [ ""pa_forms_bkt"" ], ""labels"": [ { ""text"": ""PA forms"" } ] }, { ""id"": ""c5_hitl_ui"", ""sources"": [ ""pa_manager"" ], ""targets"": [ ""hitl_app"" ], ""labels"": [ { ""text"": ""HITL UI"" } ] }, { ""id"": ""r1"", ""sources"": [ ""ur_specialist"" ], ""targets"": [ ""ur_app"" ], ""labels"": [ { ""text"": ""R1"" } ] }, { ""id"": ""r4"", ""sources"": [ ""ur_specialist"" ], ""targets"": [ ""ur_app"" ], ""labels"": [ { ""text"": ""R4"" } ] }, { ""id"": ""r7"", ""sources"": [ ""ur_specialist"" ], ""targets"": [ ""ur_search"" ], ""labels"": [ { ""text"": ""R7"" } ] } ] }"
gcp,ai-ml,model-training,https://cloud.google.com/architecture/cross-silo-cross-device-federated-learning-google-cloud#cross-silo_architecture,"The preceding diagram shows a simplistic example of a cross-silo architecture. In the diagram, all of the resources are in the same project in a Google Cloud organization. These resources include the local client model, the global client model, and their associated federated learning workloads. This reference architecture can be modified to support several configurations for data silos. Members of the consortium can host their data silos in the following ways: On Google Cloud, in the same Google Cloud organization, and same Google Cloud project. On Google Cloud, in the same Google Cloud organization, in different Google Cloud projects. On Google Cloud, in different Google Cloud organizations. In private, on-premises environments, or in other public clouds. For participating members to collaborate, they need to establish secure communication channels between their environments. For more information about the role of participating members in the federated learning effort, how they collaborate, and what they share with each other, see Use cases. The architecture includes the following components: A Virtual Private Cloud (VPC) network and subnet. A private GKE cluster that helps you to do the following: Isolate cluster nodes from the internet. Limit exposure of your cluster nodes and control plane to the internet by creating a private GKE cluster with authorized networks. Use shielded cluster nodes that use a hardened operating system image. Enable Dataplane V2 for optimized Kubernetes networking. Dedicated GKE node pools: You create a dedicated node pool to exclusively host tenant apps and resources. The nodes have taints to ensure that only tenant workloads are scheduled onto the tenant nodes. Other cluster resources are hosted in the main node pool. Data encryption (enabled by default): Data at rest. Data in transit. Cluster secrets at the application layer. In-use data encryption, by optionally enabling Confidential Google Kubernetes Engine Nodes. VPC Firewall rules which apply the following: Baseline rules that apply to all nodes in the cluster. Additional rules that only apply to nodes in the tenant node pool. These firewall rules limit ingress to and egress from tenant nodes. Cloud NAT to allow egress to the internet. Cloud DNS records to enable Private Google Access such that apps within the cluster can access Google APIs without going over the internet. Service accounts which are as follows: A dedicated service account for the nodes in the tenant node pool. A dedicated service account for tenant apps to use with Workload Identity Federation. Support for using Google Groups for Kubernetes role-based access control (RBAC). A Git repository to store configuration descriptors. An Artifact Registry repository to store container images. Config Sync and Policy Controller to deploy configuration and policies. Cloud Service Mesh gateways to selectively allow cluster ingress and egress traffic. Cloud Storage buckets to store global and local model weights. Access to other Google and Google Cloud APIs. For example, a training workload might need to access training data that's stored in Cloud Storage, BigQuery, or Cloud SQL.","{ ""id"": ""root"", ""children"": [ { ""id"": ""external_repo"", ""labels"": [ { ""text"": ""external_repo"" } ], ""children"": [ { ""id"": ""git_repo"", ""labels"": [ { ""text"": ""Cluster config repo"" } ], ""children"": [], ""data"": { ""label"": ""Cluster config repo"", ""icon"": ""third_party_api"" } } ], ""edges"": [] }, { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""project"", ""labels"": [ { ""text"": ""project"" } ], ""children"": [ { ""id"": ""networking"", ""labels"": [ { ""text"": ""networking"" } ], ""children"": [ { ""id"": ""cloud_lb"", ""labels"": [ { ""text"": ""Cloud Load Balancer"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Load Balancer"", ""icon"": ""gcp_cloud_load_balancing"" } }, { ""id"": ""cloud_fw"", ""labels"": [ { ""text"": ""Cloud Firewall"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Firewall"", ""icon"": ""gcp_cloud_firewall_rules"" } }, { ""id"": ""cloud_nat"", ""labels"": [ { ""text"": ""Cloud NAT"" } ], ""children"": [], ""data"": { ""label"": ""Cloud NAT"", ""icon"": ""gcp_cloud_nat"" } } ], ""edges"": [ { ""id"": ""lb_fw"", ""sources"": [ ""cloud_lb"" ], ""targets"": [ ""cloud_fw"" ], ""labels"": [ { ""text"": ""inbound"" } ] } ] }, { ""id"": ""storage"", ""labels"": [ { ""text"": ""storage"" } ], ""children"": [ { ""id"": ""global_weights"", ""labels"": [ { ""text"": ""Global model weights"" } ], ""children"": [], ""data"": { ""label"": ""Global model weights"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""local_weights"", ""labels"": [ { ""text"": ""Local model weights"" } ], ""children"": [], ""data"": { ""label"": ""Local model weights"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""artifact_reg"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""data"": { ""label"": ""Artifact Registry"", ""icon"": ""gcp_artifact_registry"" } }, { ""id"": ""google_apis"", ""labels"": [ { ""text"": ""Google APIs / services"" } ], ""children"": [], ""data"": { ""label"": ""Google APIs / services"", ""icon"": ""third_party_api"" } } ], ""edges"": [] }, { ""id"": ""gke_cluster"", ""labels"": [ { ""text"": ""Private GKE cluster"" } ], ""children"": [ { ""id"": ""main_node_pool"", ""labels"": [ { ""text"": ""main_node_pool"" } ], ""children"": [ { ""id"": ""main_namespace"", ""labels"": [ { ""text"": ""main_namespace"" } ], ""children"": [ { ""id"": ""gateway"", ""labels"": [ { ""text"": ""gateway"" } ], ""children"": [ { ""id"": ""egress_gw"", ""labels"": [ { ""text"": ""Egress gateway"" } ], ""children"": [], ""data"": { ""label"": ""Egress gateway"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""ingress_gw"", ""labels"": [ { ""text"": ""Ingress gateway"" } ], ""children"": [], ""data"": { ""label"": ""Ingress gateway"", ""icon"": ""gcp_google_kubernetes_engine"" } } ], ""edges"": [] }, { ""id"": ""config_policy_group"", ""labels"": [ { ""text"": ""config_policy_group"" } ], ""children"": [ { ""id"": ""config_sync"", ""labels"": [ { ""text"": ""Config Sync"" } ], ""children"": [], ""data"": { ""label"": ""Config Sync"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""policy_ctrl"", ""labels"": [ { ""text"": ""Policy controller"" } ], ""children"": [], ""data"": { ""label"": ""Policy controller"", ""icon"": ""gcp_google_kubernetes_engine"" } } ], ""edges"": [] }, { ""id"": ""global_train"", ""labels"": [ { ""text"": ""Global training workloads"" } ], ""children"": [], ""data"": { ""label"": ""Global training workloads"", ""icon"": ""gcp_google_kubernetes_engine"" } } ], ""edges"": [] } ], ""edges"": [] }, { ""id"": ""training_node_pool"", ""labels"": [ { ""text"": ""training_node_pool"" } ], ""children"": [ { ""id"": ""local_training_namespace"", ""labels"": [ { ""text"": ""local_training_namespace"" } ], ""children"": [ { ""id"": ""local_train"", ""labels"": [ { ""text"": ""Local training workloads"" } ], ""children"": [], ""data"": { ""label"": ""Local training workloads"", ""icon"": ""gcp_google_kubernetes_engine"" } } ], ""edges"": [] } ], ""edges"": [] } ], ""data"": { ""label"": ""Private GKE cluster"", ""icon"": ""gcp_google_kubernetes_engine"" } } ], ""edges"": [ { ""id"": ""fw_ing"", ""sources"": [ ""cloud_fw"" ], ""targets"": [ ""ingress_gw"" ], ""labels"": [ { ""text"": ""inbound"" } ] }, { ""id"": ""eg_nat"", ""sources"": [ ""egress_gw"" ], ""targets"": [ ""cloud_nat"" ], ""labels"": [ { ""text"": ""outbound"" } ] }, { ""id"": ""gt_save"", ""sources"": [ ""global_train"" ], ""targets"": [ ""global_weights"" ], ""labels"": [ { ""text"": ""save weights"" } ] }, { ""id"": ""lt_save"", ""sources"": [ ""local_train"" ], ""targets"": [ ""local_weights"" ], ""labels"": [ { ""text"": ""save weights"" } ] }, { ""id"": ""gt_pull"", ""sources"": [ ""global_train"" ], ""targets"": [ ""artifact_reg"" ], ""labels"": [ { ""text"": ""pull images"" } ] }, { ""id"": ""lt_pull"", ""sources"": [ ""local_train"" ], ""targets"": [ ""artifact_reg"" ], ""labels"": [ { ""text"": ""pull images"" } ] }, { ""id"": ""lt_api"", ""sources"": [ ""local_train"" ], ""targets"": [ ""google_apis"" ], ""labels"": [ { ""text"": ""call APIs"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" } } ], ""edges"": [ { ""id"": ""git_cfg"", ""sources"": [ ""git_repo"" ], ""targets"": [ ""config_sync"" ], ""labels"": [ { ""text"": ""config sync"" } ] }, { ""id"": ""git_pol"", ""sources"": [ ""git_repo"" ], ""targets"": [ ""policy_ctrl"" ], ""labels"": [ { ""text"": ""policies"" } ] } ] }"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/cross-silo-cross-device-federated-learning-google-cloud#cross-silo_architecture,"Cross-device architecture The following diagram shows an architecture that supports cross-device federated learning: Cross-device architecture, components explained in following text. The preceding cross-device architecture builds upon the cross-silo architecture with the addition of the following components: A Cloud Run service that simulates devices connecting to the server A Certificate Authority Service that creates private certificates for the server and clients to run A Vertex AI TensorBoard to visualize the result of the training A Cloud Storage bucket to store the consolidated model The private GKE cluster that uses confidential nodes as its primary pool to help secure the data in use The cross-device architecture uses components from the open source Federated Compute Platform (FCP) project. This project includes the following: Client code for communicating with a server and executing tasks on the devices A protocol for client-server communication Connection points with TensorFlow Federated to make it easier to define your federated computations The FCP components shown in the preceding diagram can be deployed as a set of microservices. These components do the following: Aggregator: This job reads device gradients and calculates aggregated result with Differential Privacy. Collector: This job runs periodically to query active tasks and encrypted gradients. This information determines when aggregation starts. Model uploader: This job listens to events and publishes results so that devices can download updated models. Task-assignment: This frontend service distributes training tasks to devices. Task-management: This job manages tasks. Task-scheduler: This job either runs periodically or is triggered by specific events.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""project"", ""labels"": [ { ""text"": ""project"" } ], ""children"": [ { ""id"": ""networking"", ""labels"": [ { ""text"": ""networking"" } ], ""children"": [ { ""id"": ""cloud_lb"", ""labels"": [ { ""text"": ""Server ingress LB"" } ], ""children"": [], ""data"": { ""label"": ""Server ingress LB"", ""icon"": ""gcp_cloud_load_balancing"" } }, { ""id"": ""cloud_fw"", ""labels"": [ { ""text"": ""Cloud Firewall"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Firewall"", ""icon"": ""gcp_cloud_firewall_rules"" } }, { ""id"": ""cloud_nat"", ""labels"": [ { ""text"": ""Cloud NAT"" } ], ""children"": [], ""data"": { ""label"": ""Cloud NAT"", ""icon"": ""gcp_cloud_nat"" } } ], ""edges"": [ { ""id"": ""lb_fw"", ""sources"": [ ""cloud_lb"" ], ""targets"": [ ""cloud_fw"" ], ""labels"": [ { ""text"": ""pass"" } ] } ] }, { ""id"": ""storage"", ""labels"": [ { ""text"": ""storage"" } ], ""children"": [ { ""id"": ""models_bkt"", ""labels"": [ { ""text"": ""Models bucket"" } ], ""children"": [], ""data"": { ""label"": ""Models bucket"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""artifact_reg"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""data"": { ""label"": ""Artifact Registry"", ""icon"": ""gcp_artifact_registry"" } }, { ""id"": ""spanner"", ""labels"": [ { ""text"": ""Cloud Spanner"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Spanner"", ""icon"": ""gcp_spanner"" } } ], ""edges"": [] }, { ""id"": ""security_ops"", ""labels"": [ { ""text"": ""security_ops"" } ], ""children"": [ { ""id"": ""tenant_sa"", ""labels"": [ { ""text"": ""Tenant SA (IAM)"" } ], ""children"": [], ""data"": { ""label"": ""Tenant SA (IAM)"", ""icon"": ""gcp_identity_and_access_management"" } }, { ""id"": ""kms"", ""labels"": [ { ""text"": ""KMS"" } ], ""children"": [], ""data"": { ""label"": ""KMS"", ""icon"": ""gcp_key_management_service"" } }, { ""id"": ""tensorboard"", ""labels"": [ { ""text"": ""Vertex Tensorboard"" } ], ""children"": [], ""data"": { ""label"": ""Vertex Tensorboard"", ""icon"": ""gcp_ai_platform"" } } ], ""edges"": [] }, { ""id"": ""pubsub"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""data"": { ""label"": ""Pub/Sub"", ""icon"": ""gcp_pubsub"" } }, { ""id"": ""gke_cluster"", ""labels"": [ { ""text"": ""Private GKE cluster"" } ], ""children"": [ { ""id"": ""conf_pool"", ""labels"": [ { ""text"": ""conf_pool"" } ], ""children"": [ { ""id"": ""aggregator"", ""labels"": [ { ""text"": ""Aggregator"" } ], ""children"": [], ""data"": { ""label"": ""Aggregator"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""model_uploader"", ""labels"": [ { ""text"": ""Model uploader"" } ], ""children"": [], ""data"": { ""label"": ""Model uploader"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""tb_uploader"", ""labels"": [ { ""text"": ""TB logs uploader"" } ], ""children"": [], ""data"": { ""label"": ""TB logs uploader"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""task_sched"", ""labels"": [ { ""text"": ""Task scheduler"" } ], ""children"": [], ""data"": { ""label"": ""Task scheduler"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""task_mgr"", ""labels"": [ { ""text"": ""Task manager"" } ], ""children"": [], ""data"": { ""label"": ""Task manager"", ""icon"": ""gcp_google_kubernetes_engine"" } } ], ""edges"": [ { ""id"": ""sched_mgr"", ""sources"": [ ""task_sched"" ], ""targets"": [ ""task_mgr"" ], ""labels"": [ { ""text"": ""assign"" } ] } ] } ], ""data"": { ""label"": ""Private GKE cluster"", ""icon"": ""gcp_google_kubernetes_engine"" } }, { ""id"": ""clients_srv"", ""labels"": [ { ""text"": ""Clients (Cloud Run)"" } ], ""children"": [], ""data"": { ""label"": ""Clients (Cloud Run)"", ""icon"": ""gcp_cloud_run"" } } ], ""edges"": [ { ""id"": ""cli_lb"", ""sources"": [ ""clients_srv"" ], ""targets"": [ ""cloud_lb"" ], ""labels"": [ { ""text"": ""HTTPS"" } ] }, { ""id"": ""fw_agg"", ""sources"": [ ""cloud_fw"" ], ""targets"": [ ""aggregator"" ], ""labels"": [ { ""text"": ""pass"" } ] }, { ""id"": ""agg_read"", ""sources"": [ ""models_bkt"" ], ""targets"": [ ""aggregator"" ], ""labels"": [ { ""text"": ""read weights"" } ] }, { ""id"": ""agg_write"", ""sources"": [ ""aggregator"" ], ""targets"": [ ""models_bkt"" ], ""labels"": [ { ""text"": ""save weights"" } ] }, { ""id"": ""upl_write"", ""sources"": [ ""model_uploader"" ], ""targets"": [ ""models_bkt"" ], ""labels"": [ { ""text"": ""upload model"" } ] }, { ""id"": ""agg_pub"", ""sources"": [ ""aggregator"" ], ""targets"": [ ""pubsub"" ], ""labels"": [ { ""text"": ""publish task"" } ] }, { ""id"": ""sub_sched"", ""sources"": [ ""pubsub"" ], ""targets"": [ ""task_sched"" ], ""labels"": [ { ""text"": ""pull task"" } ] }, { ""id"": ""mgr_spanner"", ""sources"": [ ""task_mgr"" ], ""targets"": [ ""spanner"" ], ""labels"": [ { ""text"": ""metadata"" } ] }, { ""id"": ""eg_nat"", ""sources"": [ ""aggregator"" ], ""targets"": [ ""cloud_nat"" ], ""labels"": [ { ""text"": ""egress"" } ] }, { ""id"": ""nat_art"", ""sources"": [ ""cloud_nat"" ], ""targets"": [ ""artifact_reg"" ], ""labels"": [ { ""text"": ""pull images"" } ] }, { ""id"": ""edge_tb_uploader_to_tensorboard"", ""sources"": [ ""tb_uploader"" ], ""targets"": [ ""tensorboard"" ], ""labels"": [ { ""text"": ""send logs"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" } } ] }"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/blueprints/genai-mlops-blueprint#interactive-environment,"Operations This section describes the environments that are included in the blueprint. Interactive environment To let you explore data and develop models while maintaining your organization's security posture, the interactive environment provides you with a controlled set of actions you can perform. You can deploy Google Cloud resources using one of the following methods: Using Service Catalog, which is preconfigured through automation with resource templates Building code artifacts and committing them to Git repositories using Vertex AI Workbench notebooks The following diagram depicts the interactive environment. The blueprint interactive environment. A typical interactive flow has the following steps and components associated with it: Service Catalog provides a curated list of Google Cloud resources that data scientists can deploy into the interactive environment. The data scientist deploys the Vertex AI Workbench notebook resource from the Service Catalog. Vertex AI Workbench notebooks are the main interface that data scientists use to work with Google Cloud resources that are deployed in the interactive environment. The notebooks enable data scientists to pull their code from Git and update their code as needed. Source data is stored outside of the interactive environment and managed separately from this blueprint. Access to the data is controlled by a data owner. Data scientists can request read access to source data, but data scientists can't write to the source data. Data scientists can transfer source data into the interactive environment into resources created through the Service Catalog. In the interactive environment, data scientists can read, write, and manipulate the data. However, data scientists can't transfer data out of the interactive environment or grant access to resources that are created by Service Catalog. BigQuery stores structured data and semi-structured data and Cloud Storage stores unstructured data. Feature Store provides data scientists with low-latency access to features for model training. Data scientists train models using Vertex AI custom training jobs. The blueprint also uses Vertex AI for hyperparameter tuning. Data scientists evaluate models through the use of Vertex AI Experiments and Vertex AI TensorBoard. Vertex AI Experiments lets you run multiple trainings against a model using different parameters, modeling techniques, architectures, and inputs. Vertex AI TensorBoard lets you track, visualize, and compare the various experiments that you ran and then choose the model with the best observed characteristics to validate. Data scientists validate their models with Vertex AI evaluation. To validate their models, data scientists split the source data into a training data set and a validation data set and run a Vertex AI evaluation against your model. Data scientists build containers using Cloud Build, store the containers in Artifact Registry, and use the containers in pipelines that are in the operational environment.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""svc_catalog_project"", ""labels"": [ { ""text"": ""svc_catalog_project"" } ], ""children"": [ { ""id"": ""service_catalog"", ""labels"": [ { ""text"": ""Service Catalog"" } ], ""children"": [], ""data"": { ""label"": ""Service Catalog"", ""icon"": ""gcp_container_registry"" } } ], ""edges"": [] }, { ""id"": ""ci_cd"", ""labels"": [ { ""text"": ""ci_cd"" } ], ""children"": [ { ""id"": ""git_repo"", ""labels"": [ { ""text"": ""Git"" } ], ""children"": [], ""data"": { ""label"": ""Git"", ""icon"": ""third_party_api"" } }, { ""id"": ""cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Build"", ""icon"": ""gcp_cloud_build"" } }, { ""id"": ""artifact_reg"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""data"": { ""label"": ""Artifact Registry"", ""icon"": ""gcp_artifact_registry"" } } ], ""edges"": [ { ""id"": ""git_push_trigger_build"", ""sources"": [ ""git_repo"" ], ""targets"": [ ""cloud_build"" ], ""labels"": [ { ""text"": ""push triggers build"" } ] }, { ""id"": ""build_stores_image"", ""sources"": [ ""cloud_build"" ], ""targets"": [ ""artifact_reg"" ], ""labels"": [ { ""text"": ""store image"" } ] } ] }, { ""id"": ""source_data_project"", ""labels"": [ { ""text"": ""source_data_project"" } ], ""children"": [ { ""id"": ""src_bigquery"", ""labels"": [ { ""text"": ""BigQuery (source)"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery (source)"", ""icon"": ""gcp_bigquery"" } } ], ""edges"": [] }, { ""id"": ""interactive_project"", ""labels"": [ { ""text"": ""interactive_project"" } ], ""children"": [ { ""id"": ""vertex_ai"", ""labels"": [ { ""text"": ""vertex_ai"" } ], ""children"": [ { ""id"": ""notebook"", ""labels"": [ { ""text"": ""Workbench Notebook"" } ], ""children"": [], ""data"": { ""label"": ""Workbench Notebook"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""int_bigquery"", ""labels"": [ { ""text"": ""BigQuery (interactive)"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery (interactive)"", ""icon"": ""gcp_bigquery"" } }, { ""id"": ""feature_store"", ""labels"": [ { ""text"": ""Feature Store"" } ], ""children"": [], ""data"": { ""label"": ""Feature Store"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""model_train"", ""labels"": [ { ""text"": ""Model training"" } ], ""children"": [], ""data"": { ""label"": ""Model training"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""tensorboard_eval"", ""labels"": [ { ""text"": ""TensorBoard evaluation"" } ], ""children"": [], ""data"": { ""label"": ""TensorBoard evaluation"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""model_validation"", ""labels"": [ { ""text"": ""Model validation"" } ], ""children"": [], ""data"": { ""label"": ""Model validation"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""interactive_bq_to_feature_store"", ""sources"": [ ""int_bigquery"" ], ""targets"": [ ""feature_store"" ], ""labels"": [ { ""text"": ""publish features"" } ] }, { ""id"": ""notebook_triggers_training"", ""sources"": [ ""notebook"" ], ""targets"": [ ""model_train"" ], ""labels"": [ { ""text"": ""start training"" } ] }, { ""id"": ""features_to_training"", ""sources"": [ ""feature_store"" ], ""targets"": [ ""model_train"" ], ""labels"": [ { ""text"": ""training data"" } ] }, { ""id"": ""training_to_tensorboard"", ""sources"": [ ""model_train"" ], ""targets"": [ ""tensorboard_eval"" ], ""labels"": [ { ""text"": ""send metrics"" } ] }, { ""id"": ""tensorboard_to_validation"", ""sources"": [ ""tensorboard_eval"" ], ""targets"": [ ""model_validation"" ], ""labels"": [ { ""text"": ""model candidate"" } ] } ] } ], ""edges"": [] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" }, ""edges"": [ { ""id"": ""source_to_notebook_query"", ""sources"": [ ""src_bigquery"" ], ""targets"": [ ""notebook"" ], ""labels"": [ { ""text"": ""query sample data"" } ] }, { ""id"": ""source_to_interactive_bq"", ""sources"": [ ""src_bigquery"" ], ""targets"": [ ""int_bigquery"" ], ""labels"": [ { ""text"": ""load full data"" } ] }, { ""id"": ""edge_notebook_to_git"", ""sources"": [ ""notebook"" ], ""targets"": [ ""git_repo"" ], ""labels"": [ { ""text"": ""commit code"" } ] } ] }, { ""id"": ""user_group"", ""labels"": [ { ""text"": ""user_group"" } ], ""children"": [ { ""id"": ""data_scientist"", ""labels"": [ { ""text"": ""Data scientist"" } ], ""children"": [], ""data"": { ""label"": ""Data scientist"", ""icon"": ""browser_client"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""user_to_notebook"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""notebook"" ], ""labels"": [ { ""text"": ""open notebook"" } ] }, { ""id"": ""edge_data_scientist_to_service_catalog"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""service_catalog"" ], ""labels"": [ { ""text"": ""access catalog"" } ] } ] }"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/blueprints/genai-mlops-blueprint#interactive-environment,"Operational environment The operational environment uses a Git repository and pipelines. This environment includes the production environment and non-production environment of the enterprise foundation blueprint. In the non-production environment, the data scientist selects a pipeline from one of the pipelines that was developed in the interactive environment. The data scientist can run the pipeline in the non-production environment, evaluate the results, and then determine which model to promote into the production environment. The blueprint includes an example pipeline that was built using Cloud Composer and an example pipeline that was built using Vertex AI Pipelines. The diagram below shows the operational environment. The blueprint operational environment. A typical operational flow has the following steps: A data scientist merges a development branch successfully into a deployment branch. The merge into the deployment branch triggers a Cloud Build pipeline. One of the following items occurs: If a data scientist is using Cloud Composer as the orchestrator, the Cloud Build pipeline moves a DAG into Cloud Storage. If the data scientist is using Vertex AI Pipelines as the orchestrator, the pipeline moves a Python file into Cloud Storage. The Cloud Build pipeline triggers the orchestrator (Cloud Composer or Vertex AI Pipelines). The orchestrator pulls its pipeline definition from Cloud Storage and begins to execute the pipeline. The pipeline pulls a container from Artifact Registry that is used by all stages of the pipeline to trigger Vertex AI services. The pipeline, using the container, triggers a data transfer from the source data project into the operational environment. Data is transformed, validated, split, and prepared for model training and validation by the pipeline. If needed, the pipeline moves data into Vertex AI Feature Store for easy access during model training. The pipeline uses Vertex AI custom model training to train the model. The pipeline uses Vertex AI evaluation to validate the model. A validated model is imported into the Model Registry by the pipeline. The imported model is then used to generate predictions through online predictions or batch predictions. After the model is deployed into the production environment, the pipeline uses Vertex AI Model Monitoring to detect if the model's performance degrades by monitoring for training-serving skew and prediction drift. Deployment The blueprint uses a series of Cloud Build pipelines to provision the blueprint infrastructure, the pipeline in the operational environment, and the containers used to create generative AI and ML models. The pipelines used and the resources provisioned are the following: Infrastructure pipeline: This pipeline is part of the enterprise foundation blueprint. This pipeline provisions the Google Cloud resources that are associated with the interactive environment and operational environment. Interactive pipeline: The interactive pipeline is part of the interactive environment. This pipeline copies Terraform templates from a Git repository to a Cloud Storage bucket that Service Catalog can read. The interactive pipeline is triggered when a pull request is made to merge with the main branch. Container pipeline: The blueprint includes a Cloud Build pipeline to build containers used in the operational pipeline. Containers that are deployed across environments are immutable container images. Immutable container images help ensure that the same image is deployed across all environments and cannot be modified while they are running. If you need to modify the application, you must rebuild and redeploy the image. Container images that are used in the blueprint are stored in Artifact Registry and referenced by the configuration files that are used in the operational pipeline. Operational pipeline: The operational pipeline is part of the operational environment. This pipeline copies DAGs for Cloud Composer or Vertex AI Pipelines, which are then used to build, test, and deploy models.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""ci_cd"", ""labels"": [ { ""text"": ""ci_cd"" } ], ""children"": [ { ""id"": ""git_repo"", ""labels"": [ { ""text"": ""Git"" } ], ""children"": [], ""data"": { ""label"": ""Git"", ""icon"": ""third_party_api"" } }, { ""id"": ""cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Build"", ""icon"": ""gcp_cloud_build"" } }, { ""id"": ""ci_bucket"", ""labels"": [ { ""text"": ""Pipeline bucket"" } ], ""children"": [], ""data"": { ""label"": ""Pipeline bucket"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""artifact_reg"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""data"": { ""label"": ""Artifact Registry"", ""icon"": ""gcp_artifact_registry"" } } ], ""edges"": [ { ""id"": ""git_to_build"", ""sources"": [ ""git_repo"" ], ""targets"": [ ""cloud_build"" ], ""labels"": [ { ""text"": ""trigger build"" } ] }, { ""id"": ""build_to_bucket"", ""sources"": [ ""cloud_build"" ], ""targets"": [ ""ci_bucket"" ], ""labels"": [ { ""text"": ""store pipeline spec"" } ] }, { ""id"": ""build_to_artifact"", ""sources"": [ ""cloud_build"" ], ""targets"": [ ""artifact_reg"" ], ""labels"": [ { ""text"": ""push container"" } ] } ] }, { ""id"": ""svc_catalog_project"", ""labels"": [ { ""text"": ""svc_catalog_project"" } ], ""children"": [ { ""id"": ""service_catalog"", ""labels"": [ { ""text"": ""Service Catalog"" } ], ""children"": [], ""data"": { ""label"": ""Service Catalog"", ""icon"": ""gcp_service_catalog"" } } ], ""edges"": [] }, { ""id"": ""source_data_project"", ""labels"": [ { ""text"": ""source_data_project"" } ], ""children"": [ { ""id"": ""src_bq"", ""labels"": [ { ""text"": ""BigQuery (source)"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery (source)"", ""icon"": ""gcp_bigquery"" } } ], ""edges"": [] }, { ""id"": ""operational_project"", ""labels"": [ { ""text"": ""operational_project"" } ], ""children"": [ { ""id"": ""notebook"", ""labels"": [ { ""text"": ""Workbench Notebook"" } ], ""children"": [], ""data"": { ""label"": ""Workbench Notebook"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""op_bq"", ""labels"": [ { ""text"": ""BigQuery (operational)"" } ], ""children"": [], ""data"": { ""label"": ""BigQuery (operational)"", ""icon"": ""gcp_bigquery"" } }, { ""id"": ""dataform"", ""labels"": [ { ""text"": ""Dataform"" } ], ""children"": [], ""data"": { ""label"": ""Dataform"", ""icon"": ""gcp_dataform"" } }, { ""id"": ""feature_store"", ""labels"": [ { ""text"": ""Feature Store"" } ], ""children"": [], ""data"": { ""label"": ""Feature Store"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""model_train"", ""labels"": [ { ""text"": ""Model training"" } ], ""children"": [], ""data"": { ""label"": ""Model training"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""tensorboard"", ""labels"": [ { ""text"": ""TensorBoard eval"" } ], ""children"": [], ""data"": { ""label"": ""TensorBoard eval"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""model_registry"", ""labels"": [ { ""text"": ""Model Registry"" } ], ""children"": [], ""data"": { ""label"": ""Model Registry"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""metadata_svc"", ""labels"": [ { ""text"": ""Metadata"" } ], ""children"": [], ""data"": { ""label"": ""Metadata"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""endpoint"", ""labels"": [ { ""text"": ""Endpoint"" } ], ""children"": [], ""data"": { ""label"": ""Endpoint"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""batch_pred"", ""labels"": [ { ""text"": ""Batch prediction"" } ], ""children"": [], ""data"": { ""label"": ""Batch prediction"", ""icon"": ""gcp_vertexai"" } }, { ""id"": ""model_monitor"", ""labels"": [ { ""text"": ""Model monitoring"" } ], ""children"": [], ""data"": { ""label"": ""Model monitoring"", ""icon"": ""gcp_vertexai"" } } ], ""edges"": [ { ""id"": ""prep_with_dataform"", ""sources"": [ ""op_bq"" ], ""targets"": [ ""dataform"" ], ""labels"": [ { ""text"": ""validate & prep"" } ] }, { ""id"": ""dataform_to_features"", ""sources"": [ ""dataform"" ], ""targets"": [ ""feature_store"" ], ""labels"": [ { ""text"": ""publish features"" } ] }, { ""id"": ""features_to_training"", ""sources"": [ ""feature_store"" ], ""targets"": [ ""model_train"" ], ""labels"": [ { ""text"": ""train input"" } ] }, { ""id"": ""train_to_tb"", ""sources"": [ ""model_train"" ], ""targets"": [ ""tensorboard"" ], ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""train_to_meta"", ""sources"": [ ""model_train"" ], ""targets"": [ ""metadata_svc"" ], ""labels"": [ { ""text"": ""artifacts"" } ] }, { ""id"": ""tensorboard_to_registry"", ""sources"": [ ""tensorboard"" ], ""targets"": [ ""model_registry"" ], ""labels"": [ { ""text"": ""register model"" } ] }, { ""id"": ""registry_to_endpoint"", ""sources"": [ ""model_registry"" ], ""targets"": [ ""endpoint"" ], ""labels"": [ { ""text"": ""deploy online"" } ] }, { ""id"": ""registry_to_batch"", ""sources"": [ ""model_registry"" ], ""targets"": [ ""batch_pred"" ], ""labels"": [ { ""text"": ""batch scoring"" } ] }, { ""id"": ""endpoint_to_monitor"", ""sources"": [ ""endpoint"" ], ""targets"": [ ""model_monitor"" ], ""labels"": [ { ""text"": ""monitor skew"" } ] }, { ""id"": ""batch_to_monitor"", ""sources"": [ ""batch_pred"" ], ""targets"": [ ""model_monitor"" ], ""labels"": [ { ""text"": ""monitor drift"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" }, ""edges"": [ { ""id"": ""pipeline_ingest_data"", ""sources"": [ ""src_bq"" ], ""targets"": [ ""op_bq"" ], ""labels"": [ { ""text"": ""data transfer"" } ] } ] }, { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""data_scientist"", ""labels"": [ { ""text"": ""Data scientist"" } ], ""children"": [], ""data"": { ""label"": ""Data scientist"", ""icon"": ""browser_client"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""user_to_git"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""git_repo"" ], ""labels"": [ { ""text"": ""commit/merge"" } ] }, { ""id"": ""user_to_catalog"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""service_catalog"" ], ""labels"": [ { ""text"": ""browse solutions"" } ] }, { ""id"": ""user_to_notebook"", ""sources"": [ ""data_scientist"" ], ""targets"": [ ""notebook"" ], ""labels"": [ { ""text"": ""interactive work"" } ] } ] }"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/security/confidential-computing-analytics-ai#confidential-analytics,"Confidential analytics architecture for healthcare institutions The confidential analytics architecture demonstrates how multiple healthcare institutions (such as providers, biopharmaceutical, and research institutions) can work together to accelerate drug research. This architecture uses confidential computing techniques to create a digital clean room for running confidential collaborative analytics. This architecture has the following benefits: Enhanced insights: Collaborative analytics lets health organizations gain broader insights and decrease time to market for enhanced drug discovery. Data privacy: Sensitive transaction data remains encrypted and is never exposed to other participants or the TEE, ensuring confidentiality. Regulatory compliance: The architecture helps health institutions comply with data protection regulations by maintaining strict control over their data. Trust and collaboration: The architecture enables secure collaboration between competing institutions, fostering a collective effort to discover drugs. The following diagram shows this architecture. Diagram of confidential analytics architecture for healthcare institutions. The key components in this architecture include the following: TEE OLAP aggregation server: A secure, isolated environment where machine learning model training and inference occur. Data and code within the TEE are protected from unauthorized access, even from the underlying operating system or cloud provider. Collaboration partners: Each participating health institution has a local environment that acts as an intermediary between the institution's private data and the TEE. Provider-specific encrypted data: Each healthcare institution stores its own private, encrypted patient data that includes electronic health records. This data remains encrypted during the analytics process, which ensures data privacy. The data is only released to the TEE after validating the attestation claims from the individual providers. Analytics client: Participating health institutions can run confidential queries against their data to gain immediate insights.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""gcp_region"", ""labels"": [ { ""text"": ""gcp_region"" } ], ""children"": [ { ""id"": ""model_repo"", ""labels"": [ { ""text"": ""Model repo (GCS)"" } ], ""children"": [], ""data"": { ""label"": ""Model repo (GCS)"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""config_bucket"", ""labels"": [ { ""text"": ""Config bucket"" } ], ""children"": [], ""data"": { ""label"": ""Config bucket"", ""icon"": ""gcp_cloud_storage"" } }, { ""id"": ""cloud_logs"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Logging"", ""icon"": ""gcp_cloud_logging"" } }, { ""id"": ""cloud_router"", ""labels"": [ { ""text"": ""Cloud Router"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Router"", ""icon"": ""gcp_cloud_router"" } }, { ""id"": ""cloud_lb"", ""labels"": [ { ""text"": ""Cloud Load Balancer"" } ], ""children"": [], ""data"": { ""label"": ""Cloud Load Balancer"", ""icon"": ""gcp_cloud_load_balancing"" } }, { ""id"": ""cross_cloud_ic"", ""labels"": [ { ""text"": ""Cross-Cloud Interconnect"" } ], ""children"": [], ""data"": { ""label"": ""Cross-Cloud Interconnect"", ""icon"": ""gcp_cloud_interconnect"" } }, { ""id"": ""cloud_vpn"", ""labels"": [ { ""text"": ""Cloud VPN"" } ], ""children"": [], ""data"": { ""label"": ""Cloud VPN"", ""icon"": ""gcp_cloud_vpn"" } }, { ""id"": ""gcp_pub_vpc"", ""labels"": [ { ""text"": ""gcp_pub_vpc"" } ], ""children"": [ { ""id"": ""gcp_nat"", ""labels"": [ { ""text"": ""Cloud NAT"" } ], ""children"": [], ""data"": { ""label"": ""Cloud NAT"", ""icon"": ""gcp_cloud_nat"" } } ], ""edges"": [] }, { ""id"": ""conf_space"", ""labels"": [ { ""text"": ""conf_space"" } ], ""children"": [ { ""id"": ""tee_olap"", ""labels"": [ { ""text"": ""TEE OLAP agg"" } ], ""children"": [], ""data"": { ""label"": ""TEE OLAP agg"", ""icon"": ""gcp_compute_engine"" } }, { ""id"": ""tee_train"", ""labels"": [ { ""text"": ""TEE training"" } ], ""children"": [], ""data"": { ""label"": ""TEE training"", ""icon"": ""gcp_compute_engine"" } } ], ""edges"": [] }, { ""id"": ""attest_verifier"", ""labels"": [ { ""text"": ""Attestation verifier"" } ], ""children"": [], ""data"": { ""label"": ""Attestation verifier"", ""icon"": ""gcp_security"" } } ], ""edges"": [ { ""id"": ""lb_to_olap"", ""sources"": [ ""cloud_lb"" ], ""targets"": [ ""tee_olap"" ], ""labels"": [ { ""text"": ""forward"" } ] }, { ""id"": ""lb_to_train"", ""sources"": [ ""cloud_lb"" ], ""targets"": [ ""tee_train"" ], ""labels"": [ { ""text"": ""forward"" } ] }, { ""id"": ""ic_to_cloud_router"", ""sources"": [ ""cross_cloud_ic"" ], ""targets"": [ ""cloud_router"" ], ""labels"": [ { ""text"": ""route"" } ] }, { ""id"": ""vpn_to_cloud_router"", ""sources"": [ ""cloud_vpn"" ], ""targets"": [ ""cloud_router"" ], ""labels"": [ { ""text"": ""route"" } ] }, { ""id"": ""router_to_lb_internal"", ""sources"": [ ""cloud_router"" ], ""targets"": [ ""cloud_lb"" ], ""labels"": [ { ""text"": ""internal lb"" } ] }, { ""id"": ""cfg_to_olap"", ""sources"": [ ""config_bucket"" ], ""targets"": [ ""tee_olap"" ], ""labels"": [ { ""text"": ""config"" } ] }, { ""id"": ""cfg_to_train"", ""sources"": [ ""config_bucket"" ], ""targets"": [ ""tee_train"" ], ""labels"": [ { ""text"": ""config"" } ] }, { ""id"": ""olap_logs"", ""sources"": [ ""tee_olap"" ], ""targets"": [ ""cloud_logs"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""train_logs"", ""sources"": [ ""tee_train"" ], ""targets"": [ ""cloud_logs"" ], ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""olap_attest"", ""sources"": [ ""tee_olap"" ], ""targets"": [ ""attest_verifier"" ], ""labels"": [ { ""text"": ""quote"" } ] }, { ""id"": ""olap_to_gcp_nat"", ""sources"": [ ""tee_olap"" ], ""targets"": [ ""gcp_nat"" ], ""labels"": [ { ""text"": ""egress"" } ] }, { ""id"": ""train_to_gcp_nat"", ""sources"": [ ""tee_train"" ], ""targets"": [ ""gcp_nat"" ], ""labels"": [ { ""text"": ""egress"" } ] } ] } ], ""data"": { ""label"": ""Google Cloud"", ""icon"": ""gcp_logo"" } }, { ""id"": ""shared_repo"", ""labels"": [ { ""text"": ""shared_repo"" } ], ""children"": [ { ""id"": ""code_repo"", ""labels"": [ { ""text"": ""Code repo"" } ], ""children"": [], ""data"": { ""label"": ""Code repo"", ""icon"": ""third_party_api"" } }, { ""id"": ""model_repo_shared"", ""labels"": [ { ""text"": ""Model repo (shared)"" } ], ""children"": [], ""data"": { ""label"": ""Model repo (shared)"", ""icon"": ""third_party_api"" } } ], ""edges"": [] }, { ""id"": ""partner_cloud_region"", ""labels"": [ { ""text"": ""partner_cloud_region"" } ], ""children"": [ { ""id"": ""partner_priv_vpc"", ""labels"": [ { ""text"": ""partner_priv_vpc"" } ], ""children"": [ { ""id"": ""partner_enc_storage"", ""labels"": [ { ""text"": ""Encrypted storage"" } ], ""children"": [], ""data"": { ""label"": ""Encrypted storage"", ""icon"": ""database"" } }, { ""id"": ""partner_client"", ""labels"": [ { ""text"": ""Analytics client"" } ], ""children"": [], ""data"": { ""label"": ""Analytics client"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""partner_router"", ""labels"": [ { ""text"": ""Partner router"" } ], ""children"": [], ""data"": { ""label"": ""Partner router"", ""icon"": ""gcp_cloud_router"" } }, { ""id"": ""partner_pub_vpc"", ""labels"": [ { ""text"": ""partner_pub_vpc"" } ], ""children"": [ { ""id"": ""partner_nat"", ""labels"": [ { ""text"": ""Partner NAT"" } ], ""children"": [], ""data"": { ""label"": ""Partner NAT"", ""icon"": ""gcp_cloud_nat"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""partner_priv_to_nat"", ""sources"": [ ""partner_priv_vpc"" ], ""targets"": [ ""partner_nat"" ], ""labels"": [ { ""text"": ""egress"" } ] }, { ""id"": ""partner_router_priv"", ""sources"": [ ""partner_router"" ], ""targets"": [ ""partner_priv_vpc"" ], ""labels"": [ { ""text"": ""link"" } ] } ] }, { ""id"": ""partner_onprem_dc"", ""labels"": [ { ""text"": ""partner_onprem_dc"" } ], ""children"": [ { ""id"": ""onprem_priv_vpc"", ""labels"": [ { ""text"": ""onprem_priv_vpc"" } ], ""children"": [ { ""id"": ""onprem_enc_storage"", ""labels"": [ { ""text"": ""Encrypted storage"" } ], ""children"": [], ""data"": { ""label"": ""Encrypted storage"", ""icon"": ""database"" } }, { ""id"": ""onprem_client"", ""labels"": [ { ""text"": ""Analytics client"" } ], ""children"": [], ""data"": { ""label"": ""Analytics client"", ""icon"": ""browser_client"" } } ], ""edges"": [] }, { ""id"": ""onprem_vpn"", ""labels"": [ { ""text"": ""VPN Gateway"" } ], ""children"": [], ""data"": { ""label"": ""VPN Gateway"", ""icon"": ""gcp_cloud_vpn"" } }, { ""id"": ""onprem_pub_vpc"", ""labels"": [ { ""text"": ""onprem_pub_vpc"" } ], ""children"": [ { ""id"": ""onprem_nat"", ""labels"": [ { ""text"": ""On-prem NAT"" } ], ""children"": [], ""data"": { ""label"": ""On-prem NAT"", ""icon"": ""gcp_cloud_nat"" } } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""onprem_priv_to_nat"", ""sources"": [ ""onprem_priv_vpc"" ], ""targets"": [ ""onprem_nat"" ], ""labels"": [ { ""text"": ""egress"" } ] }, { ""id"": ""onprem_vpn_priv"", ""sources"": [ ""onprem_vpn"" ], ""targets"": [ ""onprem_priv_vpc"" ], ""labels"": [ { ""text"": ""link"" } ] } ] } ], ""edges"": [ { ""id"": ""gcp_nat_repo"", ""sources"": [ ""gcp_nat"" ], ""targets"": [ ""model_repo_shared"" ], ""labels"": [ { ""text"": ""sync repo"" } ] }, { ""id"": ""partner_nat_repo"", ""sources"": [ ""partner_nat"" ], ""targets"": [ ""model_repo_shared"" ], ""labels"": [ { ""text"": ""sync repo"" } ] }, { ""id"": ""onprem_nat_repo"", ""sources"": [ ""onprem_nat"" ], ""targets"": [ ""model_repo_shared"" ], ""labels"": [ { ""text"": ""sync repo"" } ] }, { ""id"": ""onprem_vpn_to_lb"", ""sources"": [ ""onprem_vpn"" ], ""targets"": [ ""cloud_lb"" ], ""labels"": [ { ""text"": ""encrypted raw"" } ] }, { ""id"": ""olap_to_onprem_vpn"", ""sources"": [ ""tee_olap"" ], ""targets"": [ ""onprem_vpn"" ], ""labels"": [ { ""text"": ""aggregated"" } ] }, { ""id"": ""router_to_ic"", ""sources"": [ ""partner_router"" ], ""targets"": [ ""cross_cloud_ic"" ], ""labels"": [ { ""text"": ""link"" } ] } ] }"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/security/confidential-computing-analytics-ai#confidential-ai,"Data privacy: Sensitive transaction data remains encrypted and is never exposed to other participants or the TEE, ensuring confidentiality. Regulatory compliance: The architecture helps financial institutions comply with data protection regulations by maintaining strict control over their data. Trust and collaboration: This architecture enables secure collaboration between competing institutions, fostering a collective effort to combat financial fraud. The following diagram shows this architecture. Diagram of confidential analytics architecture for financial institutions. The key components of this architecture include the following: TEE OLAP aggregation server: A secure, isolated environment where machine learning model training and inference occur. Data and code within the TEE are protected from unauthorized access, even from the underlying operating system or cloud provider. TEE model training: The global fraud base model is packaged as containers to run the ML training. Within the TEE, the global model is further trained using the encrypted data from all participating banks. The training process employs techniques like federated learning or secure multi-party computation to ensure that no raw data is exposed. Collaborator partners: Each participating financial institution has a local environment that acts as an intermediary between the institution's private data and the TEE. Bank-specific encrypted data: Each bank holds its own private, encrypted transaction data that includes fraud labels. This data remains encrypted throughout the entire process, ensuring data privacy. The data is only released to the TEE after validating the attestation claims from individual banks. Model repository: A pre-trained fraud detection model that serves as the starting point for collaborative training. Global fraud trained model and weights (symbolized by the green line): The improved fraud detection model, along with its learned weights, is securely exchanged back to the participating banks. They can then deploy this enhanced model locally for fraud detection on their own transactions.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""gcp_region"",""labels"":[{""text"":""gcp_region""}],""children"":[{""id"":""model_repo"",""labels"":[{""text"":""Model repo (GCS)""}],""children"":[],""data"":{""label"":""Model repo (GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""config_bucket"",""labels"":[{""text"":""Config bucket""}],""children"":[],""data"":{""label"":""Config bucket"",""icon"":""gcp_cloud_storage""}},{""id"":""cloud_logs"",""labels"":[{""text"":""Cloud Logging""}],""children"":[],""data"":{""label"":""Cloud Logging"",""icon"":""gcp_cloud_logging""}},{""id"":""cloud_router"",""labels"":[{""text"":""Cloud Router""}],""children"":[],""data"":{""label"":""Cloud Router"",""icon"":""gcp_cloud_router""}},{""id"":""cross_cloud_ic"",""labels"":[{""text"":""Cross-Cloud Interconnect""}],""children"":[],""data"":{""label"":""Cross-Cloud Interconnect"",""icon"":""gcp_cloud_interconnect""}},{""id"":""cloud_vpn"",""labels"":[{""text"":""Cloud VPN""}],""children"":[],""data"":{""label"":""Cloud VPN"",""icon"":""gcp_cloud_vpn""}},{""id"":""gcp_pub_vpc"",""labels"":[{""text"":""gcp_pub_vpc""}],""children"":[{""id"":""gcp_nat"",""labels"":[{""text"":""Cloud NAT""}],""children"":[],""data"":{""label"":""Cloud NAT"",""icon"":""gcp_cloud_nat""}}],""edges"":[]},{""id"":""attest_verifier"",""labels"":[{""text"":""Attestation verifier""}],""children"":[],""data"":{""label"":""Attestation verifier"",""icon"":""gcp_security""}},{""id"":""private_vpc"",""labels"":[{""text"":""private_vpc""}],""children"":[{""id"":""cloud_lb"",""labels"":[{""text"":""Cloud Load Balancer""}],""children"":[],""data"":{""label"":""Cloud Load Balancer"",""icon"":""gcp_cloud_load_balancing""}},{""id"":""conf_space"",""labels"":[{""text"":""conf_space""}],""children"":[{""id"":""tee_olap"",""labels"":[{""text"":""TEE OLAP server""}],""children"":[],""data"":{""label"":""TEE OLAP server"",""icon"":""gcp_compute_engine""}},{""id"":""tee_train"",""labels"":[{""text"":""TEE model training""}],""children"":[],""data"":{""label"":""TEE model training"",""icon"":""gcp_compute_engine""}}],""edges"":[]}],""edges"":[{""id"":""lb_to_tee_train"",""sources"":[""cloud_lb""],""targets"":[""tee_train""],""labels"":[{""text"":""forward""}]},{""id"":""lb_to_tee_olap"",""sources"":[""cloud_lb""],""targets"":[""tee_olap""],""labels"":[{""text"":""forward""}]}]}],""edges"":[{""id"":""ic_to_cloud_router"",""sources"":[""cross_cloud_ic""],""targets"":[""cloud_router""],""labels"":[{""text"":""route""}]},{""id"":""cloud_vpn_to_router"",""sources"":[""cloud_vpn""],""targets"":[""cloud_router""],""labels"":[{""text"":""route""}]},{""id"":""router_to_lb"",""sources"":[""cloud_router""],""targets"":[""cloud_lb""],""labels"":[{""text"":""internal LB""}]},{""id"":""config_to_olap"",""sources"":[""config_bucket""],""targets"":[""tee_olap""],""labels"":[{""text"":""config""}]},{""id"":""olap_logs"",""sources"":[""tee_olap""],""targets"":[""cloud_logs""],""labels"":[{""text"":""logs""}]},{""id"":""olap_to_attest"",""sources"":[""tee_olap""],""targets"":[""attest_verifier""],""labels"":[{""text"":""quote""}]},{""id"":""olap_to_gcp_nat"",""sources"":[""tee_olap""],""targets"":[""gcp_nat""],""labels"":[{""text"":""egress""}]},{""id"":""config_to_train"",""sources"":[""config_bucket""],""targets"":[""tee_train""],""labels"":[{""text"":""config""}]},{""id"":""train_logs"",""sources"":[""tee_train""],""targets"":[""cloud_logs""],""labels"":[{""text"":""logs""}]},{""id"":""train_to_gcp_nat"",""sources"":[""tee_train""],""targets"":[""gcp_nat""],""labels"":[{""text"":""egress""}]}]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[]},{""id"":""shared_repo"",""labels"":[{""text"":""shared_repo""}],""children"":[{""id"":""code_repo"",""labels"":[{""text"":""Analytics code repo""}],""children"":[],""data"":{""label"":""Analytics code repo"",""icon"":""third_party_api""}},{""id"":""model_repo_shared"",""labels"":[{""text"":""Model repo (shared)""}],""children"":[],""data"":{""label"":""Model repo (shared)"",""icon"":""third_party_api""}}],""edges"":[]},{""id"":""partner_cloud_region"",""labels"":[{""text"":""partner_cloud_region""}],""children"":[{""id"":""partner_priv_vpc"",""labels"":[{""text"":""partner_priv_vpc""}],""children"":[{""id"":""partner_enc_storage"",""labels"":[{""text"":""Encrypted storage""}],""children"":[],""data"":{""label"":""Encrypted storage"",""icon"":""database""}},{""id"":""partner_iam"",""labels"":[{""text"":""IAM""}],""children"":[],""data"":{""label"":""IAM"",""icon"":""gcp_identity_and_access_management""}},{""id"":""partner_identity"",""labels"":[{""text"":""Identity federation""}],""children"":[],""data"":{""label"":""Identity federation"",""icon"":""third_party_api""}},{""id"":""partner_inference"",""labels"":[{""text"":""Local inference engine""}],""children"":[],""data"":{""label"":""Local inference engine"",""icon"":""gcp_compute_engine""}}],""edges"":[]},{""id"":""partner_router"",""labels"":[{""text"":""Partner router""}],""children"":[],""data"":{""label"":""Partner router"",""icon"":""gcp_cloud_router""}},{""id"":""partner_pub_vpc"",""labels"":[{""text"":""partner_pub_vpc""}],""children"":[{""id"":""partner_nat"",""labels"":[{""text"":""Partner NAT""}],""children"":[],""data"":{""label"":""Partner NAT"",""icon"":""gcp_cloud_nat""}}],""edges"":[]}],""edges"":[{""id"":""partner_priv_to_nat"",""sources"":[""partner_priv_vpc""],""targets"":[""partner_nat""],""labels"":[{""text"":""egress link""}]},{""id"":""partner_router_priv"",""sources"":[""partner_router""],""targets"":[""partner_priv_vpc""],""labels"":[{""text"":""internal link""}]}]},{""id"":""partner_onprem_dc"",""labels"":[{""text"":""partner_onprem_dc""}],""children"":[{""id"":""onprem_priv_vpc"",""labels"":[{""text"":""onprem_priv_vpc""}],""children"":[{""id"":""onprem_enc_storage"",""labels"":[{""text"":""Encrypted storage""}],""children"":[],""data"":{""label"":""Encrypted storage"",""icon"":""database""}},{""id"":""onprem_iam"",""labels"":[{""text"":""IAM""}],""children"":[],""data"":{""label"":""IAM"",""icon"":""gcp_identity_and_access_management""}},{""id"":""onprem_identity"",""labels"":[{""text"":""Identity federation""}],""children"":[],""data"":{""label"":""Identity federation"",""icon"":""third_party_api""}},{""id"":""onprem_inference"",""labels"":[{""text"":""Local inference engine""}],""children"":[],""data"":{""label"":""Local inference engine"",""icon"":""gcp_compute_engine""}}],""edges"":[]},{""id"":""onprem_vpn"",""labels"":[{""text"":""VPN Gateway""}],""children"":[],""data"":{""label"":""VPN Gateway"",""icon"":""gcp_cloud_vpn""}},{""id"":""onprem_pub_vpc"",""labels"":[{""text"":""onprem_pub_vpc""}],""children"":[{""id"":""onprem_nat"",""labels"":[{""text"":""On-prem NAT""}],""children"":[],""data"":{""label"":""On-prem NAT"",""icon"":""gcp_cloud_nat""}}],""edges"":[]}],""edges"":[{""id"":""onprem_priv_to_nat"",""sources"":[""onprem_priv_vpc""],""targets"":[""onprem_nat""],""labels"":[{""text"":""egress link""}]},{""id"":""onprem_vpn_priv"",""sources"":[""onprem_vpn""],""targets"":[""onprem_priv_vpc""],""labels"":[{""text"":""internal link""}]}]}],""edges"":[{""id"":""partner_nat_repo"",""sources"":[""partner_nat""],""targets"":[""model_repo_shared""],""labels"":[{""text"":""repo sync""}]},{""id"":""onprem_nat_repo"",""sources"":[""onprem_nat""],""targets"":[""model_repo_shared""],""labels"":[{""text"":""repo sync""}]},{""id"":""gcp_nat_repo"",""sources"":[""gcp_nat""],""targets"":[""model_repo_shared""],""labels"":[{""text"":""repo sync""}]},{""id"":""onprem_vpn_to_lb"",""sources"":[""onprem_vpn""],""targets"":[""cloud_lb""],""labels"":[{""text"":""encrypted data""}]},{""id"":""olap_to_onprem_vpn"",""sources"":[""tee_olap""],""targets"":[""onprem_vpn""],""labels"":[{""text"":""aggregated""}]},{""id"":""partner_router_to_ic"",""sources"":[""partner_router""],""targets"":[""cross_cloud_ic""],""labels"":[{""text"":""link""}]}]}"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/security/confidential-computing-analytics-ai#confidential-federated,"Confidential federated learning architecture for financial institutions Federated learning offers an advanced solution for customers who value stringent data privacy and data sovereignty. The confidential federated learning architecture provides a secure, scalable, and efficient way to use data for AI applications. This architecture brings the models to the location where the data is stored, rather than centralizing the data in a single location, thereby reducing the risks associated with data leakage. This architectural pattern demonstrates how multiple financial institutions can collaboratively train a fraud detection model while preserving the confidentiality of their sensitive transaction data with fraud labels. It uses federated learning along with confidential computing techniques to enable secure, multi-party machine learning without training data movement. This architecture has the following benefits: Enhanced data privacy and security: Federated learning enables data privacy and data locality by ensuring that sensitive data remains at each site. Additionally, financial institutions can use privacy preserving techniques such as homomorphic encryption and differential privacy filters to further protect any transferred data (such as the model weights). Improved accuracy and diversity: By training with a variety of data sources across different clients, financial institutions can develop a robust and generalizable global model to better represent heterogeneous datasets. Scalability and network efficiency: With the ability to perform training at the edge, institutions can scale federated learning across the globe. Additionally, institutions only need to transfer the model weights rather than entire datasets, which enables efficient use of network resources. The following diagram shows this architecture. Diagram of confidential federated learning architecture. The key components of this architecture include the following: Federated server in the TEE cluster: A secure, isolated environment where the federated learning server orchestrates the collaboration of multiple clients by first sending an initial model to the federated learning clients. The clients perform training on their local datasets, then send the model updates back to the federated learning server for aggregation to form a global model. Federated learning model repository: A pre-trained fraud detection model that serves as the starting point for federated learning. Local application inference engine: An application that executes tasks, performs local computation and learning with local datasets, and submits results back to federated learning server for secure aggregation. Local private data: Each bank holds its own private, encrypted transaction data that includes fraud labels. This data remains encrypted throughout the entire process, ensuring data privacy. Secure aggregation protocol (symbolized by the dotted blue line): The federated learning server doesn't need to access any individual bank's update to train the model; it requires only the element-wise weighted averages of the update vectors, taken from a random subset of banks or sites. Using a secure aggregation protocol to compute these weighted averages helps ensure that the server can learn only that one or more banks in this randomly selected subset wrote a given word, but not which banks, thereby preserving the privacy of each participant in the federated learning process. Global fraud-trained model and aggregated weights (symbolized by the green line): The improved fraud detection model, along with its learned weights, is securely sent back to the participating banks. The banks can then deploy this enhanced model locally for fraud detection on their own transactions.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""gcp_region"",""labels"":[{""text"":""gcp_region""}],""children"":[{""id"":""fl_logging"",""labels"":[{""text"":""FL logging (Cloud Logging)""}],""children"":[],""data"":{""label"":""FL logging (Cloud Logging)"",""icon"":""gcp_cloud_logging""}},{""id"":""cloud_router"",""labels"":[{""text"":""Cloud Router""}],""children"":[],""data"":{""label"":""Cloud Router"",""icon"":""gcp_cloud_router""}},{""id"":""cross_ic"",""labels"":[{""text"":""Cross-Cloud Interconnect""}],""children"":[],""data"":{""label"":""Cross-Cloud Interconnect"",""icon"":""gcp_cloud_interconnect""}},{""id"":""cloud_vpn"",""labels"":[{""text"":""Cloud VPN""}],""children"":[],""data"":{""label"":""Cloud VPN"",""icon"":""gcp_cloud_vpn""}},{""id"":""g_pub_vpc"",""labels"":[{""text"":""g_pub_vpc""}],""children"":[{""id"":""g_nat"",""labels"":[{""text"":""Cloud NAT""}],""children"":[],""data"":{""label"":""Cloud NAT"",""icon"":""gcp_cloud_nat""}}],""edges"":[]},{""id"":""private_vpc_group"",""labels"":[{""text"":""private_vpc_group""}],""children"":[{""id"":""cloud_lb"",""labels"":[{""text"":""Cloud Load Balancing""}],""children"":[],""data"":{""label"":""Cloud Load Balancing"",""icon"":""gcp_cloud_load_balancing""}},{""id"":""tee_cluster"",""labels"":[{""text"":""tee_cluster""}],""children"":[{""id"":""tee_agg"",""labels"":[{""text"":""Flare Aggregated Server""}],""children"":[],""data"":{""label"":""Flare Aggregated Server"",""icon"":""gcp_compute_engine""}},{""id"":""global_model_hub"",""labels"":[{""text"":""Flare Global Model Hub""}],""children"":[],""data"":{""label"":""Flare Global Model Hub"",""icon"":""gcp_compute_engine""}},{""id"":""console_srv"",""labels"":[{""text"":""Flare Console Server""}],""children"":[],""data"":{""label"":""Flare Console Server"",""icon"":""gcp_compute_engine""}},{""id"":""ui_srv"",""labels"":[{""text"":""Flare UI Server""}],""children"":[],""data"":{""label"":""Flare UI Server"",""icon"":""gcp_compute_engine""}}],""edges"":[]},{""id"":""nvflare_config"",""labels"":[{""text"":""NVFlare config bucket""}],""children"":[],""data"":{""label"":""NVFlare config bucket"",""icon"":""gcp_cloud_storage""}}],""edges"":[{""id"":""lb_to_agg"",""sources"":[""cloud_lb""],""targets"":[""tee_agg""],""labels"":[{""text"":""forward""}]},{""id"":""cfg_to_agg"",""sources"":[""nvflare_config""],""targets"":[""tee_agg""],""labels"":[{""text"":""config""}]}]},{""id"":""fl_resources"",""labels"":[{""text"":""fl_resources""}],""children"":[{""id"":""code_repo_gcp"",""labels"":[{""text"":""FL code repo (GCS)""}],""children"":[],""data"":{""label"":""FL code repo (GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""tensorboard"",""labels"":[{""text"":""Vertex Tensorboard""}],""children"":[],""data"":{""label"":""Vertex Tensorboard"",""icon"":""gcp_vertexai""}},{""id"":""model_repo"",""labels"":[{""text"":""FL model repo (GCS)""}],""children"":[],""data"":{""label"":""FL model repo (GCS)"",""icon"":""gcp_cloud_storage""}}],""edges"":[]}],""edges"":[{""id"":""ic_to_router"",""sources"":[""cross_ic""],""targets"":[""cloud_router""],""labels"":[{""text"":""route""}]},{""id"":""vpn_to_router"",""sources"":[""cloud_vpn""],""targets"":[""cloud_router""],""labels"":[{""text"":""route""}]},{""id"":""router_to_lb"",""sources"":[""cloud_router""],""targets"":[""cloud_lb""],""labels"":[{""text"":""internal lb""}]},{""id"":""agg_logs"",""sources"":[""tee_agg""],""targets"":[""fl_logging""],""labels"":[{""text"":""logs""}]},{""id"":""agg_to_nat"",""sources"":[""tee_agg""],""targets"":[""g_nat""],""labels"":[{""text"":""egress""}]}]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[]},{""id"":""nvidia_repo"",""labels"":[{""text"":""nvidia_repo""}],""children"":[{""id"":""nflare_repo"",""labels"":[{""text"":""NFlare repo""}],""children"":[],""data"":{""label"":""NFlare repo"",""icon"":""third_party_api""}},{""id"":""attestation_svc"",""labels"":[{""text"":""Remote attest svc""}],""children"":[],""data"":{""label"":""Remote attest svc"",""icon"":""third_party_api""}}],""edges"":[]},{""id"":""partner_cloud_region"",""labels"":[{""text"":""partner_cloud_region""}],""children"":[{""id"":""p_priv_vpc"",""labels"":[{""text"":""p_priv_vpc""}],""children"":[{""id"":""p_flare_client"",""labels"":[{""text"":""Flare client""}],""children"":[],""data"":{""label"":""Flare client"",""icon"":""gcp_compute_engine""}},{""id"":""p_private_data"",""labels"":[{""text"":""Local private data""}],""children"":[],""data"":{""label"":""Local private data"",""icon"":""database""}},{""id"":""p_inference"",""labels"":[{""text"":""Local inference engine""}],""children"":[],""data"":{""label"":""Local inference engine"",""icon"":""gcp_compute_engine""}}],""edges"":[]},{""id"":""p_router"",""labels"":[{""text"":""Partner router""}],""children"":[],""data"":{""label"":""Partner router"",""icon"":""gcp_cloud_router""}},{""id"":""p_pub_vpc"",""labels"":[{""text"":""p_pub_vpc""}],""children"":[{""id"":""p_nat"",""labels"":[{""text"":""Partner NAT""}],""children"":[],""data"":{""label"":""Partner NAT"",""icon"":""gcp_cloud_nat""}}],""edges"":[]}],""edges"":[{""id"":""p_router_priv"",""sources"":[""p_router""],""targets"":[""p_priv_vpc""],""labels"":[{""text"":""link""}]},{""id"":""p_priv_nat"",""sources"":[""p_priv_vpc""],""targets"":[""p_nat""],""labels"":[{""text"":""egress""}]}]},{""id"":""partner_onprem_dc"",""labels"":[{""text"":""partner_onprem_dc""}],""children"":[{""id"":""o_priv_vpc"",""labels"":[{""text"":""o_priv_vpc""}],""children"":[{""id"":""o_flare_client"",""labels"":[{""text"":""Flare client""}],""children"":[],""data"":{""label"":""Flare client"",""icon"":""gcp_compute_engine""}},{""id"":""o_private_data"",""labels"":[{""text"":""Local private data""}],""children"":[],""data"":{""label"":""Local private data"",""icon"":""database""}},{""id"":""o_inference"",""labels"":[{""text"":""Local inference engine""}],""children"":[],""data"":{""label"":""Local inference engine"",""icon"":""gcp_compute_engine""}}],""edges"":[]},{""id"":""o_vpn"",""labels"":[{""text"":""VPN Gateway""}],""children"":[],""data"":{""label"":""VPN Gateway"",""icon"":""gcp_cloud_vpn""}},{""id"":""o_pub_vpc"",""labels"":[{""text"":""o_pub_vpc""}],""children"":[{""id"":""o_nat"",""labels"":[{""text"":""On-prem NAT""}],""children"":[],""data"":{""label"":""On-prem NAT"",""icon"":""gcp_cloud_nat""}}],""edges"":[]}],""edges"":[{""id"":""o_vpn_priv"",""sources"":[""o_vpn""],""targets"":[""o_priv_vpc""],""labels"":[{""text"":""link""}]},{""id"":""o_priv_nat"",""sources"":[""o_priv_vpc""],""targets"":[""o_nat""],""labels"":[{""text"":""egress""}]}]}],""edges"":[{""id"":""p_nat_patch"",""sources"":[""p_nat""],""targets"":[""nflare_repo""],""labels"":[{""text"":""patches""}]},{""id"":""o_nat_patch"",""sources"":[""o_nat""],""targets"":[""nflare_repo""],""labels"":[{""text"":""patches""}]},{""id"":""g_nat_patch"",""sources"":[""g_nat""],""targets"":[""nflare_repo""],""labels"":[{""text"":""patches""}]},{""id"":""o_vpn_to_lb"",""sources"":[""o_vpn""],""targets"":[""cloud_lb""],""labels"":[{""text"":""encrypted data""}]},{""id"":""agg_to_p_router"",""sources"":[""tee_agg""],""targets"":[""p_router""],""labels"":[{""text"":""aggregated""}]},{""id"":""agg_to_o_vpn"",""sources"":[""tee_agg""],""targets"":[""o_vpn""],""labels"":[{""text"":""aggregated""}]},{""id"":""p_router_ic"",""sources"":[""p_router""],""targets"":[""cross_ic""],""labels"":[{""text"":""link""}]}]}"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#tfx-ml-system-on-google-cloud,"TFX ML system on Google Cloud In a production environment, the components of the system have to run at scale on a reliable platform. The following diagram shows how each step of the TFX ML pipeline runs using a managed service on Google Cloud, which ensures agility, reliability, and performance at a large scale. Steps of a TFX-based ML system on Google Cloud. Figure 3. TFX-based ML system on Google Cloud. The following table describes the key Google Cloud services shown in figure 3: Step	TFX Library	Google Cloud service Data extraction and validation	TensorFlow Data Validation	Dataflow Data transformation	TensorFlow Transform	Dataflow Model training and tuning	TensorFlow	Vertex AI Training Model evaluation and validation	TensorFlow Model Analysis	Dataflow Model serving for predictions	TensorFlow Serving	Vertex AI Prediction Model Storage	N/A	Vertex AI Model Registry Dataflow is a fully managed, serverless, and reliable service for running Apache Beam pipelines at scale on Google Cloud. Dataflow is used to scale the following processes: Computing the statistics to validate the incoming data. Performing data preparation and transformation. Evaluating the model on a large dataset. Computing metrics on different aspects of the evaluation dataset. Cloud Storage is a highly available and durable storage for binary large objects. Cloud Storage hosts artifacts produced throughout the execution of the ML pipeline, including the following: Data anomalies (if any) Transformed data and artifacts Exported (trained) model Model evaluation metrics Vertex AI Training is a managed service to train ML models at scale. You can execute model training jobs with pre-build containers for TensorFlow, Scikit learn, XGBoost and PyTorch. You can also run any framework using your own custom containers. For your training infrastructure you can use accelerators and multiple nodes for distributed training. In addition, a scalable, Bayesian optimization-based service for a hyperparameter tuning is available Vertex AI Prediction is a managed service to run batch predictions using your trained models and online predictions by deploying your models as a microservice with a REST API. The service also integrates with Vertex Explainable AI and Vertex AI Model Monitoring to understand your models and receive alerts when there is a feature or feature attribution skew and drift. Vertex AI Model Registry lets you manage the lifecycle of your ML models. You can version your imported models and view their performance metrics. A model can then be used for batch predictions or deploy your model for online serving using Vertex AI Prediction","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""data_sources"",""labels"":[{""text"":""data_sources""}],""children"":[{""id"":""dw_bq"",""labels"":[{""text"":""Data warehouse""}],""children"":[],""data"":{""label"":""Data warehouse"",""icon"":""gcp_bigquery""}},{""id"":""dm_sql"",""labels"":[{""text"":""Data mart (Cloud SQL)""}],""children"":[],""data"":{""label"":""Data mart (Cloud SQL)"",""icon"":""gcp_cloud_sql""}},{""id"":""feat_store"",""labels"":[{""text"":""Feature store""}],""children"":[],""data"":{""label"":""Feature store"",""icon"":""gcp_vertexai""}},{""id"":""oper_ds"",""labels"":[{""text"":""Operational DS (Spanner)""}],""children"":[],""data"":{""label"":""Operational DS (Spanner)"",""icon"":""gcp_spanner""}}],""edges"":[]},{""id"":""extraction_validation"",""labels"":[{""text"":""extraction_validation""}],""children"":[{""id"":""raw_bkt"",""labels"":[{""text"":""Raw data (GCS)""}],""children"":[],""data"":{""label"":""Raw data (GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""tfdv"",""labels"":[{""text"":""TensorFlow Data Validation""}],""children"":[],""data"":{""label"":""TensorFlow Data Validation"",""icon"":""gcp_dataflow""}}],""edges"":[{""id"":""raw_to_tfdv"",""sources"":[""raw_bkt""],""targets"":[""tfdv""],""labels"":[{""text"":""validate""}]},{""id"":""tfdv_to_raw"",""sources"":[""tfdv""],""targets"":[""raw_bkt""],""labels"":[{""text"":""validated data""}]}]},{""id"":""transformation"",""labels"":[{""text"":""transformation""}],""children"":[{""id"":""tft"",""labels"":[{""text"":""TensorFlow Transform""}],""children"":[],""data"":{""label"":""TensorFlow Transform"",""icon"":""gcp_dataflow""}},{""id"":""xform_bkt"",""labels"":[{""text"":""Transformed train/test (GCS)""}],""children"":[],""data"":{""label"":""Transformed train/test (GCS)"",""icon"":""gcp_cloud_storage""}}],""edges"":[{""id"":""tft_to_xform"",""sources"":[""tft""],""targets"":[""xform_bkt""],""labels"":[{""text"":""write transformed""}]}]},{""id"":""training_tuning"",""labels"":[{""text"":""training_tuning""}],""children"":[{""id"":""tf_estimator"",""labels"":[{""text"":""TensorFlow Estimator""}],""children"":[],""data"":{""label"":""TensorFlow Estimator"",""icon"":""gcp_vertexai""}},{""id"":""saved_model_bkt"",""labels"":[{""text"":""Saved model (GCS)""}],""children"":[],""data"":{""label"":""Saved model (GCS)"",""icon"":""gcp_cloud_storage""}}],""edges"":[{""id"":""train_to_saved"",""sources"":[""tf_estimator""],""targets"":[""saved_model_bkt""],""labels"":[{""text"":""output model""}]}]},{""id"":""evaluation_validation"",""labels"":[{""text"":""evaluation_validation""}],""children"":[{""id"":""tfma"",""labels"":[{""text"":""TensorFlow Model Analysis""}],""children"":[],""data"":{""label"":""TensorFlow Model Analysis"",""icon"":""gcp_dataflow""}},{""id"":""eval_metrics_bkt"",""labels"":[{""text"":""Evaluation metrics (GCS)""}],""children"":[],""data"":{""label"":""Evaluation metrics (GCS)"",""icon"":""gcp_cloud_storage""}}],""edges"":[{""id"":""tfma_to_metrics"",""sources"":[""tfma""],""targets"":[""eval_metrics_bkt""],""labels"":[{""text"":""metrics""}]}]},{""id"":""model_storage"",""labels"":[{""text"":""model_storage""}],""children"":[{""id"":""model_registry"",""labels"":[{""text"":""Model registry""}],""children"":[],""data"":{""label"":""Model registry"",""icon"":""gcp_vertexai""}}],""edges"":[]},{""id"":""model_serving"",""labels"":[{""text"":""model_serving""}],""children"":[{""id"":""tf_serving"",""labels"":[{""text"":""TensorFlow Serving""}],""children"":[],""data"":{""label"":""TensorFlow Serving"",""icon"":""gcp_vertexai""}}],""edges"":[]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""dw_to_raw"",""sources"":[""dw_bq""],""targets"":[""raw_bkt""],""labels"":[{""text"":""export""}]},{""id"":""dm_to_raw"",""sources"":[""dm_sql""],""targets"":[""raw_bkt""],""labels"":[{""text"":""export""}]},{""id"":""fs_to_raw"",""sources"":[""feat_store""],""targets"":[""raw_bkt""],""labels"":[{""text"":""export""}]},{""id"":""sp_to_raw"",""sources"":[""oper_ds""],""targets"":[""raw_bkt""],""labels"":[{""text"":""export""}]},{""id"":""raw_to_tft"",""sources"":[""raw_bkt""],""targets"":[""tft""],""labels"":[{""text"":""transform""}]},{""id"":""xform_to_train"",""sources"":[""xform_bkt""],""targets"":[""tf_estimator""],""labels"":[{""text"":""train""}]},{""id"":""saved_to_tfma"",""sources"":[""saved_model_bkt""],""targets"":[""tfma""],""labels"":[{""text"":""validate model""}]},{""id"":""xform_to_tfma"",""sources"":[""xform_bkt""],""targets"":[""tfma""],""labels"":[{""text"":""eval data""}]},{""id"":""saved_to_registry"",""sources"":[""saved_model_bkt""],""targets"":[""model_registry""],""labels"":[{""text"":""register""}]},{""id"":""registry_to_serving"",""sources"":[""model_registry""],""targets"":[""tf_serving""],""labels"":[{""text"":""deploy""}]}]}]}"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build#kubeflow-pipelines-sdk,"Kubeflow Pipelines SDK The Kubeflow Pipelines SDK lets you create components, define their orchestration, and run them as a pipeline. For details about Kubeflow Pipelines components, see Create components in the Kubeflow documentation. You can also use the TFX Pipeline DSL and use TFX components. A TFX component encapsulates metadata capabilities. The driver supplies metadata to the executor by querying the metadata store. The publisher accepts the results of the executor and stores them in metadata. You can also implement your custom component, which has the same integration with the metadata. You can compile your TFX pipelines to a Vertex AI Pipelines compatible YAML using tfx.orchestration.experimental.KubeflowV2DagRunner. Then, you can submit the file to Vertex AI Pipelines for execution. The following diagram shows how in Vertex AI Pipelines, a containerized task can invoke other services such as BigQuery jobs, Vertex AI (distributed) training jobs, and Dataflow jobs. Architecture of Vertex AI Pipelines on Google Cloud. Figure 5. Vertex AI Pipelines invoking Google Cloud managed services. Vertex AI Pipelines lets you orchestrate and automate a production ML pipeline by executing the required Google Cloud services. In figure 5, Vertex ML Metadata serves as the ML metadata store for Vertex AI Pipelines. Pipeline components aren't limited to executing TFX-related services on Google Cloud. These components can execute any data-related and compute-related services, including Dataproc for SparkML jobs, AutoML, and other compute workloads. Containerizing tasks in Vertex AI Pipelines has the following advantages: Decouples the execution environment from your code runtime. Provides reproducibility of the code between the development and production environment, because the things you test are the same in production. Isolates each component in the pipeline; each can have its own version of the runtime, different languages, and different libraries. Helps with composition of complex pipelines. Integrates with Vertex ML Metadata for traceability and reproducibility of pipeline executions and artifacts. For a comprehensive introduction to Vertex AI Pipelines, see the list of available notebooks examples.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""artifact_repo"",""labels"":[{""text"":""artifact_repo""}],""children"":[{""id"":""artifact_reg"",""labels"":[{""text"":""Artifact Registry""}],""children"":[],""data"":{""label"":""Artifact Registry"",""icon"":""gcp_artifact_registry""}}],""edges"":[]},{""id"":""storage"",""labels"":[{""text"":""storage""}],""children"":[{""id"":""gcs_bucket"",""labels"":[{""text"":""Cloud Storage""}],""children"":[],""data"":{""label"":""Cloud Storage"",""icon"":""gcp_cloud_storage""}}],""edges"":[]},{""id"":""analytics"",""labels"":[{""text"":""analytics""}],""children"":[{""id"":""bq"",""labels"":[{""text"":""BigQuery""}],""children"":[],""data"":{""label"":""BigQuery"",""icon"":""gcp_bigquery""}},{""id"":""dataflow"",""labels"":[{""text"":""Dataflow""}],""children"":[],""data"":{""label"":""Dataflow"",""icon"":""gcp_dataflow""}},{""id"":""dataproc"",""labels"":[{""text"":""Dataproc""}],""children"":[],""data"":{""label"":""Dataproc"",""icon"":""gcp_dataproc""}}],""edges"":[]},{""id"":""ml_platform"",""labels"":[{""text"":""ml_platform""}],""children"":[{""id"":""pipelines"",""labels"":[{""text"":""Vertex AI Pipelines""}],""children"":[],""data"":{""label"":""Vertex AI Pipelines"",""icon"":""gcp_vertexai""}},{""id"":""metadata_group"",""labels"":[{""text"":""metadata_group""}],""children"":[{""id"":""metadata"",""labels"":[{""text"":""Vertex AI Metadata""}],""children"":[],""data"":{""label"":""Vertex AI Metadata"",""icon"":""gcp_vertexai""}}],""edges"":[]},{""id"":""model_services"",""labels"":[{""text"":""model_services""}],""children"":[{""id"":""prediction"",""labels"":[{""text"":""Vertex AI Prediction""}],""children"":[],""data"":{""label"":""Vertex AI Prediction"",""icon"":""gcp_vertexai""}},{""id"":""training"",""labels"":[{""text"":""Vertex AI Training""}],""children"":[],""data"":{""label"":""Vertex AI Training"",""icon"":""gcp_vertexai""}},{""id"":""automl"",""labels"":[{""text"":""AutoML""}],""children"":[],""data"":{""label"":""AutoML"",""icon"":""gcp_vertexai""}}],""edges"":[]}],""edges"":[{""id"":""pipelines_to_metadata"",""sources"":[""pipelines""],""targets"":[""metadata""],""labels"":[{""text"":""track run""}]},{""id"":""pipelines_to_training"",""sources"":[""pipelines""],""targets"":[""training""],""labels"":[{""text"":""submit training""}]},{""id"":""pipelines_to_prediction"",""sources"":[""pipelines""],""targets"":[""prediction""],""labels"":[{""text"":""deploy model""}]},{""id"":""pipelines_to_automl"",""sources"":[""pipelines""],""targets"":[""automl""],""labels"":[{""text"":""AutoML job""}]}]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""artifact_to_pipelines"",""sources"":[""artifact_reg""],""targets"":[""pipelines""],""labels"":[{""text"":""pull image""}]},{""id"":""pipelines_to_gcs"",""sources"":[""pipelines""],""targets"":[""gcs_bucket""],""labels"":[{""text"":""store artefacts""}]},{""id"":""pipelines_to_bq"",""sources"":[""pipelines""],""targets"":[""bq""],""labels"":[{""text"":""write features""}]},{""id"":""pipelines_to_dataflow"",""sources"":[""pipelines""],""targets"":[""dataflow""],""labels"":[{""text"":""launch jobs""}]},{""id"":""pipelines_to_dataproc"",""sources"":[""pipelines""],""targets"":[""dataproc""],""labels"":[{""text"":""launch jobs""}]}]}]}"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build,"Setting up CI/CD for ML on Google Cloud Vertex AI Pipelines lets you orchestrate ML systems that involve multiple steps, including data preprocessing, model training and evaluation, and model deployment. In the data science exploration phase, Vertex AI Pipelines help with rapid experimentation of the whole system. In the production phase, Vertex AI Pipelines lets you automate the pipeline execution based on new data to train or retrain the ML model. CI/CD architecture The following diagram shows a high-level overview of CI/CD for ML with Vertex AI Pipelines. Architecture of CI/CD for ML pipeline using Vertex AI Pipelines. Figure 7: High-level overview of CI/CD with Vertex AI Pipelines. At the core of this architecture is Cloud Build. Cloud Build can import source from Artifact Registry, GitHub, or Bitbucket, and then execute a build to your specifications, and produce artifacts such as Docker containers or Python tar files. Cloud Build executes your build as a series of build steps, defined in a build configuration file (cloudbuild.yaml). Each build step runs in a Docker container. You can either use the supported build steps provided by Cloud Build, or write your own build steps. The Cloud Build process, which performs the required CI/CD for your ML system, can be executed either manually or through automated build triggers. Triggers execute your configured build steps whenever changes are pushed to the build source. You can set a build trigger to execute the build routine on changes to the source repository, or to execute the build routine only when changes match certain criteria. In addition, you can have build routines (Cloud Build configuration files) that are executed in response to different triggers. For example, you can have build routines that are triggered when commits are made to the development branch or to the main branch. You can use configuration variable substitutions to define the environment variables at build time. These substitutions are captured from triggered builds. These variables include $COMMIT_SHA, $REPO_NAME, $BRANCH_NAME, $TAG_NAME, and $REVISION_ID. Other non-trigger-based variables are $PROJECT_ID and $BUILD_ID. Substitutions are helpful for variables whose value isn't known until build time, or to reuse an existing build request with different variable values. CI/CD workflow use case A source code repository typically includes the following items: The Python pipelines workflow source code where the pipeline workflow is defined The Python pipeline components source code and the corresponding component specification files for the different pipeline components such as data validation, data transformation, model training, model evaluation, and model serving. Dockerfiles that are required to create Docker container images, one for each pipeline component. Python unit and integration tests to test the methods implemented in the component and overall pipeline. Other scripts, including the cloudbuild.yaml file, test triggers and pipeline deployments. Configuration files (for example, the settings.yaml file), including configurations to the pipeline input parameters. Notebooks used for exploratory data analysis, model analysis, and interactive experimentation on models. In the following example, a build routine is triggered when a developer pushes source code to the development branch from their data science environment. Example build steps. Figure 8. Example build steps performed by Cloud Build. Cloud Build typically performs the following build steps, which are also shown in figure 7: The source code repository is copied to the Cloud Build runtime environment, under the /workspace directory. Run unit and integration tests. Optional: Run static code analysis by using an analyzer such as Pylint. If the tests pass, the Docker container images are built, one for each pipeline component. The images are tagged with the $COMMIT_SHA parameter. The Docker container images are uploaded to the Artifact Registry (as shown in figure 7). The image URL is updated in each of the component.yaml files with the created and tagged Docker container images. The pipeline workflow is compiled to produce the pipeline.json file. The pipeline.json file is uploaded to Artifact Registry. Optional: Run the pipeline with the parameter values as part of an integration test or production execution. The executed pipeline generates a new model and could also deploy the model as an API on Vertex AI Prediction. For a production ready end-to-end MLOps example that includes CI/CD using Cloud Build, see Vertex Pipelines End-to-end Samples on GitHub.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""ci_cd"",""labels"":[{""text"":""ci_cd""}],""children"":[{""id"":""src_artifact_registry"",""labels"":[{""text"":""Artifact Registry (src)""}],""children"":[],""data"":{""label"":""Artifact Registry (src)"",""icon"":""gcp_artifact_registry""}},{""id"":""cloud_build"",""labels"":[{""text"":""Cloud Build""}],""children"":[],""data"":{""label"":""Cloud Build"",""icon"":""gcp_cloud_build""}},{""id"":""prod_artifact_registry"",""labels"":[{""text"":""Artifact Registry (prod)""}],""children"":[],""data"":{""label"":""Artifact Registry (prod)"",""icon"":""gcp_artifact_registry""}},{""id"":""release_gateway"",""labels"":[{""text"":""Release gateway""}],""children"":[],""data"":{""label"":""Release gateway"",""icon"":""gcp_cloud_build""}}],""edges"":[{""id"":""src_ar_triggers_build"",""sources"":[""src_artifact_registry""],""targets"":[""cloud_build""],""labels"":[{""text"":""trigger build""}]},{""id"":""build_push_docker"",""sources"":[""cloud_build""],""targets"":[""prod_artifact_registry""],""labels"":[{""text"":""push images & template""}]},{""id"":""build_to_release_gateway"",""sources"":[""cloud_build""],""targets"":[""release_gateway""],""labels"":[{""text"":""produce artifacts""}]}]},{""id"":""ml_platform"",""labels"":[{""text"":""ml_platform""}],""children"":[{""id"":""workbench"",""labels"":[{""text"":""Vertex AI Workbench""}],""children"":[],""data"":{""label"":""Vertex AI Workbench"",""icon"":""gcp_vertexai""}},{""id"":""pipelines"",""labels"":[{""text"":""Vertex AI Pipelines""}],""children"":[{""id"":""stage_extract"",""labels"":[{""text"":""Data extraction & validation""}],""children"":[],""data"":{""label"":""Data extraction & validation"",""icon"":""gcp_dataflow""}},{""id"":""stage_transform"",""labels"":[{""text"":""Data transformation""}],""children"":[],""data"":{""label"":""Data transformation"",""icon"":""gcp_dataflow""}},{""id"":""stage_train"",""labels"":[{""text"":""Model training & tuning""}],""children"":[],""data"":{""label"":""Model training & tuning"",""icon"":""gcp_vertexai""}},{""id"":""stage_eval"",""labels"":[{""text"":""Model evaluation & validation""}],""children"":[],""data"":{""label"":""Model evaluation & validation"",""icon"":""gcp_vertexai""}},{""id"":""stage_store"",""labels"":[{""text"":""Model storage""}],""children"":[],""data"":{""label"":""Model storage"",""icon"":""gcp_vertexai""}},{""id"":""stage_serve"",""labels"":[{""text"":""Model serving for prediction""}],""children"":[],""data"":{""label"":""Model serving for prediction"",""icon"":""gcp_vertexai""}}],""data"":{""label"":""Vertex AI Pipelines"",""icon"":""gcp_vertexai""},""edges"":[{""id"":""pipe_extract_to_transform"",""sources"":[""stage_extract""],""targets"":[""stage_transform""]},{""id"":""pipe_transform_to_train"",""sources"":[""stage_transform""],""targets"":[""stage_train""]},{""id"":""pipe_train_to_eval"",""sources"":[""stage_train""],""targets"":[""stage_eval""]},{""id"":""pipe_eval_to_store"",""sources"":[""stage_eval""],""targets"":[""stage_store""]},{""id"":""pipe_store_to_serve"",""sources"":[""stage_store""],""targets"":[""stage_serve""]}]}],""edges"":[{""id"":""workbench_runs_pipeline"",""sources"":[""workbench""],""targets"":[""pipelines""],""labels"":[{""text"":""run experiments""}]}]},{""id"":""data_services"",""labels"":[{""text"":""data_services""}],""children"":[{""id"":""gcs"",""labels"":[{""text"":""Cloud Storage""}],""children"":[],""data"":{""label"":""Cloud Storage"",""icon"":""gcp_cloud_storage""}},{""id"":""bq"",""labels"":[{""text"":""BigQuery""}],""children"":[],""data"":{""label"":""BigQuery"",""icon"":""gcp_bigquery""}},{""id"":""dataflow"",""labels"":[{""text"":""Dataflow""}],""children"":[],""data"":{""label"":""Dataflow"",""icon"":""gcp_dataflow""}},{""id"":""dataproc"",""labels"":[{""text"":""Dataproc""}],""children"":[],""data"":{""label"":""Dataproc"",""icon"":""gcp_dataproc""}}],""edges"":[]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""release_deploy_pipeline"",""sources"":[""release_gateway""],""targets"":[""pipelines""],""labels"":[{""text"":""deploy & run""}]},{""id"":""pipelines_pull_image"",""sources"":[""prod_artifact_registry""],""targets"":[""pipelines""],""labels"":[{""text"":""pull images""}]},{""id"":""pipeline_to_gcs"",""sources"":[""pipelines""],""targets"":[""gcs""],""labels"":[{""text"":""artifacts""}]},{""id"":""pipeline_to_bq"",""sources"":[""pipelines""],""targets"":[""bq""],""labels"":[{""text"":""analytics""}]},{""id"":""pipeline_to_dataflow"",""sources"":[""pipelines""],""targets"":[""dataflow""],""labels"":[{""text"":""jobs""}]},{""id"":""pipeline_to_dataproc"",""sources"":[""pipelines""],""targets"":[""dataproc""],""labels"":[{""text"":""jobs""}]}]},{""id"":""user_dev"",""labels"":[{""text"":""User""}],""children"":[],""data"":{""label"":""User"",""icon"":""browser_client""}}],""edges"":[{""id"":""user_to_workbench"",""sources"":[""user_dev""],""targets"":[""workbench""],""labels"":[{""text"":""interactive""}]}]}"
gcp,ai-ml,ml-ops,https://cloud.google.com/architecture/building-a-vision-analytics-solution,"Build an ML vision analytics solution with Dataflow and Cloud Vision API bookmark_border Release Notes Last reviewed 2024-05-23 UTC In this reference architecture, you'll learn about the use cases, design alternatives, and design considerations when deploying a Dataflow pipeline to process image files with Cloud Vision and to store processed results in BigQuery. You can use those stored results for large scale data analysis and to train BigQuery ML pre-built models. This reference architecture document is intended for data engineers and data scientists. Architecture The following diagram illustrates the system flow for this reference architecture. An architecture showing the flow of information for ingest and trigger, processing, and store and analyze processes. As shown in the preceding diagram, information flows as follows: Ingest and trigger: This is the first stage of the system flow where images first enter the system. During this stage, the following actions occur: Clients upload image files to a Cloud Storage bucket. For each file upload, the Cloud Storage automatically sends an input notification by publishing a message to Pub/Sub. Process: This stage immediately follows the ingest and trigger stage. For each new input notification, the following actions occur: The Dataflow pipeline listens for these file input notifications, extracts file metadata from the Pub/Sub message, and sends the file reference to Vision API for processing. Vision API reads the image and creates annotations. The Dataflow pipeline stores the annotations produced by Vision API in BigQuery tables. Store and analyze: This is the final stage in the flow. At this stage, you can do the following with the saved results: Query BigQuery tables and analyze the stored annotations. Use BigQuery ML or Vertex AI to build models and execute predictions based on the stored annotations. Perform additional analysis in the Dataflow pipeline (not shown on this diagram).","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""ingest_trigger"",""labels"":[{""text"":""ingest_trigger""}],""children"":[{""id"":""image_bucket"",""labels"":[{""text"":""Image storage\n(GCS)""}],""children"":[],""data"":{""label"":""Image storage\n(GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""pubsub"",""labels"":[{""text"":""Notifications\n(Pub/Sub)""}],""children"":[],""data"":{""label"":""Notifications\n(Pub/Sub)"",""icon"":""gcp_pubsub""}}],""edges"":[{""id"":""bucket_to_pubsub"",""sources"":[""image_bucket""],""targets"":[""pubsub""],""labels"":[{""text"":""object finalize""}]}]},{""id"":""process"",""labels"":[{""text"":""process""}],""children"":[{""id"":""dataflow"",""labels"":[{""text"":""Streaming pipeline\n(Dataflow)""}],""children"":[],""data"":{""label"":""Streaming pipeline\n(Dataflow)"",""icon"":""gcp_dataflow""}},{""id"":""vision_api"",""labels"":[{""text"":""Image analysis\n(Vision API)""}],""children"":[],""data"":{""label"":""Image analysis\n(Vision API)"",""icon"":""gcp_cloud_vision_api""}}],""edges"":[{""id"":""dataflow_calls_vision"",""sources"":[""dataflow""],""targets"":[""vision_api""],""labels"":[{""text"":""annotate""}]},{""id"":""vision_to_dataflow"",""sources"":[""vision_api""],""targets"":[""dataflow""],""labels"":[{""text"":""annotations""}]}]},{""id"":""store_analyze"",""labels"":[{""text"":""store_analyze""}],""children"":[{""id"":""bigquery_dw"",""labels"":[{""text"":""Data warehouse\n(BigQuery)""}],""children"":[],""data"":{""label"":""Data warehouse\n(BigQuery)"",""icon"":""gcp_bigquery""}}],""edges"":[]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""pubsub_to_dataflow"",""sources"":[""pubsub""],""targets"":[""dataflow""],""labels"":[{""text"":""start job""}]},{""id"":""dataflow_reads_bucket"",""sources"":[""image_bucket""],""targets"":[""dataflow""],""labels"":[{""text"":""read images""}]},{""id"":""dataflow_to_bq"",""sources"":[""dataflow""],""targets"":[""bigquery_dw""],""labels"":[{""text"":""insert rows""}]}]},{""id"":""users"",""labels"":[{""text"":""users""}],""children"":[{""id"":""web_user"",""labels"":[{""text"":""Web client""}],""children"":[],""data"":{""label"":""Web client"",""icon"":""browser_client""}},{""id"":""mobile_user"",""labels"":[{""text"":""Mobile client""}],""children"":[],""data"":{""label"":""Mobile client"",""icon"":""mobile_app""}}],""edges"":[]}],""edges"":[{""id"":""web_upload"",""sources"":[""web_user""],""targets"":[""image_bucket""],""labels"":[{""text"":""upload""}]},{""id"":""mobile_upload"",""sources"":[""mobile_user""],""targets"":[""image_bucket""],""labels"":[{""text"":""upload""}]}]}"
gcp,ai-ml,ai-ml-applications,https://cloud.google.com/architecture/partners/harness-cicd-pipeline-for-rag-app,"Harness CI/CD pipeline for RAG applications bookmark_border Release Notes Last reviewed 2025-04-11 UTC By Martin Ansong, Senior Solutions Engineer at Harness This reference architecture describes how to implement a continuous integration (CI) and continuous deployment (CD) pipeline for a retrieval-augmented generation (RAG) application in Google Cloud. The architecture uses CI/CD products from Harness to deploy containers to Cloud Run services. Harness is a software delivery platform that offers AI-driven solutions for all phases of software delivery. Harness also offers Cloud Cost Management (CCM) for cost optimization. The architecture provides a scalable and efficient approach to deploy and manage RAG-capable generative AI applications, and helps to ensure secure, automated, and cost-effective application delivery. The intended audience for this document includes architects, developers, and DevOps engineers who develop and manage generative AI applications. Architecture The following diagram shows a CI/CD pipeline for deploying Cloud Run services for a RAG-capable generative AI application by using Harness products. The diagram is described in detail in the sections that follow. A RAG-capable generative AI application uses Harness products and a CI/CD pipeline to deploy Cloud Run services. Components This architecture consists of the following components: Source code management (SCM): The development workflow starts with a Git-based SCM repository like GitHub, Harness Code Repository, or Cloud Code. The repository triggers the CI/CD pipeline for new commits or merge events. Harness Continuous Integration (CI): This component does the following: Runs unit tests and security scans and builds the container image. Pushes the built image to Google Cloud's Artifact Registry or to Harness Artifact Registry. Runs integration tests in a Cloud Run development environment. Harness Security Test Orchestration (STO): Automates and orchestrates security testing across the CI/CD pipeline. This testing includes static application security testing (SAST), dynamic application security testing (DAST), and software composition analysis (SCA). Harness Supply Chain Security (SCS): This component does the following: Automatically generates software bills of material (SBOMs) to provide transparency into open-source and third-party components. Implements policy-as-code to govern the use of open-source software based on factors like component name, version, supplier, and licensing attributes. Generates and verifies provenance in line with Supply-chain Levels for Software Artifacts (SLSA) specifications to ensure artifact integrity. Provides visibility into the usage of software components across all artifacts, deployments, and environments. Harness Continuous Delivery (CD) & GitOps: This component does the following: Deploys the application progressively from development to staging and then from staging to production. Uses feature flags for controlled feature rollout. Implements traffic-shifting strategies like canary and blue-green deployments. Harness Feature Management & Experimentation (FME): This component does the following: Uses feature flags to test different versions of the software. Routes a percentage of user traffic to each version dynamically. Collects performance metrics for decision-making. Cloud Run: Executes stateless frontend and backend services for the generative AI application with automatic scaling. Harness Policy As Code: Provides governance of pipeline components through policy enforcement, which helps to ensure security and operational compliance. Harness Cloud Cost Management (CCM): This component does the following: Monitors the cost efficiency of Google Cloud resources. Detects cost anomalies and suggests optimizations. Implements autoscaling policies based on cost insights. CI/CD workflow The following is a typical workflow for developing and deploying applications by using the CI/CD pipeline that's shown in the preceding diagram. The step numbers in this workflow correspond to the numbers that are shown in the diagram. A triggering event from an SCM system starts the pipeline. The trigger could be a code commit or a merge action, such as a GitHub pull request. During the CI phase, the code is compiled, and unit tests are executed. Harness Test Intelligence optimizes the testing process by running only the relevant tests based on the code changes. This optimization helps to reduce unnecessary testing time. Security scans are also executed to ensure the integrity of the code. The developer receives the results immediately to allow actionable improvements. The code is packaged into a container image and pushed either to Harness Artifact Registry or to Google Cloud's Artifact Registry, where it's automatically scanned. The container image is deployed to a development environment, where integration tests are conducted. The image is promoted to the staging environment for additional testing, including chaos tests and load tests to ensure robustness. An approval gate is implemented before promotion to production. The approval gate helps to ensure that business processes, such as database changes or model performance, undergo proper review and approval. For example, database changes might require approval by a change advisory board (CAB). In the production environment, the application is deployed by using a canary release strategy. Traffic shifting is implemented between the old and new versions of the application. If failures occur, the application is rolled back to a stable version to minimize downtime. Feature flags are used to validate or test any features that have been released. This approach enables controlled experimentation without fully exposing new features to all users at the same time. The cost management system tracks and monitors resource utilization, detects cost anomalies, and offers recommendations for autoscaling to ensure cost-effectiveness.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""scm"",""labels"":[{""text"":""scm""}],""children"":[{""id"":""code_repo"",""labels"":[{""text"":""Code Repository""}],""children"":[],""data"":{""label"":""Code Repository"",""icon"":""third_party_api""}},{""id"":""github"",""labels"":[{""text"":""GitHub""}],""children"":[],""data"":{""label"":""GitHub"",""icon"":""third_party_api""}},{""id"":""cloud_code"",""labels"":[{""text"":""Cloud Code""}],""children"":[],""data"":{""label"":""Cloud Code"",""icon"":""third_party_api""}}],""edges"":[]},{""id"":""release_gateway"",""labels"":[{""text"":""Release Gateway""}],""children"":[],""data"":{""label"":""Release Gateway"",""icon"":""gcp_cloud_build""}},{""id"":""harnais_policy_as_code"",""labels"":[{""text"":""harnais_policy_as_code""}],""children"":[{""id"":""ci_pipeline"",""labels"":[{""text"":""ci_pipeline""}],""children"":[{""id"":""cloud_build"",""labels"":[{""text"":""Cloud Build""}],""children"":[],""data"":{""label"":""Cloud Build"",""icon"":""gcp_cloud_build""}},{""id"":""supply_chain_sec"",""labels"":[{""text"":""STS / SCCS""}],""children"":[],""data"":{""label"":""STS / SCCS"",""icon"":""gcp_security""}}],""edges"":[]},{""id"":""deploy_dev"",""labels"":[{""text"":""deploy_dev""}],""children"":[{""id"":""dev_run_deploy"",""labels"":[{""text"":""Cloud Run Deploy (dev)""}],""children"":[],""data"":{""label"":""Cloud Run Deploy (dev)"",""icon"":""gcp_cloud_run""}},{""id"":""integration_tests"",""labels"":[{""text"":""Integration Tests""}],""children"":[],""data"":{""label"":""Integration Tests"",""icon"":""gcp_cloud_build""}}],""edges"":[{""id"":""dev_runs_tests"",""sources"":[""dev_run_deploy""],""targets"":[""integration_tests""],""labels"":[{""text"":""tests""}]}]},{""id"":""deploy_stage"",""labels"":[{""text"":""deploy_stage""}],""children"":[{""id"":""stage_canary"",""labels"":[{""text"":""Cloud Run Canary""}],""children"":[],""data"":{""label"":""Cloud Run Canary"",""icon"":""gcp_cloud_run""}},{""id"":""chaos_eng"",""labels"":[{""text"":""Chaos Engineering""}],""children"":[],""data"":{""label"":""Chaos Engineering"",""icon"":""gcp_cloud_build""}}],""edges"":[{""id"":""stage_chaos"",""sources"":[""stage_canary""],""targets"":[""chaos_eng""],""labels"":[{""text"":""chaos""}]}]},{""id"":""deploy_prod"",""labels"":[{""text"":""deploy_prod""}],""children"":[{""id"":""prod_canary"",""labels"":[{""text"":""Cloud Run Prod Canary""}],""children"":[],""data"":{""label"":""Cloud Run Prod Canary"",""icon"":""gcp_cloud_run""}},{""id"":""cont_verify"",""labels"":[{""text"":""Continuous Verification""}],""children"":[],""data"":{""label"":""Continuous Verification"",""icon"":""gcp_cloud_monitoring""}},{""id"":""traffic_100"",""labels"":[{""text"":""Traffic Shift 100%""}],""children"":[],""data"":{""label"":""Traffic Shift 100%"",""icon"":""gcp_cloud_run""}}],""edges"":[{""id"":""verify_prod"",""sources"":[""prod_canary""],""targets"":[""cont_verify""],""labels"":[{""text"":""verify""}]},{""id"":""shift_100"",""sources"":[""cont_verify""],""targets"":[""traffic_100""],""labels"":[{""text"":""traffic 100%""}]}]}],""edges"":[{""id"":""promote_stage"",""sources"":[""dev_run_deploy""],""targets"":[""stage_canary""],""labels"":[{""text"":""promote""}]},{""id"":""approve_prod"",""sources"":[""stage_canary""],""targets"":[""prod_canary""],""labels"":[{""text"":""approval""}]}]},{""id"":""google_cloud_runtime"",""labels"":[{""text"":""google_cloud_runtime""}],""children"":[{""id"":""runtime_dev"",""labels"":[{""text"":""runtime_dev""}],""children"":[{""id"":""dev_front"",""labels"":[{""text"":""Run Front-Dev""}],""children"":[],""data"":{""label"":""Run Front-Dev"",""icon"":""gcp_cloud_run""}},{""id"":""dev_back"",""labels"":[{""text"":""Run Back-Dev""}],""children"":[],""data"":{""label"":""Run Back-Dev"",""icon"":""gcp_cloud_run""}}],""edges"":[]},{""id"":""runtime_stage"",""labels"":[{""text"":""runtime_stage""}],""children"":[{""id"":""stage_front"",""labels"":[{""text"":""Run Front-Stage""}],""children"":[],""data"":{""label"":""Run Front-Stage"",""icon"":""gcp_cloud_run""}},{""id"":""stage_back"",""labels"":[{""text"":""Run Back-Stage""}],""children"":[],""data"":{""label"":""Run Back-Stage"",""icon"":""gcp_cloud_run""}}],""edges"":[]},{""id"":""runtime_prod"",""labels"":[{""text"":""runtime_prod""}],""children"":[{""id"":""prod_front"",""labels"":[{""text"":""Run Front-Prod""}],""children"":[],""data"":{""label"":""Run Front-Prod"",""icon"":""gcp_cloud_run""}},{""id"":""prod_back"",""labels"":[{""text"":""Run Back-Prod""}],""children"":[],""data"":{""label"":""Run Back-Prod"",""icon"":""gcp_cloud_run""}}],""edges"":[]},{""id"":""external_client"",""labels"":[{""text"":""External Client""}],""children"":[],""data"":{""label"":""External Client"",""icon"":""third_party_api""}}],""edges"":[{""id"":""edge_runtime_dev_to_stage"",""sources"":[""runtime_dev""],""targets"":[""runtime_stage""],""labels"":[{""text"":""promote""}]},{""id"":""edge_runtime_stage_to_prod"",""sources"":[""runtime_stage""],""targets"":[""runtime_prod""],""labels"":[{""text"":""promote""}]},{""id"":""edge_external_to_front_prod"",""sources"":[""external_client""],""targets"":[""prod_front""],""labels"":[{""text"":""access""}]}]},{""id"":""cost_mgmt"",""labels"":[{""text"":""cost_mgmt""}],""children"":[{""id"":""cost_ctrl"",""labels"":[{""text"":""Cost Mgmt""}],""children"":[],""data"":{""label"":""Cost Mgmt"",""icon"":""gcp_billing""}}],""edges"":[]},{""id"":""feature_mgmt"",""labels"":[{""text"":""feature_mgmt""}],""children"":[{""id"":""feature_flags"",""labels"":[{""text"":""Feature Flags""}],""children"":[],""data"":{""label"":""Feature Flags"",""icon"":""gcp_feature_store""}},{""id"":""ab_test"",""labels"":[{""text"":""A/B Testing""}],""children"":[],""data"":{""label"":""A/B Testing"",""icon"":""gcp_vertexai""}}],""edges"":[]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""scm_trigger_build"",""sources"":[""scm""],""targets"":[""cloud_build""],""labels"":[{""text"":""trigger""}]},{""id"":""build_to_release"",""sources"":[""cloud_build""],""targets"":[""release_gateway""],""labels"":[{""text"":""artifacts""}]},{""id"":""release_to_dev"",""sources"":[""release_gateway""],""targets"":[""dev_run_deploy""],""labels"":[{""text"":""deploy""}]},{""id"":""dev_deploy_to_runtime"",""sources"":[""dev_run_deploy""],""targets"":[""runtime_dev""],""labels"":[{""text"":""runtime""}]},{""id"":""stage_deploy_to_runtime"",""sources"":[""stage_canary""],""targets"":[""runtime_stage""],""labels"":[{""text"":""runtime""}]},{""id"":""prod_shift_to_runtime"",""sources"":[""traffic_100""],""targets"":[""runtime_prod""],""labels"":[{""text"":""runtime""}]}]},{""id"":""user_grp"",""labels"":[{""text"":""user_grp""}],""children"":[{""id"":""developer"",""labels"":[{""text"":""Developer""}],""children"":[],""data"":{""label"":""Developer"",""icon"":""browser_client""}}],""edges"":[]}],""edges"":[{""id"":""dev_commit"",""sources"":[""developer""],""targets"":[""code_repo""],""labels"":[{""text"":""commit""}]}]}"
gcp,ai-ml,ai-ml-applications,https://cloud.google.com/architecture/implement-two-tower-retrieval-large-scale-candidate-generation,"Implement two-tower retrieval for large-scale candidate generation bookmark_border Release Notes Last reviewed 2025-01-16 UTC This document provides a reference architecture that shows you how to implement an end-to-end two-tower candidate generation workflow with Vertex AI. The two-tower modeling framework is a powerful retrieval technique for personalization use cases because it learns the semantic similarity between two different entities, such as web queries and candidate items. This document is for technical practitioners like data scientists and machine learning engineers who are developing large-scale recommendation applications with low-latency serving requirements. For more information about the modeling techniques, problem framing, and data preparation for building a two-tower model, see Scaling deep retrieval with TensorFlow Recommenders and Vector Search. Architecture The following diagram shows an architecture to train a two-tower model and deploy each tower separately for different deployment and serving tasks: An architecture to train a two-tower model and deploy each tower separately. The architecture in the diagram includes the following components: Training data: Training files are stored in Cloud Storage. Two-tower training: The combined two-tower model is trained offline using the Vertex AI Training service; each tower is saved separately and used for different tasks. Registered query and candidate towers: After the towers are trained, each tower is separately uploaded to Vertex AI Model Registry. Deployed query tower: The registered query tower is deployed to a Vertex AI online endpoint. Batch predict embeddings: The registered candidate tower is used in a batch prediction job to precompute the embedding representations of all available candidate items. Embeddings JSON: The predicted embeddings are saved to a JSON file in Cloud Storage. ANN index: Vertex AI Vector Search is used to create a serving index that's configured for approximate nearest neighbor (ANN) search. Deployed index: The ANN index is deployed to a Vertex AI Vector Search index endpoint.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""training_data"",""labels"":[{""text"":""Training data (GCS)""}],""children"":[],""data"":{""label"":""Training data (GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""two_tower_train"",""labels"":[{""text"":""Two-tower training""}],""children"":[],""data"":{""label"":""Two-tower training"",""icon"":""gcp_vertexai""}},{""id"":""reg_candidate"",""labels"":[{""text"":""Registered candidate tower""}],""children"":[],""data"":{""label"":""Registered candidate tower"",""icon"":""gcp_vertexai""}},{""id"":""batch_predict"",""labels"":[{""text"":""Batch predict embeddings""}],""children"":[],""data"":{""label"":""Batch predict embeddings"",""icon"":""gcp_vertexai""}},{""id"":""embeddings_json"",""labels"":[{""text"":""Embeddings JSON (GCS)""}],""children"":[],""data"":{""label"":""Embeddings JSON (GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""ann_index"",""labels"":[{""text"":""ANN index (Vector Search)""}],""children"":[],""data"":{""label"":""ANN index (Vector Search)"",""icon"":""gcp_vertexai""}},{""id"":""deployed_index"",""labels"":[{""text"":""Deployed index\n(Vector Search endpoint)""}],""children"":[],""data"":{""label"":""Deployed index\n(Vector Search endpoint)"",""icon"":""gcp_vertexai""}},{""id"":""candidate_items"",""labels"":[{""text"":""Candidate items""}],""children"":[],""data"":{""label"":""Candidate items"",""icon"":""third_party_api""}},{""id"":""query_towers"",""labels"":[{""text"":""query_towers""}],""children"":[{""id"":""reg_query"",""labels"":[{""text"":""Registered query tower""}],""children"":[],""data"":{""label"":""Registered query tower"",""icon"":""gcp_vertexai""}},{""id"":""deployed_query"",""labels"":[{""text"":""Deployed query tower\n(online endpoint)""}],""children"":[],""data"":{""label"":""Deployed query tower\n(online endpoint)"",""icon"":""gcp_vertexai""}}],""edges"":[{""id"":""query_to_endpoint"",""sources"":[""reg_query""],""targets"":[""deployed_query""],""labels"":[{""text"":""deploy""}]}],""data"":{""label"":""query_towers"",""style"":{""bg"":""#E3F2FD"",""border"":""#1A73E8""}}}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""data_to_train"",""sources"":[""training_data""],""targets"":[""two_tower_train""],""labels"":[{""text"":""train""}]},{""id"":""train_to_candidate"",""sources"":[""two_tower_train""],""targets"":[""reg_candidate""],""labels"":[{""text"":""register""}]},{""id"":""train_to_query"",""sources"":[""two_tower_train""],""targets"":[""reg_query""],""labels"":[{""text"":""register""}]},{""id"":""candidate_to_batch"",""sources"":[""reg_candidate""],""targets"":[""batch_predict""],""labels"":[{""text"":""batch predict""}]},{""id"":""batch_to_json"",""sources"":[""batch_predict""],""targets"":[""embeddings_json""],""labels"":[{""text"":""write JSON""}]},{""id"":""json_to_index"",""sources"":[""embeddings_json""],""targets"":[""ann_index""],""labels"":[{""text"":""index build""}]},{""id"":""index_to_endpoint"",""sources"":[""ann_index""],""targets"":[""deployed_index""],""labels"":[{""text"":""deploy""}]},{""id"":""items_to_batch"",""sources"":[""candidate_items""],""targets"":[""batch_predict""],""labels"":[{""text"":""items""}]}]}]}"
gcp,ai-ml,ai-ml-applications,https://cloud.google.com/architecture/optimize-ai-ml-workloads-cloud-storage-fuse,"Optimize AI and ML workloads with Cloud Storage FUSE bookmark_border Release Notes Last reviewed 2025-04-09 UTC This document provides reference architectures that show how you can use Cloud Storage FUSE to optimize performance for AI and ML workloads on Google Kubernetes Engine (GKE). The intended audience for this document includes architects and technical practitioners who design, provision, and manage storage for their AI and ML workloads on Google Cloud. This document assumes that you have an understanding of the ML lifecycle, processes, and capabilities. Cloud Storage FUSE is an open source FUSE adapter that lets you mount Cloud Storage buckets as local file systems. This configuration enables applications to seamlessly interact with cloud-based storage buckets by using standard file-like system semantics. Cloud Storage FUSE lets you take advantage of the scalability and cost-effectiveness of Cloud Storage. Note: Cloud Storage FUSE isn't fully compliant with Portable Operating System Interface (POSIX). For information about how to select storage services for AI and ML workloads, see Design storage for AI and ML workloads in Google Cloud. Architecture Depending on your requirements for performance, availability, and disaster recovery (DR), you can choose one of the following Google Cloud deployment archetypes to run your AI and ML workloads on Google Cloud: Regional: your applications run independently within a single Google Cloud region. We recommend this deployment archetype for applications that aren't mission-critical but that need to be robust against zone outages. Multi-regional: your applications run independently across two or more Google Cloud regions, in either active-active or active-passive mode. This deployment archetype is ideal to support DR scenarios. We recommend this deployment archetype for mission-critical applications that need resilience against region outages and disasters. Dual or multi-regional deployments can reduce latency and improve throughput through closer resource proximity. The deployment archetype that you choose informs the Google Cloud products and features that you need for your architecture. The multi-regional architecture uses Anywhere Cache. To assess whether Anywhere Cache is suitable for your workload, use the Anywhere Cache recommender to analyze your data usage and storage. The following tabs provide reference architectures for the regional and multi-regional deployment archetypes: Regional Multi-regional The following diagram shows a sample regional architecture that uses Cloud Storage FUSE to optimize the performance of the model training and model serving workflows: Regional architecture that uses Cloud Storage FUSE to optimize AI and ML workloads. This architecture includes the following components: GKE cluster: GKE manages the compute nodes on which your AI and ML model training and serving processes run. GKE manages the underlying infrastructure of the Kubernetes clusters, including the control plane, nodes, and all system components. Kubernetes scheduler: the GKE control plane schedules workloads and manages their lifecycle, scaling, and upgrades. The Kubernetes node agent (kubelet), which isn't shown in the diagram, communicates with the control plane. The kubelet agent is responsible for starting and running containers that are scheduled on the GKE nodes. For more information about the scheduler, see AI/ML orchestration on GKE. Virtual Private Cloud (VPC) network: all of the Google Cloud resources in the architecture use a single VPC network. Depending on your requirements, you can choose to build an architecture that uses multiple networks. For more information about configuring a VPC network for Cloud Storage FUSE, see Deciding whether to create multiple VPC networks. Cloud Load Balancing: in this architecture, Cloud Load Balancing efficiently distributes incoming inference requests from application users to the serving containers in the GKE cluster. For more information, see Understanding GKE load balancing. Graphics Processing Unit (GPU) or Tensor Processing Units (TPUs): GPUs and TPUs are specialized machine accelerators that improve the performance of your AI and ML workloads. For information about how to choose an appropriate processor type, see Accelerator options later in this document. Cloud Storage: Cloud Storage provides persistent, scalable, and cost-effective storage for your AI and ML workloads. Cloud Storage serves as the central repository for your raw training datasets, model checkpoints, and final trained models. Cloud Storage FUSE with file cache enabled: Cloud Storage FUSE lets you mount a Cloud Storage bucket as a local file system. The file cache in Cloud Storage FUSE is a directory on the local machine that stores frequently accessed files from your Cloud Storage buckets. The Cloud Storage FUSE CSI driver manages the integration of Cloud Storage FUSE with the Kubernetes API to consume Cloud Storage buckets as volumes. The following sections describe the workflow in the training and serving workloads of the architecture. Training workload In the preceding architectures, the following are the steps in the data flow during model training: Load training data to Cloud Storage: training data is uploaded to a Cloud Storage bucket with hierarchical namespaces enabled. Cloud Storage serves as a scalable central repository. Load training data and run training jobs in GKE: the Cloud Storage bucket that's mounted to your GKE pods lets your training applications efficiently load and access the training data by using the FUSE interface. The GKE nodes run the model training process by using the mounted file cache as the data source. Your training applications continuously feed training data to the machine accelerators to perform the complex calculations that are required for model training. Depending on your workload requirements, you can use GPUs or TPUs. For information about how to choose an appropriate processor type, see Accelerator options later in this document. Checkpoint and model save and restore: Save checkpoints or model: during training, save checkpoints asynchronously at frequent intervals to a separate Cloud Storage bucket. The checkpoints capture the state of the model based on metrics or intervals that you define. Restore checkpoints or model: when your training workload requires that you restore a checkpoint or model data, you need to locate the asset that you want to restore in Cloud Storage. You can use the restored checkpoint or model to resume training, fine-tune parameters, or evaluate performance on a validation set. Serving workload In the preceding architectures, the following are the steps in the data flow during model serving: Load model: after training is complete, your pods load the trained model by using Cloud Storage FUSE with parallel downloads enabled. Parallel downloads accelerate model loading by fetching the parts of the model in parallel from Cloud Storage. To significantly reduce model loading times, the process uses the cache directory as a prefetch buffer. Inference request: application users send inference requests from the AI and ML application through the Cloud Load Balancing service. Cloud Load Balancing distributes the incoming requests across the serving containers in the GKE cluster. This distribution ensures that no single container is overwhelmed and requests are processed efficiently. Response delivery: the nodes process the request and generate a prediction. The serving containers send the responses back through Cloud Load Balancing and then to the application users.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""storage"",""labels"":[{""text"":""storage""}],""children"":[{""id"":""training_corpus"",""labels"":[{""text"":""Training data corpus (GCS)""}],""children"":[],""data"":{""label"":""Training data corpus (GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""checkpoint_bucket"",""labels"":[{""text"":""Checkpoint model data (GCS)""}],""children"":[],""data"":{""label"":""Checkpoint model data (GCS)"",""icon"":""gcp_cloud_storage""}}],""edges"":[]},{""id"":""networking"",""labels"":[{""text"":""networking""}],""children"":[{""id"":""cloud_lb"",""labels"":[{""text"":""Cloud Load Balancing""}],""children"":[],""data"":{""label"":""Cloud Load Balancing"",""icon"":""gcp_cloud_load_balancing""}}],""edges"":[]},{""id"":""gke_cluster"",""labels"":[{""text"":""GKE cluster (Autopilot)""}],""children"":[{""id"":""cluster_block"",""labels"":[{""text"":""cluster_block""}],""children"":[{""id"":""training_zone"",""labels"":[{""text"":""training_zone""}],""children"":[{""id"":""training_pod"",""labels"":[{""text"":""Training containers""}],""children"":[],""data"":{""label"":""Training containers"",""icon"":""gcp_google_kubernetes_engine""}},{""id"":""train_cache"",""labels"":[{""text"":""File cache""}],""children"":[],""data"":{""label"":""File cache"",""icon"":""gcp_filestore""}}],""edges"":[{""id"":""train_cache_to_pod"",""sources"":[""train_cache""],""targets"":[""training_pod""],""labels"":[{""text"":""mount""}]}]},{""id"":""serving_zone"",""labels"":[{""text"":""serving_zone""}],""children"":[{""id"":""serving_pod"",""labels"":[{""text"":""Serving containers""}],""children"":[],""data"":{""label"":""Serving containers"",""icon"":""gcp_google_kubernetes_engine""}},{""id"":""serve_cache"",""labels"":[{""text"":""File cache""}],""children"":[],""data"":{""label"":""File cache"",""icon"":""gcp_filestore""}}],""edges"":[{""id"":""serve_cache_to_pod"",""sources"":[""serve_cache""],""targets"":[""serving_pod""],""labels"":[{""text"":""mount""}]}]},{""id"":""k8s_scheduler"",""labels"":[{""text"":""Kubernetes scheduler""}],""children"":[],""data"":{""label"":""Kubernetes scheduler"",""icon"":""gcp_google_kubernetes_engine""}}],""edges"":[]}],""data"":{""label"":""GKE cluster (Autopilot)"",""icon"":""gcp_google_kubernetes_engine""}}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""corpus_to_train_cache"",""sources"":[""training_corpus""],""targets"":[""train_cache""],""labels"":[{""text"":""load data""}]},{""id"":""training_to_checkpoint"",""sources"":[""training_pod""],""targets"":[""checkpoint_bucket""],""labels"":[{""text"":""save checkpoint""}]},{""id"":""checkpoint_to_serve_cache"",""sources"":[""checkpoint_bucket""],""targets"":[""serve_cache""],""labels"":[{""text"":""load model""}]},{""id"":""lb_to_serving"",""sources"":[""cloud_lb""],""targets"":[""serving_pod""],""labels"":[{""text"":""route""}]},{""id"":""serving_to_lb"",""sources"":[""serving_pod""],""targets"":[""cloud_lb""],""labels"":[{""text"":""response""}]}]},{""id"":""users"",""labels"":[{""text"":""users""}],""children"":[{""id"":""training_user"",""labels"":[{""text"":""User""}],""children"":[],""data"":{""label"":""User"",""icon"":""browser_client""}},{""id"":""admins"",""labels"":[{""text"":""Administrators""}],""children"":[],""data"":{""label"":""Administrators"",""icon"":""browser_client""}}],""edges"":[]},{""id"":""applications"",""labels"":[{""text"":""applications""}],""children"":[{""id"":""app_user"",""labels"":[{""text"":""Application users""}],""children"":[],""data"":{""label"":""Application users"",""icon"":""browser_client""}}],""edges"":[]}],""edges"":[{""id"":""user_to_corpus"",""sources"":[""training_user""],""targets"":[""training_corpus""],""labels"":[{""text"":""load training data""}]},{""id"":""user_to_lb"",""sources"":[""app_user""],""targets"":[""cloud_lb""],""labels"":[{""text"":""inference request""}]},{""id"":""lb_to_app_user"",""sources"":[""cloud_lb""],""targets"":[""app_user""],""labels"":[{""text"":""deliver response""}]},{""id"":""admin_to_scheduler"",""sources"":[""admins""],""targets"":[""k8s_scheduler""],""labels"":[{""text"":""manage cluster""}]}]}"
gcp,ai-ml,ai-ml-applications,https://cloud.google.com/architecture/optimize-ai-ml-workloads-managed-lustre,"Optimize AI and ML workloads with Google Cloud Managed Lustre bookmark_border Release Notes Last reviewed 2025-06-02 UTC This document provides a reference architecture that shows how you can use Google Cloud Managed Lustre to optimize performance for AI and ML workloads that are deployed on Google Kubernetes Engine (GKE). The intended audience for this document includes architects and technical practitioners who design, provision, and manage storage for their AI workloads on Google Cloud. The document assumes that you have an understanding of the ML lifecycle, processes, and capabilities. Managed Lustre is a fully Google Cloud-managed, persistent parallel file system that's based on DDN's EXAScaler Lustre. Managed Lustre is ideal for AI workloads that meet these criteria: Require up to 8 PiB of storage capacity. Provide ultra low-latency (sub-millisecond) access with high throughput, up to 1 TB/s. Provide high input/output operations per second (IOPS). Managed Lustre offers these advantages for AI workloads: Lower total cost of ownership (TCO) for training: Managed Lustre reduces training time by efficiently delivering data to compute nodes. This functionality helps to reduce the total cost of ownership for AI and ML model training. Lower TCO for serving: Managed Lustre provides high-performance capabilities that enable faster model loading and optimized inference serving. These capabilities help to lower compute costs and improve resource utilization. Efficient resource utilization: Managed Lustre lets you combine checkpointing and training within a single instance. This resource sharing helps to maximize the efficient use of read and write throughput in a single, high-performance storage system. Architecture The following diagram shows a sample architecture for using Managed Lustre to optimize the performance of a model training workload and serving workload: An architecture uses Managed Lustre to optimize performance of a model training workload and serving workload. The workloads that are shown in the preceding architecture are described in detail in later sections. This architecture includes the following components: Google Kubernetes Engine cluster: GKE manages the compute hosts on which your AI and ML model training and serving processes execute. GKE manages the underlying infrastructure of clusters, including the control plane, nodes, and all system components. Kubernetes Scheduler: The GKE control plane schedules workloads and manages their lifecycle, scaling, and upgrades. Virtual Private Cloud (VPC) network: All of the Google Cloud resources that are in the architecture use a single VPC network. Cloud Load Balancing: In this architecture, Cloud Load Balancing efficiently distributes incoming inference requests from application users to the serving containers in the GKE cluster. The use of Cloud Load Balancing helps to ensure high availability, scalability, and optimal performance for the AI and ML application. For more information, see Understanding GKE load balancing. Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs): GPUs and TPUs are specialized machine accelerators that improve the performance of your AI and ML workload. To help ensure optimal efficiency and compatibility, use the same type of accelerator for your entire AI and ML workload. For more information about how to choose an appropriate processor type, see Accelerator options later in this document. Managed Lustre: Managed Lustre accelerates AI and ML training and serving by providing a high-performance, parallel file system that's optimized for low latency and high throughput. Compared to using Cloud Storage alone, using Managed Lustre significantly reduces training time and improves the responsiveness of your models during serving. These improvements are especially realized in demanding workloads that require fast and consistent access to shared data. Cloud Storage FUSE: Cloud Storage FUSE provides persistent and cost-effective storage for your AI and ML workloads. Cloud Storage serves as the central repository for your raw training datasets, model checkpoints, and model backups. Using Cloud Storage helps to ensure data durability, long-term availability, and cost-efficiency for data that isn't actively being used in computations. Training workload In the preceding architecture, the following are the steps in the data flow during model training: Upload training data to Cloud Storage: You upload training data to a Cloud Storage bucket, which serves as a secure and scalable central repository and source of truth. Copy data to Managed Lustre: The training data corpus is transferred through an API to import data to a Managed Lustre instance from Cloud Storage. Transferring the training data lets you take advantage of Managed Lustre's high-performance file system capabilities to optimize data loading and processing speeds during model training. Run training jobs in GKE: The model training process runs on GKE nodes. By using Managed Lustre as the data source instead of loading data from Cloud Storage directly, the GKE nodes can access and load training data with significantly increased speed and lower latency. Managed Lustre also enables shorter time for the transfer of the first byte to begin as measured by time to first byte (TTFB). Using Managed Lustre helps to reduce data loading times and accelerate the overall training process, especially for large datasets that have small read files and complex models. Depending on your workload requirements, you can use GPUs or TPUs. For information about how to choose an appropriate processor type, see Accelerator options later in this document. Save training checkpoints to Managed Lustre: During the training process, checkpoints are saved to Managed Lustre based on metrics or intervals that you define. The checkpoints capture the state of the model at frequent intervals. Serving workload In the preceding architecture, the following are the steps in the data flow during model serving: Load model for serving: When your model is ready for deployment, your GKE Pods load the trained model from your Managed Lustre instance to the serving nodes. If the Managed Lustre instance that you used during training has sufficient IOPS capacity and if it's in the same zone as your accelerators, you can use the same Managed Lustre instance to serve your model. Reusing the Managed Lustre instance enables efficient resource sharing between training and serving. To maintain optimal performance and compatibility, use the same GPU or TPU processor type that you selected for your serving GKE nodes. Inference request: Application users send inference requests through the serving endpoints. These requests are directed to the Cloud Load Balancing service. Cloud Load Balancing distributes the incoming requests across the serving containers in the GKE cluster. This distribution ensures that no single container is overwhelmed and that requests are processed efficiently. Serving inference requests: When an inference request is received, the compute nodes access the pre-loaded model to perform the necessary computations and generate a prediction. Response delivery: The serving containers send the responses back through Cloud Load Balancing. Cloud Load Balancing routes the responses back to the appropriate application users, which completes the inference request cycle.","{""id"":""root"",""children"":[{""id"":""gcp"",""labels"":[{""text"":""Google Cloud""}],""children"":[{""id"":""cloud_lb"",""labels"":[{""text"":""Cloud Load Balancing""}],""children"":[],""data"":{""label"":""Cloud Load Balancing"",""icon"":""gcp_cloud_load_balancing""}},{""id"":""gke_cluster"",""labels"":[{""text"":""GKE cluster (Autopilot)""}],""children"":[{""id"":""cluster_block"",""labels"":[{""text"":""cluster_block""}],""children"":[{""id"":""training_zone"",""labels"":[{""text"":""training_zone""}],""children"":[{""id"":""training_pod"",""labels"":[{""text"":""Training containers""}],""children"":[],""data"":{""label"":""Training containers"",""icon"":""gcp_google_kubernetes_engine""}},{""id"":""train_cache"",""labels"":[{""text"":""File cache (train)""}],""children"":[],""data"":{""label"":""File cache (train)"",""icon"":""gcp_filestore""}}],""edges"":[]},{""id"":""serving_zone"",""labels"":[{""text"":""serving_zone""}],""children"":[{""id"":""serving_pod"",""labels"":[{""text"":""Serving containers""}],""children"":[],""data"":{""label"":""Serving containers"",""icon"":""gcp_google_kubernetes_engine""}},{""id"":""serve_cache"",""labels"":[{""text"":""File cache (serve)""}],""children"":[],""data"":{""label"":""File cache (serve)"",""icon"":""gcp_filestore""}}],""edges"":[]},{""id"":""k8s_sched"",""labels"":[{""text"":""Kubernetes scheduler""}],""children"":[],""data"":{""label"":""Kubernetes scheduler"",""icon"":""gcp_google_kubernetes_engine""}}],""edges"":[{""id"":""edge_sched_to_training_zone"",""sources"":[""k8s_sched""],""targets"":[""training_zone""],""labels"":[{""text"":""manage training zone""}]},{""id"":""edge_sched_to_serving_zone"",""sources"":[""k8s_sched""],""targets"":[""serving_zone""],""labels"":[{""text"":""manage serving zone""}]}]},{""id"":""lustre_dataset"",""labels"":[{""text"":""Working dataset (Managed Lustre)""}],""children"":[],""data"":{""label"":""Working dataset (Managed Lustre)"",""icon"":""gcp_filestore""}}],""data"":{""label"":""GKE cluster (Autopilot)"",""icon"":""gcp_google_kubernetes_engine""},""edges"":[{""id"":""a_load_model"",""sources"":[""lustre_dataset""],""targets"":[""serving_pod""],""labels"":[{""text"":""load model""}]},{""id"":""e4_save_ckpt"",""sources"":[""training_pod""],""targets"":[""lustre_dataset""],""labels"":[{""text"":""save checkpoint""}]},{""id"":""e3_load_data"",""sources"":[""lustre_dataset""],""targets"":[""training_pod""],""labels"":[{""text"":""load data""}]}]},{""id"":""data_storage"",""labels"":[{""text"":""data_storage""}],""children"":[{""id"":""training_corpus"",""labels"":[{""text"":""Training data corpus (GCS)""}],""children"":[],""data"":{""label"":""Training data corpus (GCS)"",""icon"":""gcp_cloud_storage""}},{""id"":""data_backup"",""labels"":[{""text"":""Data backup (GCS)""}],""children"":[],""data"":{""label"":""Data backup (GCS)"",""icon"":""gcp_cloud_storage""}}],""edges"":[]}],""data"":{""label"":""Google Cloud"",""icon"":""gcp_logo""},""edges"":[{""id"":""e2_bulk_import"",""sources"":[""training_corpus""],""targets"":[""lustre_dataset""],""labels"":[{""text"":""bulk import""}]},{""id"":""backup_data"",""sources"":[""lustre_dataset""],""targets"":[""data_backup""],""labels"":[{""text"":""backup data""}]},{""id"":""c_route"",""sources"":[""cloud_lb""],""targets"":[""serving_zone""],""labels"":[{""text"":""route request""}]}]},{""id"":""data_engineers"",""labels"":[{""text"":""data_engineers""}],""children"":[{""id"":""data_eng"",""labels"":[{""text"":""Data Engineers""}],""children"":[],""data"":{""label"":""Data Engineers"",""icon"":""browser_client""}}],""edges"":[]},{""id"":""application_users"",""labels"":[{""text"":""application_users""}],""children"":[{""id"":""app_users"",""labels"":[{""text"":""Application Users""}],""children"":[],""data"":{""label"":""Application Users"",""icon"":""browser_client""}}],""edges"":[]},{""id"":""administrators"",""labels"":[{""text"":""administrators""}],""children"":[{""id"":""admins"",""labels"":[{""text"":""Administrators""}],""children"":[],""data"":{""label"":""Administrators"",""icon"":""browser_client""}}],""edges"":[]}],""edges"":[{""id"":""e1_load_corpus"",""sources"":[""data_eng""],""targets"":[""training_corpus""],""labels"":[{""text"":""load training data""}]},{""id"":""admin_to_sched"",""sources"":[""admins""],""targets"":[""k8s_sched""],""labels"":[{""text"":""manage cluster""}]},{""id"":""b_request"",""sources"":[""app_users""],""targets"":[""cloud_lb""],""labels"":[{""text"":""inference request""}]}]}"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/architecture,"Architecture bookmark_border Last reviewed 2024-12-13 UTC The following diagram shows the high-level architecture that is deployed by the blueprint for a single environment. You deploy this architecture across three separate environments: production, non-production, and development. The blueprint architecture. This diagram includes the following: Cloud Load Balancing distributes application traffic across regions to Kubernetes service objects. Behind each service is a logical grouping of related pods. Cloud Service Mesh lets Kubernetes services communicate with each other. Kubernetes services are grouped into tenants, which are represented as Kubernetes namespaces. Tenants are an abstraction that represent multiple users and workloads that operate in a cluster, with separate RBAC for access control. Each tenant also has its own project for tenant-specific cloud resources such as databases, storage buckets, and Pub/Sub subscriptions. Namespaces with their own identities for accessing peer services and cloud resources. The identity is consistent across the same namespace in different clusters because of fleet Workload Identity Federation for GKE. Each environment has a separate workload identity pool to mitigate privilege escalation between environments. Each service has a dedicated pipeline that builds and deploys that service. The same pipeline is used to deploy the service into the development environment, then deploy the service into the non-production environment, and finally deploy the service into the production environment. Key architectural decisions for developer platform The following table describes the architecture decisions that the blueprint implements. Decision area	Decision	Reason Deployment archetype Deploy across multiple regions. Permit availability of applications during region outages. Organizational architecture Deploy on top of the enterprise foundation blueprint. Use the organizational structure and security controls that are provided by the foundation. Use the three environment folders that are set up in the foundation: development, nonproduction, and production. Provide isolation for environments that have different access controls. Developer platform cluster architecture Package and deploy applications as containers. Support separation of responsibilities, efficient operations, and application portability. Run applications on GKE clusters. Use a managed container service that is built by the company that pioneered containers. Replicate and run application containers in an active-active configuration. Achieve higher availability and rapid progressive rollouts, improving development velocity. Provision the production environment with two GKE clusters in two different regions. Achieve higher availability than a single cloud region. Provision the non-production environment with two GKE clusters in two different regions. Stage changes to cross-regional settings, such as load balancers, before deployment to production. Provision the development environment with a single GKE cluster instance. Helps reduce cost. Configure highly-available control planes for each GKE cluster. Ensure that the cluster control plane is available during upgrade and resizing. Use the concept of sameness across namespaces, services, and identity in each GKE cluster. Ensure that Kubernetes objects with the same name in different clusters are treated as the same thing. This normalization is done to make administering fleet resources easier. Enable private IP address spaces for GKE clusters through Private Service Connect access to the control plane and private node pools. Help protect the Kubernetes cluster API from scanning attacks. Enable administrative access to the GKE clusters through the Connect gateway. Use one command to fetch credentials for access to multiple clusters. Use groups and third-party identity providers to manage cluster access. Use Cloud NAT to provide GKE pods with access to resources with public IP addresses. Improve the overall security posture of the cluster, because pods are not directly exposed to the internet, but are still able to access internet-facing resources. Configure nodes to use Container-Optimized OS and Shielded GKE Nodes. Limit the attack surface of the nodes. Associate each environment with a GKE fleet. Permit management of sets of GKE clusters as a unit. Use the foundation infrastructure pipeline to deploy the application factory, fleet-scope pipeline, and multi-tenant infrastructure pipeline. Provide a controllable, auditable, and repeatable mechanism to deploy application infrastructure. Configure GKE clusters using GKE Enterprise configuration and policy management features. Provide a service that allows configuration-as-code for GKE clusters. Use an application factory to deploy the application CI/CD pipelines used in the blueprint. Provide a repeatable pattern to deploy application pipelines more easily. Use an application CI/CD pipeline to build and deploy the blueprint application components. Provide a controllable, auditable, and repeatable mechanism to deploy applications. Configure the application CI/CD pipeline to use Cloud Build, Cloud Deploy, and Artifact Registry. Use managed build and deployment services to optimize for security, scale, and simplicity. Use immutable containers across environments, and sign the containers with Binary Authorization. Provide clear code provenance and ensure that code has been tested across environments. Use Google Cloud Observability, which includes Cloud Logging and Cloud Monitoring. Simplify operations by using an integrated managed service of Google Cloud. Enable Container Threat Detection (a service in Security Command Center) to monitor the integrity of containers. Use a managed service that enhances security by continually monitoring containers. Control access to the GKE clusters by Kubernetes role-based access control (RBAC), which is based on Google Groups for GKE. Enhance security by linking access control to Google Cloud identities. Service architecture Use a unique Kubernetes service account for each Kubernetes service. This account acts as an IAM service account through the use of Workload Identity Federation for GKE. Enhance security by minimizing the permissions each service needs to be provided. Expose services through the GKE Gateway API. Simplify configuration management by providing a declarative-based and resource-based approach to managing ingress rules and load-balancing configurations. Run services as distributed services through the use of Cloud Service Mesh with Certificate Authority Service. Provide enhanced security through enforcing authentication between services and also provides automatic fault tolerance by redirecting traffic away from unhealthy services. Use cross-region replication for AlloyDB for PostgreSQL. Provide for high-availability in the database layer. Network architecture Shared VPC instances are configured in each environment and GKE clusters are created in service projects. Shared VPC provides centralized network configuration management while maintaining separation of environments. Use Cloud Load Balancing in a multi-cluster, multi-region configuration. Provide a single anycast IP address to access regionalized GKE clusters for high availability and low-latency services. Use HTTPS connections for client access to services. Redirect any client HTTP requests to HTTPS. Help protect sensitive data in transit and help prevent person-in-the-middle-attacks. Use Certificate Manager to manage public certificates. Manage certificates in a unified way. Protect the web interface with Google Cloud Armor. Enhance security by protecting against common web application vulnerabilities and volumetric attacks.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""web_user"", ""labels"": [ { ""text"": ""User"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_cloud"", ""labels"": [ { ""text"": ""gcp_cloud"" } ], ""children"": [ { ""id"": ""infrastructure_group"", ""labels"": [ { ""text"": ""infrastructure_group"" } ], ""children"": [ { ""id"": ""networking"", ""labels"": [ { ""text"": ""networking"" } ], ""children"": [ { ""id"": ""multi_cluster_gateway"", ""labels"": [ { ""text"": ""Multi-cluster Gateway"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_load_balancer"", ""labels"": [ { ""text"": ""Cloud Load Balancer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_armor"", ""labels"": [ { ""text"": ""Cloud Armor"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_gateway_lb"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_armor_lb"", ""labels"": [ { ""text"": ""protects"" } ] } ] }, { ""id"": ""fleet_config"", ""labels"": [ { ""text"": ""fleet_config"" } ], ""children"": [ { ""id"": ""config_sync"", ""labels"": [ { ""text"": ""Config Sync"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""workload_identity_pool"", ""labels"": [ { ""text"": ""Workload Identity Pool"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""policy_controller"", ""labels"": [ { ""text"": ""Policy Controller"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""service_mesh"", ""labels"": [ { ""text"": ""Service Mesh"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""observability"", ""labels"": [ { ""text"": ""observability"" } ], ""children"": [ { ""id"": ""cloud_logging_node"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring_node"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] }, { ""id"": ""clusters_group"", ""labels"": [ { ""text"": ""clusters_group"" } ], ""children"": [ { ""id"": ""reg1_cluster"", ""labels"": [ { ""text"": ""GKE Cluster us-west1"" } ], ""children"": [ { ""id"": ""tenant1_uswest1"", ""labels"": [ { ""text"": ""tenant1_uswest1"" } ], ""children"": [ { ""id"": ""service_app_a_west1"", ""labels"": [ { ""text"": ""Service app-a"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""tenant2_uswest1"", ""labels"": [ { ""text"": ""tenant2_uswest1"" } ], ""children"": [ { ""id"": ""service_app_b_west1"", ""labels"": [ { ""text"": ""Service app-b"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""service_app_c_west1"", ""labels"": [ { ""text"": ""Service app-c"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_appb_appc_w1"", ""labels"": [ { ""text"": ""calls"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_reg1_appa"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_reg1_appb"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_reg1_appc"", ""labels"": [ { ""text"": ""routes"" } ] } ], ""data"": {} }, { ""id"": ""reg2_cluster"", ""labels"": [ { ""text"": ""GKE Cluster us-east1"" } ], ""children"": [ { ""id"": ""tenant1_useast1"", ""labels"": [ { ""text"": ""tenant1_useast1"" } ], ""children"": [ { ""id"": ""service_app_a_east1"", ""labels"": [ { ""text"": ""Service app-a"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""tenant2_useast1"", ""labels"": [ { ""text"": ""tenant2_useast1"" } ], ""children"": [ { ""id"": ""service_app_b_east1"", ""labels"": [ { ""text"": ""Service app-b"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""service_app_c_east1"", ""labels"": [ { ""text"": ""Service app-c"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_appb_appc_e1"", ""labels"": [ { ""text"": ""calls"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_reg2_appa"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_reg2_appb"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_reg2_appc"", ""labels"": [ { ""text"": ""routes"" } ] } ], ""data"": {} } ], ""edges"": [] }, { ""id"": ""cicd_group"", ""labels"": [ { ""text"": ""cicd_group"" } ], ""children"": [ { ""id"": ""multi_tenant_cicd"", ""labels"": [ { ""text"": ""multi_tenant_cicd"" } ], ""children"": [ { ""id"": ""multi_tenant_pipeline"", ""labels"": [ { ""text"": ""Infra Plan & Apply"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""application_cicd"", ""labels"": [ { ""text"": ""application_cicd"" } ], ""children"": [ { ""id"": ""cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""artifact_registry"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_deploy"", ""labels"": [ { ""text"": ""Cloud Deploy"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""single_tenant_pipeline"", ""labels"": [ { ""text"": ""Single-tenant Plan & Apply"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_cb_ar"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""edge_ar_cd"", ""labels"": [ { ""text"": ""provides"" } ] } ] } ], ""edges"": [] }, { ""id"": ""application_group"", ""labels"": [ { ""text"": ""application_group"" } ], ""children"": [ { ""id"": ""application_project"", ""labels"": [ { ""text"": ""application_project"" } ], ""children"": [ { ""id"": ""app_region_west1"", ""labels"": [ { ""text"": ""App Project us-west1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""app_region_east1"", ""labels"": [ { ""text"": ""App Project us-east1"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_lb_reg1"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_lb_reg2"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_fleet_reg1"", ""labels"": [ { ""text"": ""configures"" } ] }, { ""id"": ""edge_fleet_reg2"", ""labels"": [ { ""text"": ""configures"" } ] }, { ""id"": ""edge_reg1_logging"", ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""edge_reg1_monitor"", ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""edge_reg2_logging"", ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""edge_reg2_monitor"", ""labels"": [ { ""text"": ""metrics"" } ] }, { ""id"": ""edge_mt_reg1"", ""labels"": [ { ""text"": ""manages"" } ] }, { ""id"": ""edge_mt_reg2"", ""labels"": [ { ""text"": ""manages"" } ] }, { ""id"": ""edge_cd_reg1"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_cd_reg2"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_pipe_reg1"", ""labels"": [ { ""text"": ""manages"" } ] }, { ""id"": ""edge_pipe_reg2"", ""labels"": [ { ""text"": ""manages"" } ] }, { ""id"": ""edge_reg1_appwest"", ""labels"": [ { ""text"": ""traffic"" } ] }, { ""id"": ""edge_reg2_appeast"", ""labels"": [ { ""text"": ""traffic"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_user_gateway"", ""labels"": [ { ""text"": ""accesses"" } ] } ] }"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/developer-platform-controls,"Developer platform controls bookmark_border Last reviewed 2024-12-13 UTC This section describes the controls that are used in the developer platform. Platform identity, roles, and groups Access to Google Cloud services requires Google Cloud identities. The blueprint uses fleet Workload Identity Federation for GKE to map the Kubernetes service accounts that are used as identities for pods to Google Cloud service accounts that control access to Google Cloud services. To help protect against cross-environment escalations, each environment has a separate identity pool (known as a set of trusted identity providers) for its Workload Identity Federation for GKE accounts. Platform personas When you deploy the blueprint, you create three types of user groups: a developer platform team, an application team (one team per application), and a security operations team. The developer platform team is responsible for development and management of the developer platform. The members of this team are the following: Developer platform developers: These team members extend the blueprint and integrate it into existing systems. This team also creates new application templates. Developer platform administrator: This team is responsible for the following: Approving the creation of new tenants teams. Performing scheduled and unscheduled tasks that affect multiple tenants, including the following: Approving the promotion of applications to the nonproduction environment and the production environment. Coordinating infrastructure updates. Making platform-level capacity plans. A tenant of the developer platform is a single software development team and those responsible for the operation of that software. The tenant team consists of two groups: application developers and application operators. The duties of the two groups of the tenant team are as follows: Application developers: This team writes and debugs application code. They are sometimes also called software engineers or full-stack developers. Their responsibilities include the following: Performing testing and quality assurance on an application component when it is deployed into the development environment. Managing application-owned cloud resources (such as databases and storage buckets) in the development environment. Designing database or storage schemas for use by applications. Application operators or site reliability engineers (SREs): This team manages the reliability of applications that are running in the production environments, and the safe advancement of changes made by application developers into production. They are sometimes called service operators, systems engineers, or system administrators. Their responsibilities include the following: Planning application-level capacity needs. Creating alerting policies and setting service level objectives (SLOs) for services. Diagnosing service issues using logs and metrics that are specific to that application. Responding to alerts and pages, such as when a service doesn't meet its SLOs. Working with a group or several groups of application developers. Approving deployment of new versions to production. Managing application-owned cloud resources in the non-production and production environments (for example, backups and schema updates). Platform organization structure The enterprise application blueprint uses the organization structure that is provided by the enterprise foundation blueprint. The following diagram shows how the enterprise application blueprint projects fit within the structure of the foundation blueprint. The blueprint projects and folders. Platform projects The following table describes the additional projects, beyond those provided by the foundation blueprint, that the application blueprint needs for deploying resources, configurations, and applications. Folder	Project	Description common eab-infra-cicd Contains the multi-tenant infrastructure pipeline to deploy the tenant infrastructure. eab-app-factory Contains the application factory , which is used to create single-tenant application architecture and continuous integration and continuous deployment (CI/CD) pipelines. This project also contains the Config Sync that's used for GKE cluster configuration. eab-{tenant}-cicd Contains the application CI/CD pipelines, which are in independent projects to enable separation between development teams. There is one pipeline for each application. development, nonproduction, production eab-gke Contains the GKE clusters for the developer platform and the logic that is used to register clusters for fleet management. eab-{tenant} (1-n) Contains any single-tenant application services such as databases or other managed services that are used by an application team.","{ ""id"": ""root"", ""children"": [ { ""id"": ""external_identity"", ""labels"": [ { ""text"": ""external_identity"" } ], ""children"": [ { ""id"": ""example_identity"", ""labels"": [ { ""text"": ""Example.com Cloud Identity"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""google_cloud"", ""labels"": [ { ""text"": ""google_cloud"" } ], ""children"": [ { ""id"": ""organization"", ""labels"": [ { ""text"": ""Organization"" } ], ""children"": [ { ""id"": ""platform_segments"", ""labels"": [ { ""text"": ""platform_segments"" } ], ""children"": [ { ""id"": ""bootstrap"", ""labels"": [ { ""text"": ""bootstrap"" } ], ""children"": [ { ""id"": ""bootstrap_config"", ""labels"": [ { ""text"": ""Bootstrap Config"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""common"", ""labels"": [ { ""text"": ""common"" } ], ""children"": [ { ""id"": ""common_components"", ""labels"": [ { ""text"": ""common_components"" } ], ""children"": [ { ""id"": ""eab_infra_cloud"", ""labels"": [ { ""text"": ""eab_infra_cloud"" } ], ""children"": [ { ""id"": ""multi_tenant_pipeline"", ""labels"": [ { ""text"": ""Multi-tenant Infrastructure pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_app_factory"", ""labels"": [ { ""text"": ""eab_app_factory"" } ], ""children"": [ { ""id"": ""application_factory_pipeline"", ""labels"": [ { ""text"": ""Application factory pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_app_cicd_01"", ""labels"": [ { ""text"": ""eab_app_cicd_01"" } ], ""children"": [ { ""id"": ""cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""artifact_registry"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_deploy"", ""labels"": [ { ""text"": ""Google Cloud Deploy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_cloudbuild_artifact"", ""labels"": [ { ""text"": ""pushes"" } ] }, { ""id"": ""edge_cloudbuild_deploy"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_artifact_deploy"", ""labels"": [ { ""text"": ""sources"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_mtpipeline_cloudbuild"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""edge_appfactory_cloudbuild"", ""labels"": [ { ""text"": ""triggers"" } ] } ] } ], ""edges"": [] }, { ""id"": ""environments"", ""labels"": [ { ""text"": ""environments"" } ], ""children"": [ { ""id"": ""production"", ""labels"": [ { ""text"": ""production"" } ], ""children"": [ { ""id"": ""prod_components"", ""labels"": [ { ""text"": ""prod_components"" } ], ""children"": [ { ""id"": ""eab_boot_prod"", ""labels"": [ { ""text"": ""eab_boot_prod"" } ], ""children"": [ { ""id"": ""boot_config_prod"", ""labels"": [ { ""text"": ""eab-boot-prod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_gke_prod"", ""labels"": [ { ""text"": ""eab_gke_prod"" } ], ""children"": [ { ""id"": ""cluster1_prod"", ""labels"": [ { ""text"": ""cluster-1-prod"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cluster2_prod"", ""labels"": [ { ""text"": ""cluster-2-prod"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring_prod"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_logging_prod"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_cluster1_monitoring_prod"", ""labels"": [ { ""text"": ""sends metrics"" } ] }, { ""id"": ""edge_cluster2_monitoring_prod"", ""labels"": [ { ""text"": ""sends metrics"" } ] }, { ""id"": ""edge_cluster1_logging_prod"", ""labels"": [ { ""text"": ""sends logs"" } ] }, { ""id"": ""edge_cluster2_logging_prod"", ""labels"": [ { ""text"": ""sends logs"" } ] } ] }, { ""id"": ""eab_app_tenant1_prod"", ""labels"": [ { ""text"": ""eab_app_tenant1_prod"" } ], ""children"": [ { ""id"": ""tenant1_service_a_prod"", ""labels"": [ { ""text"": ""tenant-1-service-a-prod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_app_tenantn_prod"", ""labels"": [ { ""text"": ""eab_app_tenantn_prod"" } ], ""children"": [ { ""id"": ""tenantn_service_a_prod"", ""labels"": [ { ""text"": ""tenant-n-service-a-prod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_cluster1_service_prod"", ""labels"": [ { ""text"": ""hosts"" } ] } ] } ], ""edges"": [] }, { ""id"": ""nonproduction"", ""labels"": [ { ""text"": ""nonproduction"" } ], ""children"": [ { ""id"": ""nonprod_components"", ""labels"": [ { ""text"": ""nonprod_components"" } ], ""children"": [ { ""id"": ""eab_boot_nonprod"", ""labels"": [ { ""text"": ""eab_boot_nonprod"" } ], ""children"": [ { ""id"": ""boot_config_nonprod"", ""labels"": [ { ""text"": ""eab-boot-nonprod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_gke_nonprod"", ""labels"": [ { ""text"": ""eab_gke_nonprod"" } ], ""children"": [ { ""id"": ""cluster1_nonprod"", ""labels"": [ { ""text"": ""cluster-1-nonprod"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cluster2_nonprod"", ""labels"": [ { ""text"": ""cluster-2-nonprod"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring_nonprod"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_logging_nonprod"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_cluster1_monitoring_nonprod"", ""labels"": [ { ""text"": ""sends metrics"" } ] }, { ""id"": ""edge_cluster2_monitoring_nonprod"", ""labels"": [ { ""text"": ""sends metrics"" } ] }, { ""id"": ""edge_cluster1_logging_nonprod"", ""labels"": [ { ""text"": ""sends logs"" } ] }, { ""id"": ""edge_cluster2_logging_nonprod"", ""labels"": [ { ""text"": ""sends logs"" } ] } ] }, { ""id"": ""eab_app_tenant1_nonprod"", ""labels"": [ { ""text"": ""eab_app_tenant1_nonprod"" } ], ""children"": [ { ""id"": ""tenant1_service_a_nonprod"", ""labels"": [ { ""text"": ""tenant-1-service-a-nonprod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_app_tenantn_nonprod"", ""labels"": [ { ""text"": ""eab_app_tenantn_nonprod"" } ], ""children"": [ { ""id"": ""tenantn_service_a_nonprod"", ""labels"": [ { ""text"": ""tenant-n-service-a-nonprod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_cluster1_service_nonprod"", ""labels"": [ { ""text"": ""hosts"" } ] } ] } ], ""edges"": [] }, { ""id"": ""development"", ""labels"": [ { ""text"": ""development"" } ], ""children"": [ { ""id"": ""dev_components"", ""labels"": [ { ""text"": ""dev_components"" } ], ""children"": [ { ""id"": ""eab_boot_dev"", ""labels"": [ { ""text"": ""eab_boot_dev"" } ], ""children"": [ { ""id"": ""boot_config_dev"", ""labels"": [ { ""text"": ""eab-boot-dev"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_gke_dev"", ""labels"": [ { ""text"": ""eab_gke_dev"" } ], ""children"": [ { ""id"": ""cluster1_dev"", ""labels"": [ { ""text"": ""cluster-1-dev"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring_dev"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_logging_dev"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_cluster1_monitoring_dev"", ""labels"": [ { ""text"": ""sends metrics"" } ] }, { ""id"": ""edge_cluster1_logging_dev"", ""labels"": [ { ""text"": ""sends logs"" } ] } ] }, { ""id"": ""eab_app_tenant1_dev"", ""labels"": [ { ""text"": ""eab_app_tenant1_dev"" } ], ""children"": [ { ""id"": ""tenant1_service_a_dev"", ""labels"": [ { ""text"": ""tenant-1-service-a-dev"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""eab_app_tenantn_dev"", ""labels"": [ { ""text"": ""eab_app_tenantn_dev"" } ], ""children"": [ { ""id"": ""tenantn_service_a_dev"", ""labels"": [ { ""text"": ""tenant-n-service-a-dev"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_cluster1_service_dev"", ""labels"": [ { ""text"": ""hosts"" } ] } ] } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_deploy_cluster1_prod"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_deploy_cluster2_prod"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_deploy_cluster1_nonprod"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_deploy_cluster2_nonprod"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_deploy_cluster1_dev"", ""labels"": [ { ""text"": ""deploys"" } ] } ] } ], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_identity_platform_segments"", ""labels"": [ { ""text"": ""authenticates"" } ] } ] }"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/developer-platform-controls,"Platform cluster architecture The blueprint deploys applications across three environments: development, non-production, and production. Each environment is associated with a fleet. In this blueprint, a fleet is a project that includes one or more clusters. However, fleets can also group several projects. A fleet provides a logical security boundary for administrative control. A fleet provides a way to logically group and normalize Kubernetes clusters, and makes administration of infrastructure easier. The following diagram shows two GKE clusters, which are created in each environment to deploy applications. The two clusters act as identical GKE clusters in two different regions to provide multi-region resiliency. To take advantage of fleet capabilities, the blueprint uses the concept of sameness across namespace objects, services, and identity. Blueprint clusters. The enterprise application blueprint uses GKE clusters with private spaces enabled through Private Service Connect access to the control plane and private node pools to remove potential attack surfaces from the internet. Neither the cluster nodes nor the control plane has a public endpoint. The cluster nodes run Container-Optimized OS to limit their attack surface and the cluster nodes use Shielded GKE Nodes to limit the ability of an attacker to impersonate a node. Administrative access to the GKE clusters is enabled through the Connect gateway. As part of the blueprint deployment, one Cloud NAT instance is used for each environment to give pods and Config Sync a mechanism to access resources on the internet such as the GitHub. Access to the GKE clusters is controlled by Kubernetes RBAC authorization that is based on Google Groups for GKE. Groups let you control identities using a central identity management system that's controlled by identity administrators.","{ ""id"": ""root"", ""children"": [ { ""id"": ""external"", ""labels"": [ { ""text"": ""external"" } ], ""children"": [ { ""id"": ""internet"", ""labels"": [ { ""text"": ""Internet"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_environments"", ""labels"": [ { ""text"": ""gcp_environments"" } ], ""children"": [ { ""id"": ""production_folder"", ""labels"": [ { ""text"": ""production_folder"" } ], ""children"": [ { ""id"": ""production_shared_vpc_host_project"", ""labels"": [ { ""text"": ""production_shared_vpc_host_project"" } ], ""children"": [ { ""id"": ""prod_shared_vpc_network"", ""labels"": [ { ""text"": ""Shared VPC Network"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""production_cluster_project"", ""labels"": [ { ""text"": ""production_cluster_project"" } ], ""children"": [ { ""id"": ""region1"", ""labels"": [ { ""text"": ""region1"" } ], ""children"": [ { ""id"": ""subnet1"", ""labels"": [ { ""text"": ""subnet1"" } ], ""children"": [ { ""id"": ""nat_region1"", ""labels"": [ { ""text"": ""Cloud NAT"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""clb_region1"", ""labels"": [ { ""text"": ""Cloud Load Balancing"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gke_region1"", ""labels"": [ { ""text"": ""Google Kubernetes Engine"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_clb1_gke1"", ""labels"": [ { ""text"": ""distributes to"" } ] }, { ""id"": ""e_gke1_nat1"", ""labels"": [ { ""text"": ""egress via"" } ] } ] } ], ""edges"": [] }, { ""id"": ""region2"", ""labels"": [ { ""text"": ""region2"" } ], ""children"": [ { ""id"": ""subnet2"", ""labels"": [ { ""text"": ""subnet2"" } ], ""children"": [ { ""id"": ""nat_region2"", ""labels"": [ { ""text"": ""Cloud NAT"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""clb_region2"", ""labels"": [ { ""text"": ""Cloud Load Balancing"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gke_region2"", ""labels"": [ { ""text"": ""Google Kubernetes Engine"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_clb2_gke2"", ""labels"": [ { ""text"": ""distributes to"" } ] }, { ""id"": ""e_gke2_nat2"", ""labels"": [ { ""text"": ""egress via"" } ] } ] } ], ""edges"": [] }, { ""id"": ""connect_gateway"", ""labels"": [ { ""text"": ""Connect Gateway Endpoint"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""shared_vpc_attachment"", ""labels"": [ { ""text"": ""Shared VPC Attachment"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_cluster_network"", ""labels"": [ { ""text"": ""uses network"" } ] } ] }, { ""id"": ""nonprod_folder"", ""labels"": [ { ""text"": ""nonprod_folder"" } ], ""children"": [ { ""id"": ""nonprod_shared_vpc_host_project"", ""labels"": [ { ""text"": ""nonprod_shared_vpc_host_project"" } ], ""children"": [ { ""id"": ""nonprod_shared_vpc_network"", ""labels"": [ { ""text"": ""Shared VPC Network"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""nonprod_cluster_project"", ""labels"": [ { ""text"": ""nonprod_cluster_project"" } ], ""children"": [ { ""id"": ""nonprod_cluster_placeholder"", ""labels"": [ { ""text"": ""Non-prod Cluster"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""nonprod_edge_cluster_network"", ""labels"": [ { ""text"": ""uses network"" } ] } ] }, { ""id"": ""dev_folder"", ""labels"": [ { ""text"": ""dev_folder"" } ], ""children"": [ { ""id"": ""dev_shared_vpc_host_project"", ""labels"": [ { ""text"": ""dev_shared_vpc_host_project"" } ], ""children"": [ { ""id"": ""dev_shared_vpc_network"", ""labels"": [ { ""text"": ""Shared VPC Network"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""dev_cluster_project"", ""labels"": [ { ""text"": ""dev_cluster_project"" } ], ""children"": [ { ""id"": ""dev_cluster_placeholder"", ""labels"": [ { ""text"": ""Development Cluster"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""dev_edge_cluster_network"", ""labels"": [ { ""text"": ""uses network"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_internet_clb1"", ""labels"": [ { ""text"": ""routes to"" } ] }, { ""id"": ""e_internet_clb2"", ""labels"": [ { ""text"": ""routes to"" } ] }, { ""id"": ""e_nat1_internet"", ""labels"": [ { ""text"": ""egress to"" } ] }, { ""id"": ""e_nat2_internet"", ""labels"": [ { ""text"": ""egress to"" } ] } ] }"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/service-architecture,"Service architecture bookmark_border Last reviewed 2024-12-13 UTC A Kubernetes service is an abstraction that lets you expose a set of pods as a single entity. Services are fundamental building blocks for exposing and managing containerized applications in a Kubernetes cluster. Services in this blueprint are architected in a standardized manner in consideration to namespaces, identity, service exposure, and service-to-service communication. Namespaces Each namespace has its own set of resources, such as pods, services, and deployments. Namespaces let you organize your applications and isolate them from each other. The blueprint uses namespaces to group services by their purpose. For example, you can create a namespace for all your frontend services and a namespace for your backend services. This grouping makes it easier to manage your services and to control access to them. Service exposure A service is exposed to the internet through the GKE Gateway controller. The GKE Gateway controller creates a load balancer using Cloud Load Balancing in a multi-cluster, multi-region configuration. Cloud Load Balancing uses Google's network infrastructure to provide the service with an anycast IP address that enables low-latency access to the service. Client access to the service is done over HTTPS connections and client HTTP requests are redirected to HTTPS. The load balancer uses Certificate Manager to manage public certificates. Services are further protected by Cloud Armour and Cloud CDN. The following diagram shows how services are exposed to the internet. Blueprint services that are exposed to the internet. Cloud Service Mesh The blueprint uses Cloud Service Mesh for mutual authentication and authorization for all communications between services. For this deployment, the Cloud Service Mesh uses CA Service for issuing TLS certificates to authenticate peers and to help ensure that only authorized clients can access a service. Using mutual TLS (mTLS) for authentication also helps ensure that all TCP communications between services are encrypted in transit. For service ingress traffic into the service mesh, the blueprint uses the GKE Gateway controller. Distributed services A distributed service is an abstraction of a Kubernetes service that runs in the same namespace across multiple clusters. A distributed service remains available even if one or more GKE clusters are unavailable, as long as any remaining healthy clusters are able to serve the load. To create a distributed service across clusters, Cloud Service Mesh provides Layer 4 and Layer 7 connectivity between an application's services on all clusters in the environment. This connectivity enables the Kubernetes services on multiple clusters to act as a single logical service. Traffic between clusters is only routed to another region if intra-region traffic cannot occur because of a regional failure. Service identity Services running on GKE have identities that are associated with them. The blueprint configures Workload Identity Federation for GKE to let a Kubernetes service account act as a Google Cloud service account. Each instance of a distributed service within the same environment has a common identity which simplifies permission management. When accessing Google Cloud APIs, services that run as the Kubernetes service account automatically authenticate as the Google Cloud service account. Each service has only the minimal permissions necessary for the service to operate.","{ ""id"": ""root"", ""children"": [ { ""id"": ""external_clients"", ""labels"": [ { ""text"": ""external_clients"" } ], ""children"": [ { ""id"": ""external_client"", ""labels"": [ { ""text"": ""External Client"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_env"", ""labels"": [ { ""text"": ""gcp_env"" } ], ""children"": [ { ""id"": ""api_gateway"", ""labels"": [ { ""text"": ""api_gateway"" } ], ""children"": [ { ""id"": ""cloud_lb"", ""labels"": [ { ""text"": ""Cloud Load Balancing"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""edge_security"", ""labels"": [ { ""text"": ""edge_security"" } ], ""children"": [ { ""id"": ""cloud_armor"", ""labels"": [ { ""text"": ""Cloud Armor"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""certificate_manager"", ""labels"": [ { ""text"": ""Certificate Manager"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_cdn"", ""labels"": [ { ""text"": ""Cloud CDN"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gateway_mgmt"", ""labels"": [ { ""text"": ""gateway_mgmt"" } ], ""children"": [ { ""id"": ""cloud_dns"", ""labels"": [ { ""text"": ""Cloud DNS"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gke_gateway_controller"", ""labels"": [ { ""text"": ""GKE Gateway Controller"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""k8s_gateway_api"", ""labels"": [ { ""text"": ""Kubernetes Gateway API"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_dns_lb"", ""labels"": [ { ""text"": ""resolves"" } ] }, { ""id"": ""edge_gkeconf_lb"", ""labels"": [ { ""text"": ""configures"" } ] }, { ""id"": ""edge_armor_lb"", ""labels"": [ { ""text"": ""protects"" } ] }, { ""id"": ""edge_cert_lb"", ""labels"": [ { ""text"": ""manages"" } ] }, { ""id"": ""edge_cdn_lb"", ""labels"": [ { ""text"": ""caches"" } ] } ] }, { ""id"": ""service_mesh"", ""labels"": [ { ""text"": ""service_mesh"" } ], ""children"": [ { ""id"": ""cluster1"", ""labels"": [ { ""text"": ""cluster1"" } ], ""children"": [ { ""id"": ""anthos_svc1_c1"", ""labels"": [ { ""text"": ""Service 1 (Cluster1)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""anthos_svc2_c1"", ""labels"": [ { ""text"": ""Service 2 (Cluster1)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_svc1_c1_svc2_c1"", ""labels"": [ { ""text"": ""calls"" } ] } ] }, { ""id"": ""cluster2"", ""labels"": [ { ""text"": ""cluster2"" } ], ""children"": [ { ""id"": ""anthos_svc1_c2"", ""labels"": [ { ""text"": ""Service 1 (Cluster2)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""anthos_svc2_c2"", ""labels"": [ { ""text"": ""Service 2 (Cluster2)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_svc1_c2_svc2_c2"", ""labels"": [ { ""text"": ""calls"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_svc1_c1_svc1_c2"", ""labels"": [ { ""text"": ""syncs"" } ] }, { ""id"": ""edge_svc1_c1_svc2_c2"", ""labels"": [ { ""text"": ""communicates"" } ] }, { ""id"": ""edge_svc2_c2_svc1_c1"", ""labels"": [ { ""text"": ""communicates"" } ] }, { ""id"": ""edge_svc1_c2_svc2_c1"", ""labels"": [ { ""text"": ""communicates"" } ] }, { ""id"": ""edge_svc2_c1_svc2_c2"", ""labels"": [ { ""text"": ""syncs"" } ] }, { ""id"": ""edge_svc2_c1_svc1_c2"", ""labels"": [ { ""text"": ""communicates"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_k8s_svc1_c1"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""edge_k8s_svc1_c2"", ""labels"": [ { ""text"": ""routes"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_client_lb"", ""labels"": [ { ""text"": ""requests"" } ] } ] }"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/deployment-methodology,"Deployment methodology bookmark_border Last reviewed 2024-12-13 UTC The enterprise application blueprint is deployed through a series of automated systems and pipelines. Each pipeline deploys a particular aspect of the blueprint. Pipelines provide a controllable, auditable, and repeatable mechanism for building out the blueprint. The following diagram shows the interaction of the various pipelines, repositories, and personas. Blueprint pipelines. The blueprint uses the following pipelines: The foundation infrastructure pipeline (part of the enterprise foundations blueprint) deploys the application factory, the multi-tenant infrastructure pipeline, and the fleet-scope pipeline. The multi-tenant infrastructure pipeline deploys the GKE clusters, and the other managed services that the enterprise application blueprint relies on. The fleet-scope pipeline configures fleet scopes, namespaces, and RBAC roles and bindings. The application factory provides a mechanism to deploy new application pipelines through a templated process. The application CI/CD pipeline provides a CI/CD pipeline to deploy services into GKE clusters. Config Sync deploys and maintains additional Kubernetes configurations, including Policy Controller constraints. Repositories, repository contributors, and repository change approvers The blueprint pipelines are triggered through changes to Git repositories. The following table describes the repositories that are used throughout the blueprint, who contributes to the repository, who approves changes to the repository, which pipeline uses the repository, and the description of what the repository contains. Repository	Repository contributor	Repository change approver	Pipeline	Description infra Developer platform developer Developer platform administrator Foundation infrastructure pipeline Repository that contains the code to deploy the multi-tenant infrastructure pipeline, the application, and the fleet-scope pipeline eab-infra Developer platform developer Developer platform administrator Multi-tenant infrastructure The Terraform modules that are used by developer platform teams when they create the infrastructure fleet-scope Developer platform developer Developer platform administrator Fleet-scope The repository that defines the fleet team scopes and namespaces in the fleet app-factory Developer platform developer Developer platform administrator Application factory The code that defines the application repository and references the modules in the terraform-modules repository app-template Application developer Application operator Application factory The base code that is placed in the app-code repository when the repository is first created terraform-modules Developer platform developer Developer platform administrator Application factory Multi-tenant infrastructure The Terraform modules that define the application and the infrastructure app-code Application developer Application operator Application CI/CD The application code that is deployed into the GKE clusters config-policy Developer platform developer Developer platform administrator Config Sync The policies that are used by the GKE clusters to maintain their configurations Automated pipelines help build security, auditability, traceability, repeatability, controllability, and compliance into the deployment process. By using different systems that have different permissions and putting different people into different operating groups, you create separation of responsibilities and follow the principle of least privilege.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_provider"", ""labels"": [ { ""text"": ""gcp_provider"" } ], ""children"": [ { ""id"": ""pipelines_parent"", ""labels"": [ { ""text"": ""pipelines_parent"" } ], ""children"": [ { ""id"": ""infrastructure_pipeline"", ""labels"": [ { ""text"": ""infrastructure_pipeline"" } ], ""children"": [ { ""id"": ""infra_cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""fleet_scope_pipeline"", ""labels"": [ { ""text"": ""fleet_scope_pipeline"" } ], ""children"": [ { ""id"": ""fleet_cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""multitenant_pipeline"", ""labels"": [ { ""text"": ""multitenant_pipeline"" } ], ""children"": [ { ""id"": ""multitenant_cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""application_factory_pipeline"", ""labels"": [ { ""text"": ""application_factory_pipeline"" } ], ""children"": [ { ""id"": ""app_factory_cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e1"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e4"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e6"", ""labels"": [ { ""text"": ""triggers"" } ] } ] }, { ""id"": ""infrastructure_outputs"", ""labels"": [ { ""text"": ""infrastructure_outputs"" } ], ""children"": [ { ""id"": ""fleet_infrastructure"", ""labels"": [ { ""text"": ""Fleet infrastructure"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""config_sync"", ""labels"": [ { ""text"": ""Config Sync"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""platform_infrastructure"", ""labels"": [ { ""text"": ""Platform infrastructure"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""n_pipelines_group"", ""labels"": [ { ""text"": ""n_pipelines_group"" } ], ""children"": [ { ""id"": ""application_infrastructure_pipeline"", ""labels"": [ { ""text"": ""application_infrastructure_pipeline"" } ], ""children"": [ { ""id"": ""app_infra_cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""application_ci_cd_pipeline"", ""labels"": [ { ""text"": ""application_ci_cd_pipeline"" } ], ""children"": [ { ""id"": ""app_ci_cd_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""app_ci_cd_deploy"", ""labels"": [ { ""text"": ""Cloud Deploy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e10"", ""labels"": [ { ""text"": ""builds"" } ] } ] }, { ""id"": ""app_specific_infrastructure"", ""labels"": [ { ""text"": ""Application-specific infrastructure"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""application_node"", ""labels"": [ { ""text"": ""Application"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e11"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""e8"", ""labels"": [ { ""text"": ""provisions"" } ] }, { ""id"": ""e9"", ""labels"": [ { ""text"": ""integrates"" } ] } ] } ], ""edges"": [ { ""id"": ""e2"", ""labels"": [ { ""text"": ""provisions"" } ] }, { ""id"": ""e3"", ""labels"": [ { ""text"": ""configures"" } ] }, { ""id"": ""e5"", ""labels"": [ { ""text"": ""provisions"" } ] }, { ""id"": ""e7"", ""labels"": [ { ""text"": ""triggers"" } ] } ] } ] }"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/deployment-methodology#continuous_integration,"Continuous integration The following diagram shows the continuous integration process. Blueprint continuous integration process. The process is the following: A developer commits application code to the application source repository. This operation triggers Cloud Build to begin the integration pipeline. Cloud Build creates a container image, pushes the container image to Artifact Registry, and creates an image digest. Cloud Build performs automated tests for the application. Depending on the application language, different testing packages may be performed. Cloud Build performs the following scans on the container image: Cloud Build analyzes the container using the Container Structure Tests framework. This framework performs command tests, file existence tests, file content tests, and metadata tests. Cloud Build uses vulnerability scanning to identify any vulnerabilities in the container image against a vulnerability database that's maintained by Google Cloud. Cloud Build approves the image to continue in the pipeline after successful scan results. Binary Authorization signs the image. Binary Authorization is a service on Google Cloud that provides software supply-chain security for container-based applications by using policies, rules, notes, attestations, attestors, and signers. At deployment time, the Binary Authorization policy enforcer helps ensure the provenance of the container before allowing the container to deploy. Cloud Build creates a release in Cloud Deploy to begin the deployment process. To see the security information for a build, go to the Security insights panel. These insights include vulnerabilities that were detected using Artifact Analysis, and the build's level of security assurance denoted by SLSA guidelines.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""gcp"" } ], ""children"": [ { ""id"": ""source_control"", ""labels"": [ { ""text"": ""source_control"" } ], ""children"": [ { ""id"": ""cloud_code"", ""labels"": [ { ""text"": ""Cloud Code"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""source_repo"", ""labels"": [ { ""text"": ""Source Repository"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_cloudcode_commit_repo"", ""labels"": [ { ""text"": ""commit"" } ] } ] }, { ""id"": ""binary_auth"", ""labels"": [ { ""text"": ""Binary Authorization"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ci_pipeline"", ""labels"": [ { ""text"": ""ci_pipeline"" } ], ""children"": [ { ""id"": ""ci_core_steps"", ""labels"": [ { ""text"": ""ci_core_steps"" } ], ""children"": [ { ""id"": ""build_step"", ""labels"": [ { ""text"": ""Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""test_step"", ""labels"": [ { ""text"": ""Test"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""scan_step"", ""labels"": [ { ""text"": ""Scan"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_build_tests"", ""labels"": [ { ""text"": ""tests"" } ] }, { ""id"": ""e_tests_scan"", ""labels"": [ { ""text"": ""scans"" } ] } ] }, { ""id"": ""approval_steps"", ""labels"": [ { ""text"": ""approval_steps"" } ], ""children"": [ { ""id"": ""approve_step"", ""labels"": [ { ""text"": ""Approve"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""authorize_step"", ""labels"": [ { ""text"": ""Authorize"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_approve_authorize"", ""labels"": [ { ""text"": ""approves"" } ] } ] }, { ""id"": ""build_services"", ""labels"": [ { ""text"": ""build_services"" } ], ""children"": [ { ""id"": ""cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_build_sec"", ""labels"": [ { ""text"": ""Cloud Build Security Insights"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_cloudbuild_sec"", ""labels"": [ { ""text"": ""analyzes"" } ] } ] } ], ""edges"": [ { ""id"": ""e_scan_approve"", ""labels"": [ { ""text"": ""submits"" } ] }, { ""id"": ""e_build_cloudbuild"", ""labels"": [ { ""text"": ""executes"" } ] } ] }, { ""id"": ""release_management"", ""labels"": [ { ""text"": ""release_management"" } ], ""children"": [ { ""id"": ""artifact_registry"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_deploy"", ""labels"": [ { ""text"": ""Cloud Deploy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_registry_release_deploy"", ""labels"": [ { ""text"": ""creates release"" } ] } ] } ], ""edges"": [ { ""id"": ""e_repo_trigger_build"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_binary_auth_authorize"", ""labels"": [ { ""text"": ""validates"" } ] }, { ""id"": ""e_authorize_artifact"", ""labels"": [ { ""text"": ""publishes"" } ] } ] } ] }"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/deployment-methodology#continuous_deployment,"Continuous deployment The following diagram shows the continuous deployment process. Blueprint continuous deployment process. The process is the following: At the end of the build process, the application CI/CD pipeline creates a new Cloud Deploy release to launch the newly built container images progressively to each environment. Cloud Deploy initiates a rollout to the first environment of the deployment pipeline, which is usually development. Each deployment stage is configured to require manual approval. The Cloud Deploy pipelines uses sequential deployment to deploy images to each cluster in an environment in order. At the end of each deployment stage, Cloud Deploy verifies the functionality of the deployed containers. These steps are configurable within the Skaffold configuration for the applications.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""devops_engineer"", ""labels"": [ { ""text"": ""DevOps Engineer"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud Platform"" } ], ""children"": [ { ""id"": ""pipeline"", ""labels"": [ { ""text"": ""pipeline"" } ], ""children"": [ { ""id"": ""cloud_deploy"", ""labels"": [ { ""text"": ""Cloud Deploy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""approvals"", ""labels"": [ { ""text"": ""approvals"" } ], ""children"": [ { ""id"": ""approval_dev"", ""labels"": [ { ""text"": ""Approval Dev"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""approval_np"", ""labels"": [ { ""text"": ""Approval NonProd"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""approval_prod"", ""labels"": [ { ""text"": ""Approval Prod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""verifications"", ""labels"": [ { ""text"": ""verifications"" } ], ""children"": [ { ""id"": ""verification_dev"", ""labels"": [ { ""text"": ""Deployment Verification Dev"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""verification_np"", ""labels"": [ { ""text"": ""Deployment Verification NonProd"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""verification_prod"", ""labels"": [ { ""text"": ""Deployment Verification Prod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""environments"", ""labels"": [ { ""text"": ""environments"" } ], ""children"": [ { ""id"": ""dev_environment"", ""labels"": [ { ""text"": ""dev_environment"" } ], ""children"": [ { ""id"": ""gke_dev_cluster"", ""labels"": [ { ""text"": ""GKE Dev Cluster"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""nonprod_environment"", ""labels"": [ { ""text"": ""nonprod_environment"" } ], ""children"": [ { ""id"": ""gke_nonprod_cluster_a"", ""labels"": [ { ""text"": ""GKE NonProd Cluster A"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gke_nonprod_cluster_b"", ""labels"": [ { ""text"": ""GKE NonProd Cluster B"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""prod_environment"", ""labels"": [ { ""text"": ""prod_environment"" } ], ""children"": [ { ""id"": ""gke_prod_cluster_a"", ""labels"": [ { ""text"": ""GKE Prod Cluster A"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gke_prod_cluster_b"", ""labels"": [ { ""text"": ""GKE Prod Cluster B"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_clouddeploy_approval_dev"", ""labels"": [ { ""text"": ""request approval"" } ] }, { ""id"": ""e_verif_dev_approval_np"", ""labels"": [ { ""text"": ""promote"" } ] }, { ""id"": ""e_verif_np_approval_prod"", ""labels"": [ { ""text"": ""promote"" } ] }, { ""id"": ""e_approval_dev_gke_dev"", ""labels"": [ { ""text"": ""deploy"" } ] }, { ""id"": ""e_gke_dev_verif_dev"", ""labels"": [ { ""text"": ""verify"" } ] }, { ""id"": ""e_approval_np_gke_npa"", ""labels"": [ { ""text"": ""deploy"" } ] }, { ""id"": ""e_approval_np_gke_npb"", ""labels"": [ { ""text"": ""deploy"" } ] }, { ""id"": ""e_gke_npa_verif_np"", ""labels"": [ { ""text"": ""verify"" } ] }, { ""id"": ""e_gke_npb_verif_np"", ""labels"": [ { ""text"": ""verify"" } ] }, { ""id"": ""e_approval_prod_gke_pa"", ""labels"": [ { ""text"": ""deploy"" } ] }, { ""id"": ""e_approval_prod_gke_pb"", ""labels"": [ { ""text"": ""deploy"" } ] }, { ""id"": ""e_gke_pa_verif_prod"", ""labels"": [ { ""text"": ""verify"" } ] }, { ""id"": ""e_gke_pb_verif_prod"", ""labels"": [ { ""text"": ""verify"" } ] } ], ""data"": {} } ], ""edges"": [ { ""id"": ""e_user_clouddeploy"", ""labels"": [ { ""text"": ""creates release"" } ] }, { ""id"": ""e_user_approval_dev"", ""labels"": [ { ""text"": ""approves"" } ] } ] }"
gcp,app-dev,app-dev,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/deployment-methodology#deploying-new,"Deploying a new application The following diagram shows how the application factory and application CI/CD pipeline work together to create and deploy a new application. Process for deploying an application. The process for defining a new application is the following: An application operator defines a new application within their tenant by executing a Cloud Build trigger to generate the application definition. The trigger adds a new entry for the application in Terraform and commits the change to the application factory repository. The committed change triggers the creation of application-specific repositories and projects. Cloud Build completes the following: Creates two new Git repositories to host the application's source code and IaC. Pushes the Kubernetes manifests for network policies, and Workload Identity Federation for GKE to the Configuration management repository. Creates the application's CI/CD project and the Cloud Build IaC trigger. The Cloud Build IaC trigger for the application creates the application CI/CD pipeline and the Artifact Registry repository in the application's CI/CD project. Config Sync deploys the network policies and Workload Identity Federation for GKE configurations to the multi-tenant GKE clusters. The fleet scope creation pipeline creates the fleet scope and namespace for the application on multi-tenant GKE clusters. The application's CI/CD pipeline performs the initial deployment of the application to the GKE clusters. Optionally, the application team uses the Cloud Build IaC trigger to deploy projects and additional resources (for example, databases and other managed services) to dedicated single-tenant projects, one for each environment. GKE Enterprise configuration and policy management In the blueprint, developer platform administrators use Config Sync to create cluster-level configurations in each environment. Config Sync connects to a Git repository which serves as the source of truth for the chosen state of the cluster configuration. Config Sync continuously monitors the actual state of the configuration in the clusters and reconciles any discrepancies by applying updates to ensure adherence to the chosen state, despite manual changes. Configs are applied to the environments (development, non-production, and production) by using a branching strategy on the repository. In this blueprint, Config Sync applies Policy Controller constraints. These configurations define security and compliance controls as defined by developer platform administrators for the organization. This blueprint relies on other pipelines to apply other configurations: the application CI/CD pipelines apply application-specific configuration, and the fleet-scope pipeline creates namespaces and associated role bindings.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_cloud_group"", ""labels"": [ { ""text"": ""gcp_cloud_group"" } ], ""children"": [ { ""id"": ""application_factory_group"", ""labels"": [ { ""text"": ""application_factory_group"" } ], ""children"": [ { ""id"": ""factory_build_group"", ""labels"": [ { ""text"": ""factory_build_group"" } ], ""children"": [ { ""id"": ""factory_user"", ""labels"": [ { ""text"": ""Factory User"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_build_app_def"", ""labels"": [ { ""text"": ""Cloud Build - App Definition"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_build_app_creation"", ""labels"": [ { ""text"": ""Cloud Build - App Creation"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_factory_user_def"", ""labels"": [ { ""text"": ""initiates"" } ] } ] }, { ""id"": ""factory_repos_group"", ""labels"": [ { ""text"": ""factory_repos_group"" } ], ""children"": [ { ""id"": ""app_factory_repo"", ""labels"": [ { ""text"": ""App Factory Repo"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""terraform_modules"", ""labels"": [ { ""text"": ""Terraform Modules"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""app_template_repos"", ""labels"": [ { ""text"": ""App Template Repos"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_app_def_to_repo"", ""labels"": [ { ""text"": ""commits"" } ] }, { ""id"": ""edge_repo_to_creation"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""edge_terraform_to_creation"", ""labels"": [ { ""text"": ""imports"" } ] } ] }, { ""id"": ""config_management"", ""labels"": [ { ""text"": ""Configuration Management"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""application_project_group"", ""labels"": [ { ""text"": ""application_project_group"" } ], ""children"": [ { ""id"": ""application_cicd_project_group"", ""labels"": [ { ""text"": ""application_cicd_project_group"" } ], ""children"": [ { ""id"": ""ci_cd_repos_group"", ""labels"": [ { ""text"": ""ci_cd_repos_group"" } ], ""children"": [ { ""id"": ""application_code_repo"", ""labels"": [ { ""text"": ""App Code Repo"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""application_infrastructure_repo"", ""labels"": [ { ""text"": ""App Infra Repo"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""secret_manager"", ""labels"": [ { ""text"": ""Secret Manager"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""ci_cd_pipeline_group"", ""labels"": [ { ""text"": ""ci_cd_pipeline_group"" } ], ""children"": [ { ""id"": ""artifact_registry"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_build_app_build"", ""labels"": [ { ""text"": ""Cloud Build - App Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_deploy"", ""labels"": [ { ""text"": ""Cloud Deploy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_build_to_artifacts"", ""labels"": [ { ""text"": ""pushes artifacts"" } ] }, { ""id"": ""edge_build_to_deploy"", ""labels"": [ { ""text"": ""triggers deployment"" } ] } ] }, { ""id"": ""cloud_build_iac"", ""labels"": [ { ""text"": ""Cloud Build - IaC"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_code_to_build"", ""labels"": [ { ""text"": ""provides code"" } ] }, { ""id"": ""edge_infra_repo_to_iac"", ""labels"": [ { ""text"": ""provides IaC"" } ] }, { ""id"": ""edge_secret_to_build"", ""labels"": [ { ""text"": ""supplies secrets"" } ] }, { ""id"": ""edge_build_to_iac"", ""labels"": [ { ""text"": ""notifies"" } ] } ] }, { ""id"": ""application_dev_project"", ""labels"": [ { ""text"": ""Application Dev Project"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""application_nonprod_project"", ""labels"": [ { ""text"": ""Application NonProd Project"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""application_prod_project"", ""labels"": [ { ""text"": ""Application Prod Project"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_deploy_to_dev"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_deploy_to_nonprod"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_deploy_to_prod"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_iac_to_dev"", ""labels"": [ { ""text"": ""provisions"" } ] }, { ""id"": ""edge_iac_to_nonprod"", ""labels"": [ { ""text"": ""provisions"" } ] }, { ""id"": ""edge_iac_to_prod"", ""labels"": [ { ""text"": ""provisions"" } ] } ] }, { ""id"": ""multi_tenant_infrastructure_group"", ""labels"": [ { ""text"": ""multi_tenant_infrastructure_group"" } ], ""children"": [ { ""id"": ""dev_project_env_group"", ""labels"": [ { ""text"": ""dev_project_env_group"" } ], ""children"": [ { ""id"": ""gke_dev_env"", ""labels"": [ { ""text"": ""Dev Environment (GKE)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""nonprod_project_env_group"", ""labels"": [ { ""text"": ""nonprod_project_env_group"" } ], ""children"": [ { ""id"": ""gke_nonprod_env"", ""labels"": [ { ""text"": ""NonProd Environment (GKE)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""prod_project_env_group"", ""labels"": [ { ""text"": ""prod_project_env_group"" } ], ""children"": [ { ""id"": ""gke_prod_env"", ""labels"": [ { ""text"": ""Prod Environment (GKE)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_template_to_code_repo"", ""labels"": [ { ""text"": ""scaffolds"" } ] }, { ""id"": ""edge_template_to_infra_repo"", ""labels"": [ { ""text"": ""scaffolds"" } ] }, { ""id"": ""edge_creation_to_config"", ""labels"": [ { ""text"": ""pushes config"" } ] }, { ""id"": ""edge_config_to_multi_tenant"", ""labels"": [ { ""text"": ""configures"" } ] } ] } ] }"
gcp,app-dev,devlopment-platform,https://cloud.google.com/architecture/blueprints/enterprise-application-blueprint/cymbal-bank,"Cymbal Bank application architecture bookmark_border Last reviewed 2024-12-13 UTC The blueprint includes a sample application that is named Cymbal Bank. Cymbal Bank demonstrates the best practices that are recommended for containerized applications. The Cymbal Bank application lets users create login accounts, sign in to their account, see their transaction history, make deposits, and transfer money to other users' accounts. Cymbal Bank services run as containers that connect to each other over REST APIs and gRPC APIs. The following diagram shows the Cymbal Bank application that is deployed on the blueprint developer platform. Cymbal Bank architecture. Each application is also a network service. Only the frontend application is exposed externally to the cluster through the GKE Gateway controller. All applications run as distributed services through the use of Cloud Service Mesh. For more information about the services that are included in the Cymbal Bank application, see the Cymbal Bank repository on GitHub. Cymbal Bank tenants To provide separation between tenants, each tenant in the developer platform has one team scope and at least one fleet namespace. Tenants never share a namespace. To deploy Cymbal Bank, each tenant only needs one namespace. In more complex scenarios, a tenant can have several namespaces. To illustrate how Cymbal Bank is deployed on the developer platform, this example assumes that there were three separate application development teams with different focus areas. The Terraform creates the following developer platform tenant for each of those teams: frontend tenant: A development team that focuses on the website and mobile application backends. accounts tenant: A development team that focuses on customer data. ledger tenant: A team that manages the ledger services. Cymbal Bank apps The Cymbal Bank application consists of six microservices: frontend, ledgerwriter, balancereader, transactionhistory, userservice, and contacts. Each microservice is mapped to an application within the tenant that owns it. The following table describes the mapping of the teams, team scope, fleet namespace, and microservices for Cymbal Bank. For the purpose of this mapping, this example assumes that Cymbal Bank is developed by three separate application operator teams. Teams manage a varying number of services. Each team is assigned a team scope. Team	Team scope	Fleet namespace	Application - Microservice	Kubernetes service account Frontend team frontend frontend frontend ksa-frontend Ledger team ledger ledger ledgerwriter ksa-ledgerwriter balancereader ksa-balancereader transactionhistory ksa-transactionhistory Accounts team accounts accounts userservice ksa-userservice contacts ksa-contacts Cymbal Bank database structure Cymbal Bank databases are deployed using AlloyDB for PostgreSQL. The databases are configured with a highly available primary instance in one region with redundant nodes in different zones, and cross-region replicas are used for disaster recovery. Cymbal Bank uses IAM database authentication to allow services access to the databases. The databases are encrypted using CMEK. Two PostgreSQL databases are used: ledger-db for the ledger, and accounts-db for user accounts.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_env"", ""labels"": [ { ""text"": ""gcp_env"" } ], ""children"": [ { ""id"": ""accounts_data_layer"", ""labels"": [ { ""text"": ""accounts_data_layer"" } ], ""children"": [ { ""id"": ""accounts_db_primary"", ""labels"": [ { ""text"": ""AlloyDB Accounts (us-west1)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""accounts_db_secondary"", ""labels"": [ { ""text"": ""AlloyDB Accounts Secondary (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_acc_rep"", ""labels"": [ { ""text"": ""replicates"" } ] } ] }, { ""id"": ""transactions_data_layer"", ""labels"": [ { ""text"": ""transactions_data_layer"" } ], ""children"": [ { ""id"": ""transactions_db_primary"", ""labels"": [ { ""text"": ""AlloyDB Ledger (us-west1)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""transactions_db_secondary"", ""labels"": [ { ""text"": ""AlloyDB Ledger Secondary (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_tx_rep"", ""labels"": [ { ""text"": ""replicates"" } ] } ] }, { ""id"": ""cluster_project"", ""labels"": [ { ""text"": ""cluster_project"" } ], ""children"": [ { ""id"": ""edge_security"", ""labels"": [ { ""text"": ""edge_security"" } ], ""children"": [ { ""id"": ""multi_gateway"", ""labels"": [ { ""text"": ""Multi-Cluster Gateway"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_lb"", ""labels"": [ { ""text"": ""Cloud Load Balancing"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_armor"", ""labels"": [ { ""text"": ""Cloud Armor"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_gateway_lb"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_lb_armor"", ""labels"": [ { ""text"": ""protects"" } ] } ] }, { ""id"": ""clusters_group"", ""labels"": [ { ""text"": ""clusters_group"" } ], ""children"": [ { ""id"": ""region1_cluster"", ""labels"": [ { ""text"": ""region1_cluster"" } ], ""children"": [ { ""id"": ""frontend_ns_r1"", ""labels"": [ { ""text"": ""frontend_ns_r1"" } ], ""children"": [ { ""id"": ""frontend_r1"", ""labels"": [ { ""text"": ""Frontend Service (us-west1)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""accounts_ns_r1"", ""labels"": [ { ""text"": ""accounts_ns_r1"" } ], ""children"": [ { ""id"": ""user_service_r1"", ""labels"": [ { ""text"": ""User Service"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""contact_service_r1"", ""labels"": [ { ""text"": ""Contact Service"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_user_contact1"", ""labels"": [ { ""text"": ""queries"" } ] } ] }, { ""id"": ""transactions_ns_r1"", ""labels"": [ { ""text"": ""transactions_ns_r1"" } ], ""children"": [ { ""id"": ""ledger_writer_r1"", ""labels"": [ { ""text"": ""Ledger Writer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""balance_reader_r1"", ""labels"": [ { ""text"": ""Balance Reader"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""transactions_history_r1"", ""labels"": [ { ""text"": ""Transactions History"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ledger_hist"", ""labels"": [ { ""text"": ""stores history"" } ] } ] } ], ""edges"": [ { ""id"": ""e_frontend_user1"", ""labels"": [ { ""text"": ""calls"" } ] }, { ""id"": ""e_user_balance1"", ""labels"": [ { ""text"": ""reads balance"" } ] }, { ""id"": ""e_balance_frontend1"", ""labels"": [ { ""text"": ""responds"" } ] } ] }, { ""id"": ""region2_cluster"", ""labels"": [ { ""text"": ""region2_cluster"" } ], ""children"": [ { ""id"": ""frontend_ns_r2"", ""labels"": [ { ""text"": ""frontend_ns_r2"" } ], ""children"": [ { ""id"": ""front_end_r2"", ""labels"": [ { ""text"": ""Frontend Service (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""accounts_ns_r2"", ""labels"": [ { ""text"": ""accounts_ns_r2"" } ], ""children"": [ { ""id"": ""user_service_r2"", ""labels"": [ { ""text"": ""User Service (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""contact_service_r2"", ""labels"": [ { ""text"": ""Contact Service (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_user2_contact2"", ""labels"": [ { ""text"": ""queries"" } ] } ] }, { ""id"": ""transactions_ns_r2"", ""labels"": [ { ""text"": ""transactions_ns_r2"" } ], ""children"": [ { ""id"": ""ledger_writer_r2"", ""labels"": [ { ""text"": ""Ledger Writer (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""balance_reader_r2"", ""labels"": [ { ""text"": ""Balance Reader (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""transactions_history_r2"", ""labels"": [ { ""text"": ""Transactions History (us-east1)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ledger2_hist2"", ""labels"": [ { ""text"": ""stores history"" } ] } ] } ], ""edges"": [ { ""id"": ""e_frontend2_user2"", ""labels"": [ { ""text"": ""calls"" } ] }, { ""id"": ""e_user2_balance2"", ""labels"": [ { ""text"": ""reads balance"" } ] }, { ""id"": ""e_balance2_frontend2"", ""labels"": [ { ""text"": ""responds"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_armor_frontend1"", ""labels"": [ { ""text"": ""forwards"" } ] }, { ""id"": ""e_armor_frontend2"", ""labels"": [ { ""text"": ""forwards"" } ] } ] } ], ""edges"": [ { ""id"": ""e_user_accdb"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""e_ledger_txdb"", ""labels"": [ { ""text"": ""writes ledger"" } ] }, { ""id"": ""e_acc_resp"", ""labels"": [ { ""text"": ""responds"" } ] }, { ""id"": ""e_tx_resp"", ""labels"": [ { ""text"": ""responds"" } ] }, { ""id"": ""e_user2_accdb"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""e_ledger2_txdb"", ""labels"": [ { ""text"": ""writes ledger"" } ] } ] }, { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""web_user"", ""labels"": [ { ""text"": ""Web User"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mobile_user"", ""labels"": [ { ""text"": ""Mobile User"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_web_lb"", ""labels"": [ { ""text"": ""accesses"" } ] }, { ""id"": ""e_mobile_lb"", ""labels"": [ { ""text"": ""accesses"" } ] } ] }"
gcp,app-dev,devlopment-platform,https://cloud.google.com/architecture/exposing-service-mesh-apps-through-gke-ingress,"From edge to mesh: Expose service mesh applications through GKE Gateway bookmark_border Last reviewed 2023-10-24 UTC This reference architecture shows how to combine Cloud Service Mesh with Cloud Load Balancing to expose applications in a service mesh to internet clients. Cloud Service Mesh is a managed service mesh, based on Istio, that provides a security-enhanced, observable, and standardized communication layer for applications. A service mesh provides a holistic communications platform for clients that are communicating in the mesh. However, a challenge remains in how to connect clients that are outside the mesh to applications hosted inside the mesh. You can expose an application to clients in many ways, depending on where the client is. This reference architecture is intended for advanced practitioners who run Cloud Service Mesh but it works for Istio on Google Kubernetes Engine (GKE) too. Mesh ingress gateway Istio 0.8 introduced the mesh ingress gateway. The gateway provides a dedicated set of proxies whose ports are exposed to traffic coming from outside the service mesh. These mesh ingress proxies let you control network exposure behavior separately from application routing behavior. The proxies also let you apply routing and policy to mesh-external traffic before it arrives at an application sidecar. Mesh ingress defines the treatment of traffic when it reaches a node in the mesh, but external components must define how traffic first arrives at the mesh. To manage this external traffic, you need a load balancer that is external to the mesh. This reference architecture uses Google Cloud Load Balancing provisioned through GKE Gateway resources to automate deployment. For Google Cloud, the canonical example of this setup is an external load balancing service that deploys a public network load balancer (L4). That load balancer points at the NodePorts of a GKE cluster. These NodePorts expose the Istio ingress gateway Pods, which route traffic to downstream mesh sidecar proxies. Architecture The following diagram illustrates this topology. Load balancing for internal private traffic looks similar to this architecture, except that you deploy an internal passthrough Network Load Balancer instead. An external load balancer routes external clients to the mesh through ingress gateway proxies. The preceding diagram shows that using L4 transparent load balancing with a mesh ingress gateway offers the following advantages: The setup simplifies deploying the load balancer. The load balancer provides a stable virtual IP address (VIP), health checking, and reliable traffic distribution when cluster changes, node outages, or process outages occur. All routing rules, TLS termination, and traffic policy is handled in a single location at the mesh ingress gateway. Note: For applications in Cloud Service Mesh, deployment of the external TCP/UDP load balancer is the default exposure type. You deploy the load balancer through the Kubernetes Service, which selects the Pods of the mesh ingress proxies. GKE Gateway and Services You can provide access to applications for clients that are outside the cluster in many ways. GKE Gateway is an implementation of the Kubernetes Gateway API. GKE Gateway evolves the Ingress resource and improves it. As you deploy GKE Gateway resources to your GKE cluster, the Gateway controller watches the Gateway API resources and reconciles Cloud Load Balancing resources to implement the networking behavior that's specified by the GKE Gateway resources. When using GKE Gateway, the type of load balancer you use to expose applications to clients depends largely on the following factors: The status of the clients (external or internal). The required capabilities of the load balancer, including the capability to integrate with Google Cloud Armor security policies. The spanning requirements of the service mesh. Service meshes can span multiple GKE clusters or can be contained in a single cluster. In GKE Gateway, this behavior is controlled by specifying the appropriate GatewayClass. Note: Although you can expose applications hosted in Cloud Service Mesh through Node IP addresses directly, we don't recommend this approach for most production environments. Direct exposure through Node IP addresses is fragile and can also become a security risk. To direct traffic to applications hosted in Cloud Service Mesh, we recommend that you use an L4 or L7 load balancer. Although the default load balancer for Cloud Service Mesh is the Network Load Balancer, this reference architecture focuses on the external Application Load Balancer (L7). The external Application Load Balancer provides integration with edge services like Identity-Aware Proxy and Google Cloud Armor, URL redirects and rewrites, as well as a globally distributed network of edge proxies. The next section describes the architecture and advantages of using two layers of HTTP load balancing.","{ ""id"": ""root"", ""children"": [ { ""id"": ""external_clients"", ""labels"": [ { ""text"": ""external_clients"" } ], ""children"": [ { ""id"": ""external_client"", ""labels"": [ { ""text"": ""External Client"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""google_cloud"", ""labels"": [ { ""text"": ""google_cloud"" } ], ""children"": [ { ""id"": ""load_balancer"", ""labels"": [ { ""text"": ""TCP/UDP Load Balancer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gke_cluster"", ""labels"": [ { ""text"": ""gke_cluster"" } ], ""children"": [ { ""id"": ""ingress_gateways"", ""labels"": [ { ""text"": ""ingress_gateways"" } ], ""children"": [ { ""id"": ""ingress_gateway_1"", ""labels"": [ { ""text"": ""Ingress Gateway Proxy 1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ingress_gateway_2"", ""labels"": [ { ""text"": ""Ingress Gateway Proxy 2"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""application_services"", ""labels"": [ { ""text"": ""application_services"" } ], ""children"": [ { ""id"": ""app_service_a1"", ""labels"": [ { ""text"": ""Application A1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""app_service_a2"", ""labels"": [ { ""text"": ""Application A2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""app_service_b1"", ""labels"": [ { ""text"": ""Application B1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""app_service_b2"", ""labels"": [ { ""text"": ""Application B2"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_app1_app4"", ""labels"": [ { ""text"": ""calls"" } ] }, { ""id"": ""e_app3_app2"", ""labels"": [ { ""text"": ""calls"" } ] } ] } ], ""edges"": [ { ""id"": ""e_ing1_app1"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_ing2_app3"", ""labels"": [ { ""text"": ""routes"" } ] } ] } ], ""edges"": [ { ""id"": ""e_lb_ing1"", ""labels"": [ { ""text"": ""routes to"" } ] }, { ""id"": ""e_lb_ing2"", ""labels"": [ { ""text"": ""routes to"" } ] } ] } ], ""edges"": [ { ""id"": ""e_external_lb"", ""labels"": [ { ""text"": ""connects to"" } ] } ] }"
gcp,app-dev,devlopment-platform,https://cloud.google.com/architecture/build-apps-using-gateway-and-cloud-service,"From edge to multi-cluster mesh: Globally distributed applications exposed through GKE Gateway and Cloud Service Mesh bookmark_border Release Notes Last reviewed 2024-06-30 UTC This reference architecture describes the benefits of exposing applications externally through Google Kubernetes Engine (GKE) Gateways running on multiple GKE clusters within a service mesh. This guide is intended for platform administrators. You can increase the resiliency and redundancy of your services by deploying applications consistently across multiple GKE clusters, where each cluster becomes an additional failure domain. For example, a service's compute infrastructure with a service level objective (SLO) of 99.9% when deployed in a single GKE cluster achieves an SLO of 99.9999% when deployed across two GKE clusters (1 - (0.001)2). You can also provide users with an experience where incoming requests are automatically directed to the least latent and available mesh ingress gateway. If you're interested in the benefits of exposing service-mesh-enabled applications that run on a single cluster, see From edge to mesh: Expose service mesh applications through GKE Gateway. Architecture The following architecture diagram shows how data flows through cloud ingress and mesh ingress: TLS encryption from the client, a load balancer, and from the mesh. The preceding diagram shows the following data flow scenarios: From the client terminating at the Google Cloud load balancer using its own Google-managed TLS certificate. From the Google Cloud load balancer to the mesh ingress proxy using its own self-signed TLS certificate. From the mesh ingress gateway proxy to the workload sidecar proxies using service mesh-enabled mTLS. This reference architecture contains the following two ingress layers: Cloud ingress: in this reference architecture, you use the Kubernetes Gateway API (and the GKE Gateway controller) to program the external, multi-cluster HTTP(S) load balancing layer. The load balancer check the mesh ingress proxies across multiple regions, sending requests to the nearest healthy cluster. It also implements a Google Cloud Armor security policy. Mesh ingress: In the mesh, you perform health checks on the backends directly so that you can run load balancing and traffic management locally. When you use the ingress layers together, there are complementary roles for each layer. To achieve the following goals, Google Cloud optimizes the most appropriate features from the cloud ingress layer and the mesh ingress layer: Provide low latency. Increase availability. Use the security features of the cloud ingress layer. Use the security, authorization, and observability features of the mesh ingress layer. Cloud ingress When paired with mesh ingress, the cloud ingress layer is best used for edge security and global load balancing. Because the cloud ingress layer is integrated with the following services, it excels at running those services at the edge, outside the mesh: DDoS protection Cloud firewalls Authentication and authorization Encryption The routing logic is typically straightforward at the cloud ingress layer. However, it can be more complex for multi-cluster and multi-region environments. Because of the critical function of internet-facing load balancers, the cloud ingress layer is likely managed by a platform team that has exclusive control over how applications are exposed and secured on the internet. This control makes this layer less flexible and dynamic than a developer-driven infrastructure. Consider these factors when determining administrative access rights to this layer and how you provide that access. Mesh ingress When paired with cloud ingress, the mesh ingress layer provides a point of entry for traffic to enter the service mesh. The layer also provides backend mTLS, authorization policies, and flexible regex matching. Deploying external application load balancing outside of the mesh with a mesh ingress layer offers significant advantages, especially for internet traffic management. Although service mesh and Istio ingress gateways provide advanced routing and traffic management in the mesh, some functions are better served at the edge of the network. Taking advantage of internet-edge networking through Google Cloud's external Application Load Balancer might provide significant performance, reliability, or security-related benefits over mesh-based ingress. Note: This reference architecture refers to the routing for cloud ingress layers as north-south routing. The routing between mesh ingress layers and service layers, or the routing from service layers to service layers, is referred to as east-west routing. Products and features used The following list summarizes of all the Google Cloud products and features that this referenence architecture uses: GKE Enterprise: A managed Kubernetes service that you can use to deploy and operate containerized applications at scale using Google's infrastructure. For the purpose of this reference architecture, each of the GKE clusters serving an application must be in the same fleet. Fleets and multi-cluster Gateways: Services that are used to create containerized applications at enterprise scale using Google's infrastructure and GKE Enterprise. Google Cloud Armor: A service that helps you to protect your applications and websites against denial of service and web attacks. Cloud Service Mesh: A fully managed service mesh based on Envoy and Istio Application Load Balancer: A proxy-based L7 load balancer that lets you run and scale your services. Certificate Manager: A service that lets you acquire and manage TLS certificates for use with Cloud Load Balancing. Fleets To manage multi-cluster deployments, GKE Enterprise and Google Cloud use fleets to logically group and normalize Kubernetes clusters. Using one or more fleets can help you uplevel management from individual clusters to entire groups of clusters. To reduce cluster-management friction, use the fleet principle of namespace sameness. For each GKE cluster in a fleet, ensure that you configure all mesh ingress gateways the same way. Also, consistently deploy application services so that the service balance-reader in the namespace account relates to an identical service in each GKE cluster in the fleet. The principles of sameness and trust that are assumed within a fleet are what let you use the full range of fleet-enabled features in GKE Enterprise and Google Cloud. East-west routing rules within the service mesh and traffic policies are handled at the mesh ingress layer. The mesh ingress layer is deployed on every GKE cluster in the fleet. Configure each mesh ingress gateway in the same manner, adhering to the fleet's principle of namespace sameness. Although there's a single configuration cluster for GKE Gateway, you should synchronize your GKE Gateway configurations across all GKE clusters in the fleet. If you need to nominate a new configuration cluster, use ConfigSync. ConfigSync helps ensure that all such configurations are synchronized across all GKE clusters in the fleet and helps avoid reconciling with a non-current configuration. Mesh ingress gateway Istio 0.8 introduced the mesh ingress gateway. The gateway provides a dedicated set of proxies whose ports are exposed to traffic coming from outside the service mesh. These mesh ingress proxies let you control network exposure behavior separately from application routing behavior. The proxies also let you apply routing and policy to mesh-external traffic before it arrives at an application sidecar. Mesh ingress defines the treatment of traffic when it reaches a node in the mesh, but external components must define how traffic first arrives at the mesh. To manage external traffic, you need a load balancer that's external to the mesh. To automate deployment, this reference architecture uses Cloud Load Balancing, which is provisioned through GKE Gateway resources. GKE Gateway and multi-cluster services There are many ways to provide application access to clients that are outside the cluster. GKE Gateway is an implementation of the Kubernetes Gateway API. GKE Gateway evolves and improves the Ingress resource. As you deploy GKE Gateway resources to your GKE cluster, the Gateway controller watches the Gateway API resources. The controller reconciles Cloud Load Balancing resources to implement the networking behavior that's specified by the Gateway resources. When using GKE Gateway, the type of load balancer you use to expose applications to clients depends largely on the following factors: Whether the backend services are in a single GKE cluster or distributed across multiple GKE clusters (in the same fleet). The status of the clients (external or internal). The required capabilities of the load balancer, including the capability to integrate with Google Cloud Armor security policies. The spanning requirements of the service mesh. Service meshes can span multiple GKE clusters or can be contained in a single cluster. In Gateway, this behavior is controlled by specifying the appropriate GatewayClass. When referring to Gateway classes, those classes which can be used in multi-cluster scenarios have a class name ending in -mc. This reference architecture discusses how to expose application services externally through an external Application Load Balancer. However, when using Gateway, you can also create a multi-cluster regional internal Application Load Balancer. To deploy application services in multi-cluster scenarios you can define the Google Cloud load balancer components in the following two ways: Multi Cluster Ingress and MultiClusterService resources Multi-cluster Gateway and Multi-cluster Services For more information about these two approaches to deploying application services, see Choose your multi-cluster load balancing API for GKE. Note: Both multi-cluster Ingress and multi-cluster GKE Gateway use multi-cluster services. Multi-cluster services can be deployed across GKE clusters in a fleet, and are identical for the purposes of cross-cluster service discovery. However, the manner in which these services are exposed across clusters within the fleet differ. Multi Cluster Ingress relies on creating MultiClusterService resources. Multi-cluster Gateway relies on creating ServiceExport resources, and referring to ServiceImport resources. When you use a multi-cluster Gateway, you can enable the additional capabilities of the underlying Google Cloud load balancer by creating Policies. The deployment guide associated with this reference architecture shows how to configure a Google Cloud Armor security policy to help protect backend services from cross-site scripting. These policy resources target the backend services in the fleet that are exposed across multiple clusters. In multi-cluster scenarios, all such policies must reference the ServiceImport resource and API group. Health checking One complexity of using two layers of L7 load balancing is health checking. You must configure each load balancer to check the health of the next layer. The GKE Gateway checks the health of the mesh ingress proxies, and the mesh, in return, checks the health of the application backends. Cloud ingress: In this reference architecture, you configure the Google Cloud load balancer through GKE Gateway to check the health of the mesh ingress proxies on their exposed health check ports. If a mesh proxy is down, or if the cluster, mesh, or region is unavailable, the Google Cloud load balancer detects this condition and doesn't send traffic to the mesh proxy. In this case, traffic would be routed to an alternate mesh proxy in a different GKE cluster or region. Mesh ingress: In the mesh application, you perform health checks on the backends directly so that you can run load balancing and traffic management locally.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_provider"", ""labels"": [ { ""text"": ""gcp_provider"" } ], ""children"": [ { ""id"": ""edge_gateway"", ""labels"": [ { ""text"": ""edge_gateway"" } ], ""children"": [ { ""id"": ""external_lb"", ""labels"": [ { ""text"": ""External App Load Balancer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""managed_tls"", ""labels"": [ { ""text"": ""Managed TLS Certificate"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_tls_lb"", ""labels"": [ { ""text"": ""terminates"" } ] } ] }, { ""id"": ""service_mesh"", ""labels"": [ { ""text"": ""service_mesh"" } ], ""children"": [ { ""id"": ""gateway_security"", ""labels"": [ { ""text"": ""gateway_security"" } ], ""children"": [ { ""id"": ""cloud_armor"", ""labels"": [ { ""text"": ""Cloud Armor"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""region1_cluster"", ""labels"": [ { ""text"": ""region1_cluster"" } ], ""children"": [ { ""id"": ""region1_workloads"", ""labels"": [ { ""text"": ""region1_workloads"" } ], ""children"": [ { ""id"": ""mesh_ingress_r1"", ""labels"": [ { ""text"": ""Mesh Ingress Gateway Pods R1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""frontend_r1"", ""labels"": [ { ""text"": ""Frontend Service Pods R1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""backend_r1"", ""labels"": [ { ""text"": ""Backend Service Pods R1"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ingress_frontend_r1"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_frontend_backend_r1"", ""labels"": [ { ""text"": ""calls"" } ] } ] }, { ""id"": ""region1_certs"", ""labels"": [ { ""text"": ""region1_certs"" } ], ""children"": [ { ""id"": ""self_signed_cert_r1"", ""labels"": [ { ""text"": ""Self-signed Cert R1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mtls_cert_r1"", ""labels"": [ { ""text"": ""mTLS Cert R1"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_cert_ingress_r1"", ""labels"": [ { ""text"": ""terminates"" } ] }, { ""id"": ""e_mtls_frontend_r1"", ""labels"": [ { ""text"": ""secures"" } ] }, { ""id"": ""e_mtls_backend_r1"", ""labels"": [ { ""text"": ""secures"" } ] } ] }, { ""id"": ""region2_cluster"", ""labels"": [ { ""text"": ""region2_cluster"" } ], ""children"": [ { ""id"": ""region2_workloads"", ""labels"": [ { ""text"": ""region2_workloads"" } ], ""children"": [ { ""id"": ""mesh_ingress_r2"", ""labels"": [ { ""text"": ""Mesh Ingress Gateway Pods R2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""frontend_r2"", ""labels"": [ { ""text"": ""Frontend Service Pods R2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""backend_r2"", ""labels"": [ { ""text"": ""Backend Service Pods R2"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ingress_frontend_r2"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_frontend_backend_r2"", ""labels"": [ { ""text"": ""calls"" } ] } ] }, { ""id"": ""region2_certs"", ""labels"": [ { ""text"": ""region2_certs"" } ], ""children"": [ { ""id"": ""self_signed_cert_r2"", ""labels"": [ { ""text"": ""Self-signed Cert R2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mtls_cert_r2"", ""labels"": [ { ""text"": ""mTLS Cert R2"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_cert_ingress_r2"", ""labels"": [ { ""text"": ""terminates"" } ] }, { ""id"": ""e_mtls_frontend_r2"", ""labels"": [ { ""text"": ""secures"" } ] }, { ""id"": ""e_mtls_backend_r2"", ""labels"": [ { ""text"": ""secures"" } ] } ] }, { ""id"": ""service_discovery"", ""labels"": [ { ""text"": ""Service Discovery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ca_ingress_r1"", ""labels"": [ { ""text"": ""protects"" } ] }, { ""id"": ""e_ca_ingress_r2"", ""labels"": [ { ""text"": ""protects"" } ] }, { ""id"": ""e_ingress1_sd"", ""labels"": [ { ""text"": ""registers"" } ] }, { ""id"": ""e_ingress2_sd"", ""labels"": [ { ""text"": ""registers"" } ] } ] } ], ""edges"": [ { ""id"": ""e_lb_ca"", ""labels"": [ { ""text"": ""routes"" } ] } ] }, { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""web_user"", ""labels"": [ { ""text"": ""Web User"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mobile_user"", ""labels"": [ { ""text"": ""Mobile User"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_user_web_lb"", ""labels"": [ { ""text"": ""HTTPS"" } ] }, { ""id"": ""e_user_mobile_lb"", ""labels"": [ { ""text"": ""HTTPS"" } ] } ] }"
gcp,app-dev,devops,https://cloud.google.com/architecture/app-development-and-delivery-with-cloud-code-gcb-cd-and-gke,"CI/CD pipeline for developing and delivering containerized apps bookmark_border Last reviewed 2022-11-18 UTC This document describes an integrated set of Google Cloud tools to set up a system for development, for continuous integration (CI), and for continuous delivery (CD) that you can use to develop and deploy applications to Google Kubernetes Engine (GKE). This reference architecture document is intended for both software developers and operators. It assumes that you're familiar with running gcloud commands on Google Cloud and with deploying application containers to GKE. Architecture The following diagram shows the resources that are used in this architecture: Develop and deploy system with Cloud Code, Cloud Build, Artifact Registry, Cloud Deploy, and GKE This architecture includes the following components: Cloud Code as a development workspace. As part of this workspace, you can see changes in the development cluster, which runs on minikube. You run Cloud Code and the minikube cluster in Cloud Shell. Cloud Shell is an online development environment accessible from your browser. It has compute resources, memory, an integrated development environment, (IDE), and it also has Cloud Code installed. Cloud Build to build and test the application—the ""CI"" part of the pipeline This part of the pipeline includes the following actions: Cloud Build monitors changes to the source repository, using a Cloud Build trigger. When a change is committed into the main branch, the Cloud Build trigger does the following: Rebuilds the application container. Places build artifacts in a Cloud Storage bucket. Places the application container in Artifact Registry. Runs tests on the container. Calls Cloud Deploy to deploy the container to the staging environment. In this example, the staging environment is a Google Kubernetes Engine cluster. If the build and tests are successful, you can then use Cloud Deploy to promote the container from staging to production. Cloud Deploy to manage the deployment—the ""CD"" part of the pipeline. In this part of the pipeline, Cloud Deploy does the following: Registers a delivery pipeline and targets. The targets represent the staging and production clusters. Creates a Cloud Storage bucket and stores the Skaffold rendering source and rendered manifests in that bucket. Generates a new release for each source code change. Deploys the application to the production environment. For this deployment to production, an operator (or other designated person) manually approves the deployment. In this architecture, the production environment is a Google Kubernetes Engine cluster. In this architecture, configuration is shared among the development, staging, and production environments through Skaffold, a command-line tool that facilitates continuous development for Kubernetes-native applications. Google Cloud stores the application's source code in GitHub. This architecture uses Google Cloud products for most of the components of the system, with Skaffold enabling the integration of the system. Because Skaffold is open source, you can use these principles to create a similar system using a combination of Google Cloud, in-house, and third-party components. The modularity of this solution means that you can adopt it incrementally as part of your development and deployment pipeline. Use cases The following are the key features of this integrated system: Develop and deploy faster. The development loop is efficient because you can validate changes in the developer workspace. Deployment is fast because the automated CI/CD system and increased parity across the environments allow you to detect more issues when you deploy changes to production. Benefit from increased parity across development, staging, and production. The components of this system use a common set of Google Cloud tools. Reuse configurations across the different environments. This reuse is done with Skaffold, which allows a common configuration format for the different environments. It also allows developers and operators to update and use the same configuration. Apply governance early in the workflow. This system applies validation tests for governance at production and in the CI system and development environment. Applying governance in the development environment allows problems to be found and fixed earlier. Let opinionated tooling manage your software delivery. Continuous delivery is fully managed, separating the stages of your CD pipeline from the details of rendering and deploying. Deployment To deploy this architecture, see Develop and deploy containerized apps using a CI/CD pipeline.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_env"", ""labels"": [ { ""text"": ""gcp_env"" } ], ""children"": [ { ""id"": ""dev_workspace_group"", ""labels"": [ { ""text"": ""dev_workspace_group"" } ], ""children"": [ { ""id"": ""cloud_shell_group"", ""labels"": [ { ""text"": ""cloud_shell_group"" } ], ""children"": [ { ""id"": ""develop_env"", ""labels"": [ { ""text"": ""Develop"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""development_cluster"", ""labels"": [ { ""text"": ""Development Cluster"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_deploy_dev_cluster"", ""labels"": [ { ""text"": ""deploys to"" } ] } ] }, { ""id"": ""cloud_source_repo"", ""labels"": [ { ""text"": ""Cloud Source Repositories"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_dev_commit"", ""labels"": [ { ""text"": ""commits"" } ] } ] }, { ""id"": ""cicd_pipeline_group"", ""labels"": [ { ""text"": ""cicd_pipeline_group"" } ], ""children"": [ { ""id"": ""cicd_services_group"", ""labels"": [ { ""text"": ""cicd_services_group"" } ], ""children"": [ { ""id"": ""cloud_build_service"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_deploy_service"", ""labels"": [ { ""text"": ""Cloud Deploy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_build_notify_deploy"", ""labels"": [ { ""text"": ""notifies"" } ] } ] }, { ""id"": ""artifacts_storage_group"", ""labels"": [ { ""text"": ""artifacts_storage_group"" } ], ""children"": [ { ""id"": ""build_artifacts_bucket"", ""labels"": [ { ""text"": ""Build Artifacts"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""deploy_artifacts_bucket"", ""labels"": [ { ""text"": ""Deploy Artifacts"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""artifact_registry_group"", ""labels"": [ { ""text"": ""artifact_registry_group"" } ], ""children"": [ { ""id"": ""application_container"", ""labels"": [ { ""text"": ""Application Container"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""clusters_group"", ""labels"": [ { ""text"": ""clusters_group"" } ], ""children"": [ { ""id"": ""staging_cluster"", ""labels"": [ { ""text"": ""Staging Cluster"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""production_cluster"", ""labels"": [ { ""text"": ""Production Cluster"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_deploy_to_staging"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_deploy_to_prod"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""edge_build_push_container"", ""labels"": [ { ""text"": ""pushes"" } ] }, { ""id"": ""edge_build_store_artifacts"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""edge_deploy_store_artifacts"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""edge_container_pull_staging"", ""labels"": [ { ""text"": ""pulls"" } ] }, { ""id"": ""edge_container_pull_prod"", ""labels"": [ { ""text"": ""pulls"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_repo_build_trigger"", ""labels"": [ { ""text"": ""triggers"" } ] } ] }, { ""id"": ""external_users"", ""labels"": [ { ""text"": ""external_users"" } ], ""children"": [ { ""id"": ""developer"", ""labels"": [ { ""text"": ""Developer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""github"", ""labels"": [ { ""text"": ""GitHub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""operations_approval"", ""labels"": [ { ""text"": ""Operations Approval"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_dev_develop"", ""labels"": [ { ""text"": ""develops on"" } ] }, { ""id"": ""edge_github_clone"", ""labels"": [ { ""text"": ""clones"" } ] }, { ""id"": ""edge_approval"", ""labels"": [ { ""text"": ""approves"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/deploy-guacamole-gke,"Apache Guacamole on GKE and Cloud SQL bookmark_border Last reviewed 2025-01-09 UTC Apache Guacamole offers a fully browser-based way to access remote desktops through Remote Desktop Protocol (RDP), Virtual Network Computing (VNC), and Secure Shell Protocol (SSH) on Compute Engine virtual machines (VMs). Identity-Aware Proxy (IAP) provides access to Guacamole with improved security. This reference architecture document is intended for server administrators and engineers who want to host Apache Guacamole on Google Kubernetes Engine (GKE) and Cloud SQL. This document assumes you are familiar with deploying workloads to Kubernetes and Cloud SQL for MySQL. This document also assumes you are familiar with Identity and Access Management and Google Compute Engine. Note: Apache Guacamole is not a full Virtual Desktop Infrastructure (VDI) solution by itself. For alternative solutions that provide a full VDI, see Virtual Desktops Solutions. Architecture The following diagram shows how a Google Cloud load balancer is configured with IAP, to protect an instance of the Guacamole client running in GKE: Architecture for Google Cloud load balancer configured with IAP. This architecture includes the following components: Google Cloud load balancer: Distributes traffic across multiple instances, which reduces the risk of performance issues. IAP: Provides improved security through a custom authentication extension. Guacamole client: Runs in GKE and connects to the guacd backend service. Guacd backend service: Brokers remote desktop connections to one or more Compute Engine VMs. Guacamole database in Cloud SQL: Manages configuration data for Guacamole. Compute Engine instances: VMs hosted on the Google infrastructure. Design considerations The following guidelines can help you to develop an architecture that meets your organization's requirements for security, cost, and performance. Security and compliance This architecture uses IAP to help protect access to the Guacamole service. Authorized users sign in to the Guacamole instance through a custom IAP authentication extension. For details, see the custom extension in GitHub. When you add additional users (through the Guacamole user interface), these additional users must have permissions through IAM, with the IAP-secured Web App User role. The OAuth configuration that this deployment creates is set to internal. Because of this setting, you must use a Google account in the same organization as the one you use to deploy Guacamole. If you use a Google account outside the organization, you receive an HTTP/403 org_internal error. Performance Google Cloud load balancer and GKE distributes traffic across multiple instances, which helps to reduce the risk of performance issues. Deployment To deploy this architecture, see Deploy Apache Guacamole on GKE and Cloud SQL.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""user_client"", ""labels"": [ { ""text"": ""User"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp"", ""labels"": [ { ""text"": ""gcp"" } ], ""children"": [ { ""id"": ""edge"", ""labels"": [ { ""text"": ""edge"" } ], ""children"": [ { ""id"": ""cloud_lb"", ""labels"": [ { ""text"": ""Cloud Load Balancer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""iap"", ""labels"": [ { ""text"": ""Identity-Aware Proxy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_lb_iap"", ""labels"": [ { ""text"": ""routes"" } ] } ] }, { ""id"": ""vpc"", ""labels"": [ { ""text"": ""vpc"" } ], ""children"": [ { ""id"": ""guacamole"", ""labels"": [ { ""text"": ""Guacamole (Web Client)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""guacd"", ""labels"": [ { ""text"": ""guacd (Backend)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""compute_instances"", ""labels"": [ { ""text"": ""compute_instances"" } ], ""children"": [ { ""id"": ""compute1"", ""labels"": [ { ""text"": ""Compute Engine 1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""compute2"", ""labels"": [ { ""text"": ""Compute Engine 2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""compute3"", ""labels"": [ { ""text"": ""Compute Engine 3"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_layer"", ""labels"": [ { ""text"": ""data_layer"" } ], ""children"": [ { ""id"": ""guacamole_db"", ""labels"": [ { ""text"": ""Guacamole DB"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_guac_guacd"", ""labels"": [ { ""text"": ""calls"" } ] }, { ""id"": ""e_guac_db"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_guacd_compute"", ""labels"": [ { ""text"": ""connects to"" } ] } ] } ], ""edges"": [ { ""id"": ""e_iap_guac"", ""labels"": [ { ""text"": ""forwards"" } ] } ] } ], ""edges"": [ { ""id"": ""e_user_lb"", ""labels"": [ { ""text"": ""requests"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/connected-devices/mqtt-broker-architecture,"Standalone MQTT broker architecture on Google Cloud bookmark_border Last reviewed 2024-08-09 UTC MQTT is an OASIS standard protocol for connected device applications that provides bidirectional messaging using a publish-and-subscribe broker architecture. The MQTT protocol is lightweight to reduce the network overhead, and MQTT clients are very small to minimize the use of resources on constrained devices. One solution for organizations who want to support connected device applications on Google Cloud is to run a standalone MQTT broker on Compute Engine or GKE. To deploy an MQTT broker in your organization, you need to make several key decisions which affect the overall architecture; in particular, load-balancing and the deployment environment. This document describes an architecture for deploying an MQTT broker, the core application in an MQTT deployment, on Google Cloud. It also describes the decisions that you need to make when you deploy this broker, and how they impact the architecture. This document is part of a series of documents that provide information about IoT architectures on Google Cloud. The other documents in this series include the following: Connected device architectures on Google Cloud overview Standalone MQTT broker architecture on Google Cloud (this document) IoT platform product architecture on Google Cloud Best practices for running an IoT backend on Google Cloud Device on Pub/Sub architecture to Google Cloud Best practices for automatically provisioning and configuring edge and bare metal systems and servers The following diagram shows an MQTT broker architecture running on Google Cloud. MQTT broker architecture diagram (explained in following text). The architecture in the preceding image is composed as follows: The MQTT broker is deployed as a cluster of three instances that are connected to the Cloud Load Balancing service. For the cloud load balancer, you can choose from one of several load-balancing products, which are described later in this document. The broker cluster includes a device credential store and a device authentication and authorization service. The cluster connects with the backend workloads through Dataflow or Pub/Sub. On the client side, edge gateways provide bidirectional communication between edge devices and the MQTT broker cluster through MQTT over TLS. Generally, we recommend that you deploy the MQTT broker application for this architecture in a cluster for scalability. Factors such as the clustering functionality, the scale-up and scale-down cluster management, data synchronization, and network partition handling are addressed by the specific broker implementations. Architectural considerations and choices The following sections describe the different architectural choices and considerations that you must make for a standalone MQTT broker architecture, and the impact that these choices have on the architecture. Connected devices Internet-connected edge devices publish their telemetry events and other information to the MQTT broker. To implement the standalone MQTT broker architecture that's described in this document, the device needs to have an MQTT client, the server certificate public key for TLS authentication, and the credentials needed to authenticate with the MQTT broker. In addition, edge devices generally have connectors to local sensors, to on-premises data systems, and to other devices that do not have internet access or IP connectivity. For example, the edge device can serve as a gateway for other local constrained devices connected to the gateway using BLE, to a wired connection, or to another near-field protocol. A detailed specification of the connected device architecture is outside the scope of this guide. Load balancing In the architecture, an external load-balancing service is configured between the public network and the MQTT broker cluster. This service provides several important networking functions, including distribution of incoming connections across backend nodes, session encryption, and authentication. Google Cloud supports several load balancer types. To choose the best load balancer for your architecture, consider the following: mTLS. mTLS handles both encryption and device authentication methods, while standard TLS handles only encryption and requires a separate device authentication method: If your application uses mTLS for device authentication and needs to terminate the TLS tunnel, we recommend that you use an external passthrough Network Load Balancer or an external proxy Network Load Balancer with a target TCP proxy. External proxy Network Load Balancers terminate the TLS session and proxy the connection to the broker node, along with any authentication credentials that are contained in the message. If you need the client connection information as part of the authentication scheme, you can preserve it in the backend connection by enabling the PROXY protocol. If your application doesn't use mTLS, we recommend that you use an external proxy Network Load Balancer with a target SSL proxy to offload the external TLS and SSL processing to the load balancer. External proxy Network Load Balancers terminate the TLS session and proxy the connection to the broker node, along with any authentication credentials that are contained in the message. If you need the client connection information as part of the authentication scheme, you can preserve it in the backend connection by enabling the PROXY protocol. HTTP(S) endpoints. If you need to expose HTTP(S) endpoints, we recommend that you configure a separate external Application Load Balancer for these endpoints. For more information about the load balancer types that Cloud Load Balancing supports, see Summary of Google Cloud load balancers. Load balancing strategy Any load-balancing service distributes connections from edge devices across the nodes in the cluster according to one of several algorithms or balancing modes. For MQTT, a session affinity load-balancing strategy is better than random load balancing. Because MQTT client-server connections are persistent bidirectional sessions, state is maintained on the broker node that stops the connection. In a clustered configuration, if a client disconnects and then reconnects to a different node, the session state is moved to the new node, which adds load on the cluster. This issue can be largely avoided by using session affinity load balancing. If clients frequently change their IP addresses, the connection can break, but in most cases session affinity is better for MQTT. Session affinity is available in all Cloud Load Balancing products. Device authentication and credential management MQTT broker applications handle device authentication and access control separately from Google Cloud. A broker application also provides its own credential store and management interface. The MQTT protocol supports basic username and password authentication in the initial Connect packet, and these fields are also frequently used by broker implementations to support other forms of authentication such as X.509 Certificate or JWT authentication. MQTT 5.0 also adds support for enhanced authentication methods that use challenge and response-style authentication. The authentication method that's used depends on the choice of MQTT broker application and your connected device use case. Regardless of the authentication method that the broker uses, the broker maintains a device credential store. This store can be in a local SQL database or a flat file. Some brokers also support the use of a managed database service such as Cloud SQL. You need a separate service to manage the device credential store and handle any integrations with other authentication services such as IAM. The development of this service is outside the scope of this guide. For more information about authentication and credential management, see Best practices for running an IoT backend on Google Cloud. Backend workloads Any connected device use case includes one or more backend applications that use the data ingested from the connected devices. Sometimes, these applications also need to send commands and configuration updates to the devices. In the standalone MQTT broker architecture in this document, incoming data and outgoing commands are both routed through the MQTT broker. There are different topics within the broker's topic hierarchy to differentiate between the data and the commands. Data and commands can be sent between the broker and the backend applications in one of several ways. If the application itself supports MQTT, or if it can be modified to support MQTT, the application can subscribe directly to the broker as a client. This approach enables you to use the MQTT Pub/Sub bidirectional messaging capability directly by using your application to receive data from and send commands to the connected devices. If your application does not support MQTT, there are several other options. In the architecture described in this document, Apache Beam provides an MQTT driver, which allows bidirectional integration with Dataflow and other Beam deployments. Many brokers also have plugin capabilities that support integration with services like Google Pub/Sub. These are typically one-way integrations for data integration, although some brokers support bidirectional integration. Use cases An MQTT broker architecture is particularly well suited for the device use cases that are described in the following sections. Standards-based data ingestion from heterogeneous devices When you want to collect and analyze data from a large fleet of heterogeneous devices, an MQTT broker is often a good solution. Because MQTT is a widely adopted and implemented standard, many edge devices have built-in support for it, and lightweight MQTT clients are available to add MQTT support to devices that don't. The publish-and-subscribe paradigm is also a part of the MQTT standard, so MQTT-enabled devices can take advantage of this architecture without additional implementation work. By contrast, devices that connect to Pub/Sub must implement the Pub/Sub API or use the Pub/Sub SDK. Running a standards-compliant MQTT broker on Google Cloud thus provides a simple solution for collecting data from a wide range of devices. When your connected devices are not controlled by your application but by a third party, you might not have access to the device system software, and the management of the device itself would be the other party's responsibility. In that circumstance, we recommend that you run an MQTT broker and provide authentication credentials to the third party to set up the device-to-cloud communication channel. Bidirectional communication for multi-party application integration The bidirectional messaging capability of MQTT makes it very suitable for a multiparty-mobile-application use case such as on-demand food delivery or a large-scale web chat application. MQTT has low protocol overhead, and MQTT clients have low resource demands. MQTT also features publish-and-subscribe routing, multiple quality of service (QoS) levels, built-in message retention, and broad protocol support. An MQTT broker can be the core component of a scalable messaging platform for on-demand services applications and similar use cases. Edge-to-cloud integrated messaging Because of the standardization and low overhead that MQTT offers, it can also be a good solution for integrating on-premises and cloud-based messaging applications. For instance, a factory operator can deploy multiple MQTT brokers in the on-premises environment to connect to sensors, machines, gateways, and other devices that are behind the firewall. The local MQTT broker can handle all bidirectional command and control and telemetry messaging for the on-premises infrastructure. The local broker can also be connected by two-way subscription to a parallel MQTT broker cluster in the cloud, allowing communication between the cloud and the edge environment without exposing the on-premises devices and systems to the public internet.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_cloud"", ""labels"": [ { ""text"": ""gcp_cloud"" } ], ""children"": [ { ""id"": ""processing_services"", ""labels"": [ { ""text"": ""processing_services"" } ], ""children"": [ { ""id"": ""mqtt_cluster"", ""labels"": [ { ""text"": ""mqtt_cluster"" } ], ""children"": [ { ""id"": ""broker_1"", ""labels"": [ { ""text"": ""Broker 1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""broker_2"", ""labels"": [ { ""text"": ""Broker 2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""broker_n"", ""labels"": [ { ""text"": ""Broker N"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""broker_admin"", ""labels"": [ { ""text"": ""broker_admin"" } ], ""children"": [ { ""id"": ""device_credential_store"", ""labels"": [ { ""text"": ""Device Credential Store"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_auth_service"", ""labels"": [ { ""text"": ""Device Auth & Authorize"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""broker_admin_api"", ""labels"": [ { ""text"": ""Broker Admin API"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_connectors"", ""labels"": [ { ""text"": ""gcp_connectors"" } ], ""children"": [ { ""id"": ""beam_mqttio_connector"", ""labels"": [ { ""text"": ""Beam MqttIO Connector"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mqtt_pubsub_connector"", ""labels"": [ { ""text"": ""MQTT Pub/Sub Connector"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""messaging"", ""labels"": [ { ""text"": ""messaging"" } ], ""children"": [ { ""id"": ""cloud_dataflow"", ""labels"": [ { ""text"": ""Cloud Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_pubsub"", ""labels"": [ { ""text"": ""Cloud Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_cluster_connectors"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_connector_dataflow"", ""labels"": [ { ""text"": ""streams"" } ] }, { ""id"": ""e_connector_pubsub"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_cluster_admin"", ""labels"": [ { ""text"": ""manages"" } ] } ] }, { ""id"": ""backend_apps"", ""labels"": [ { ""text"": ""backend_apps"" } ], ""children"": [ { ""id"": ""data_analytics"", ""labels"": [ { ""text"": ""Data Analytics"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""iot_applications"", ""labels"": [ { ""text"": ""IoT Applications"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_shadow"", ""labels"": [ { ""text"": ""Device Shadow/Twin"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gateway_services"", ""labels"": [ { ""text"": ""gateway_services"" } ], ""children"": [ { ""id"": ""cloud_load_balancer"", ""labels"": [ { ""text"": ""Cloud Load Balancing"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""project_resources"", ""labels"": [ { ""text"": ""project_resources"" } ], ""children"": [ { ""id"": ""security_resources"", ""labels"": [ { ""text"": ""security_resources"" } ], ""children"": [ { ""id"": ""cloud_iam"", ""labels"": [ { ""text"": ""Cloud IAM"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_kms"", ""labels"": [ { ""text"": ""Cloud KMS"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""operations_resources"", ""labels"": [ { ""text"": ""operations_resources"" } ], ""children"": [ { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""storage_resources"", ""labels"": [ { ""text"": ""storage_resources"" } ], ""children"": [ { ""id"": ""cloud_storage"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""artifact_registry"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_clb_cluster"", ""labels"": [ { ""text"": ""forwards MQTT"" } ] }, { ""id"": ""e_dataflow_analytics"", ""labels"": [ { ""text"": ""stores results"" } ] }, { ""id"": ""e_pubsub_iot"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_pubsub_shadow"", ""labels"": [ { ""text"": ""updates"" } ] }, { ""id"": ""e_apps_admin"", ""labels"": [ { ""text"": ""configures"" } ] } ] }, { ""id"": ""external_systems"", ""labels"": [ { ""text"": ""external_systems"" } ], ""children"": [ { ""id"": ""cloud_clients"", ""labels"": [ { ""text"": ""cloud_clients"" } ], ""children"": [ { ""id"": ""web_client"", ""labels"": [ { ""text"": ""Web Client"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mobile_client"", ""labels"": [ { ""text"": ""Mobile Client"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""edge_devices"", ""labels"": [ { ""text"": ""edge_devices"" } ], ""children"": [ { ""id"": ""device_tractor"", ""labels"": [ { ""text"": ""Tractor Device"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_drone"", ""labels"": [ { ""text"": ""Drone Device"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_wind"", ""labels"": [ { ""text"": ""Wind Turbine"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""edge_gateway_group"", ""labels"": [ { ""text"": ""edge_gateway_group"" } ], ""children"": [ { ""id"": ""edge_gateway"", ""labels"": [ { ""text"": ""Edge Gateway"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""certificate_store"", ""labels"": [ { ""text"": ""Certificate Store"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""edge_software"", ""labels"": [ { ""text"": ""Edge Software"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_connectors"", ""labels"": [ { ""text"": ""Device Connectors"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_web_gateway"", ""labels"": [ { ""text"": ""publishes MQTT"" } ] }, { ""id"": ""e_mobile_gateway"", ""labels"": [ { ""text"": ""publishes MQTT"" } ] }, { ""id"": ""e_device1_gateway"", ""labels"": [ { ""text"": ""publishes MQTT"" } ] }, { ""id"": ""e_device2_gateway"", ""labels"": [ { ""text"": ""publishes MQTT"" } ] }, { ""id"": ""e_device3_gateway"", ""labels"": [ { ""text"": ""publishes MQTT"" } ] } ] } ], ""edges"": [ { ""id"": ""e_gateway_clb"", ""labels"": [ { ""text"": ""routes"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/connected-devices/iot-platform-product-architecture,"IoT platform product architecture on Google Cloud bookmark_border Last reviewed 2024-08-09 UTC IoT platform products typically provide basic MQTT and HTTPS data connectivity. They also let you provision devices, and they provide authentication and management, telemetry storage and visualization, data processing, and alerting. Organizations often use IoT platforms when a standalone MQTT broker isn't sufficient for a use case and a more complete IoT platform product is needed. An IoT platform provides a unified interface for managing a heterogeneous collection of devices. This interface is important for many connected device applications, and it's a key difference between an IoT platform and a standalone MQTT broker. This document outlines the basic architectural considerations and recommendations that you need to make before you deploy an IoT platform product architecture on Google Cloud. This document is part of a series of documents that provide information about IoT architectures on Google Cloud. The other documents in this series include the following: Connected device architectures on Google Cloud overview Standalone MQTT broker architecture on Google Cloud IoT platform product architecture on Google Cloud (this document) Best practices for running an IoT backend on Google Cloud Device on Pub/Sub architecture to Google Cloud Best practices for automatically provisioning and configuring edge and bare metal systems and servers The following diagram shows an example architecture with a generic IoT platform product running on Google Cloud. A generic IoT platform architecture (flow of events explained in following text. As shown in the preceding diagram, the IoT platform deploys an MQTT broker or endpoint for device connectivity. The IoT platform is connected to an external proxy Network Load Balancer to distribute traffic from the edge devices. Additional IoT applications can connect to the IoT platform through Pub/Sub, or by using the Dataflow MQTT connector. The IoT platform provides a set of device management services. As shown in the diagram, these services are as follows: Device credential store Rules engine Device authentication and authorization Device configuration management Device registry Device update management IoT platform products also generally include services such as digital twin features, low-code development interfaces, alerting and notification capabilities, and other analytics functionality. Architectural considerations and choices The following sections describe the architectural choices that you can make for an IoT platform product architecture, and the impact of these choices. Ingestion endpoints Most commercial IoT platform applications include an MQTT endpoint, and usually also an HTTPS endpoint for data ingestion from connected devices. MQTT An IoT platform implements an MQTT endpoint in one of the following ways: A connector between MQTT and another message service An MQTT broker which implements the full MQTT specification When you evaluate commercial IoT platforms, it's important to know which of the preceding approaches the vendor has chosen for the product so that you can determine the implications for your use case. In some cases, the MQTT endpoint only connects the MQTT clients with a backend messaging service, such as Kafka or Pub/Sub. This type of endpoint usually does not implement the complete MQTT protocol specification, and often doesn't include features such as QoS levels 1 and 2, or shared subscriptions. The advantage of this approach is that it decreases complexity in the IoT platform, because there's no separate MQTT broker application. Operational costs are lower, and maintenance is simpler than if the platform uses a separate MQTT broker. However, because of the reduced support for more advanced MQTT protocol features, this approach means that there is less flexibility and functionality for MQTT message transport than a standalone MQTT broker that implements the complete MQTT specification. Other IoT platforms provide a complete MQTT broker as part of the platform, as shown in the example architecture in this document. This broker might be one of the existing open source brokers, or a proprietary broker implementation. A full MQTT broker provides the full bidirectional MQTT capability described earlier, but a full broker can add complexity and operational costs to the management of the IoT platform. HTTPS and other supplementary protocols In addition to MQTT, many IoT platforms provide more data ingestion endpoints than those that are shown in the main architecture that this document describes. HTTPS is a common alternative protocol to MQTT for connected device use cases. It has higher overhead than MQTT, but it's more widely supported by mobile devices such as phones, and by web browsers and other applications. It's frequently used in certain connected device applications, and it's supported by open source platforms like Eclipse Hono and many commercial products. Many constrained device applications use (Constrained Application Protocol (CoAP), defined in RFC 7252) as an MQTT alternative. CoAP targets low-overhead and small-footprint clients for embedded devices and sensors. Many commercial IoT platform applications also provide a CoAP endpoint. Load balancing For more information about choosing the best load balancer for your architecture, see the load balancing section of the Standalone MQTT broker architecture on Google Cloud because those considerations are applicable to this case as well. Device authentication and credential management Management of device credentials and authentication is a key part of the operation of an IoT platform. The authentication methods supported by connected devices vary widely across applications and device form factors. It's important to select the appropriate authentication method for the target use case, and implement the chosen authentication scheme correctly. Unlike a standalone MQTT Broker, an IoT platform provides integrated services to manage device identity and credentials. Most IoT platforms use X.509 client certificate authentication for authentication, JWT token-based authentication (often combined with OAuth 2.0), and username and password authentication. Some platforms also support integration with an external LDAP authentication provider. For some constrained devices, JWT or username and password authentication might be more appropriate, because these schemes require fewer resources on a connected device. When you use either JWT or username and password authentication, it's important that you encrypt the network connection separately to mTLS authentication, because an encrypted connection is not required by either of these authentication methods. X.509 certificate authentication, by contrast, consumes more resources on the connected device, but is typically used in an mTLS-encrypted connection and thus provides a high level of security. Provisioning the authentication credentials on the edge device at manufacturing time is also an important part of the connected device authentication scheme, but is outside the scope of this document. For more information about authentication and credential management, see Best practices for running an IoT backend on Google Cloud. Manage connected devices Typically, connected devices publish telemetry events and state information to the platform through one of the ingestion endpoints such as MQTT. If you're using a multi-protocol IoT platform, devices can communicate using any of the supported protocols. We recommend that your organization use an IoT platform that has the following capabilities: Software and system updates: The delivery and rollback of firmware, software, and application updates to the connected devices. These updates usually also involve storage and management of the updates themselves. Configuration updates: The delivery, storage, and rollback of updates to the configuration of applications deployed on the connected devices. Credential creation and management: The creation of new device credentials, delivery of those credentials to the connected device, auditing of device access attempts and activity, and revocation of compromised or expired credentials at the appropriate time. Rules engine and data processing: The definition and execution of data-driven rules and other data processing steps. This capability often includes some type of low-code interface for defining rules and data processing pipelines. Backend workloads Most IoT platforms provide their own internal data storage and transport capabilities that let you connect to your backend workloads and applications. AMQP, RabbitMQ, and Kafka are commonly used to provide internal data transport. These can all be connected to Pub/Sub using the Pub/Sub SDK. You can also use an integrated database system such as PostgreSQL to store data in the platform. In many cases, the IoT platform can be configured to use one of the Cloud Storage products directly, such as Cloud SQL, Firebase, or BigQuery. If the IoT platform has a complete MQTT broker, backend applications can also communicate with devices by using the MQTT capability of the platform. If the application supports MQTT, the application can connect with the broker as a subscriber. If there is no MQTT support, Apache Beam provides an MQTT driver, which enables bidirectional integration with Dataflow as well as other Beam deployments. Use cases The following sections describe example scenarios where an IoT platform is a better architectural choice than a standalone MQTT broker or a direct connection to Pub/Sub. Smart appliance management Applications that manage multiple smart appliances are well-suited to an IoT platform. An example of such an application is a platform to manage kitchen appliances such as dishwashers and coffee makers. These devices generally connect to a cloud-based application, either directly over Wi-Fi or through a local gateway that uses a Bluetooth Low Energy (BLE) or another local protocol. The management capabilities of an IoT platform are important here, for monitoring the state of each device, managing software updates and security patches, and capturing device activity to provide critical intelligence to the manufacturer and the customer. These capabilities are beyond the scope of a basic MQTT broker. At a minimum, a device information repository, a device state database, a telemetry datastore, and an analytics interface are all critical to building a successful smart appliance platform. Logistics and asset tracking For a logistics and asset tracking application, an IoT platform product offers more complete functionality than a basic MQTT broker, so is a better choice for this use case. Monitoring the current and past state and location of a large fleet of assets depends on a robust device state database and identity management system. As new assets are deployed, they need to be connected to the platform with as little friction as possible, and subsequently monitored throughout the asset lifecycle. In many cases, the application also collects other sensor information about the asset, such as local temperature, humidity and atmospheric pressure, or 3D positioning and acceleration data to detect unexpected movements or drops. All this data must be ingested and associated with the correct asset for analysis in any backend application, so the full-featured device management provided by the IoT platform is an important capability.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_cloud"", ""labels"": [ { ""text"": ""gcp_cloud"" } ], ""children"": [ { ""id"": ""support_services"", ""labels"": [ { ""text"": ""support_services"" } ], ""children"": [ { ""id"": ""backend_apps"", ""labels"": [ { ""text"": ""backend_apps"" } ], ""children"": [ { ""id"": ""data_analytics"", ""labels"": [ { ""text"": ""Data Analytics"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""iot_applications"", ""labels"": [ { ""text"": ""IoT Applications"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_shadow"", ""labels"": [ { ""text"": ""Device Shadow / Twin"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""project_resources"", ""labels"": [ { ""text"": ""project_resources"" } ], ""children"": [ { ""id"": ""operational_metrics"", ""labels"": [ { ""text"": ""operational_metrics"" } ], ""children"": [ { ""id"": ""cloud_iam"", ""labels"": [ { ""text"": ""Cloud IAM"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""storage_security"", ""labels"": [ { ""text"": ""storage_security"" } ], ""children"": [ { ""id"": ""cloud_storage"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_kms"", ""labels"": [ { ""text"": ""Cloud KMS"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""artifact_registry"", ""labels"": [ { ""text"": ""Artifact Registry"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_backend_resources"", ""labels"": [ { ""text"": ""uses"" } ] } ] }, { ""id"": ""core_platform"", ""labels"": [ { ""text"": ""core_platform"" } ], ""children"": [ { ""id"": ""iot_platform"", ""labels"": [ { ""text"": ""iot_platform"" } ], ""children"": [ { ""id"": ""connectivity"", ""labels"": [ { ""text"": ""connectivity"" } ], ""children"": [ { ""id"": ""mqtt_broker"", ""labels"": [ { ""text"": ""MQTT Broker"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_connectors"", ""labels"": [ { ""text"": ""gcp_connectors"" } ], ""children"": [ { ""id"": ""beam_mqtt_connector"", ""labels"": [ { ""text"": ""Beam MQTT Connector"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mqtt_pubsub_connector"", ""labels"": [ { ""text"": ""MQTT Pub/Sub Connector"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""device_management"", ""labels"": [ { ""text"": ""device_management"" } ], ""children"": [ { ""id"": ""dm_storage_security"", ""labels"": [ { ""text"": ""dm_storage_security"" } ], ""children"": [ { ""id"": ""device_credential_store"", ""labels"": [ { ""text"": ""Device Credential Store"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_registry"", ""labels"": [ { ""text"": ""Device Registry"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_auth"", ""labels"": [ { ""text"": ""Device AuthN/AuthZ"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""dm_config_rules"", ""labels"": [ { ""text"": ""dm_config_rules"" } ], ""children"": [ { ""id"": ""device_config_manager"", ""labels"": [ { ""text"": ""Device Config Manager"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""rules_engine"", ""labels"": [ { ""text"": ""Rules Engine"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ota_update_manager"", ""labels"": [ { ""text"": ""OTA Update Manager"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_management_api"", ""labels"": [ { ""text"": ""Device Management API"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_mqtt_beam"", ""labels"": [ { ""text"": ""streams"" } ] }, { ""id"": ""e_mqtt_pubsub"", ""labels"": [ { ""text"": ""streams"" } ] }, { ""id"": ""e_mqtt_rules"", ""labels"": [ { ""text"": ""internal messaging"" } ] } ] }, { ""id"": ""messaging"", ""labels"": [ { ""text"": ""messaging"" } ], ""children"": [ { ""id"": ""cloud_dataflow"", ""labels"": [ { ""text"": ""Cloud Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_pubsub"", ""labels"": [ { ""text"": ""Cloud Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_dataflow_pubsub"", ""labels"": [ { ""text"": ""publishes"" } ] } ] }, { ""id"": ""networking"", ""labels"": [ { ""text"": ""networking"" } ], ""children"": [ { ""id"": ""cloud_lb"", ""labels"": [ { ""text"": ""Cloud Load Balancing"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_lb_mqtt"", ""labels"": [ { ""text"": ""routes MQTT"" } ] }, { ""id"": ""e_beam_dataflow"", ""labels"": [ { ""text"": ""transforms"" } ] }, { ""id"": ""e_pubsubconn_dataflow"", ""labels"": [ { ""text"": ""transforms"" } ] } ] } ], ""edges"": [ { ""id"": ""e_pubsub_backend"", ""labels"": [ { ""text"": ""notifies"" } ] }, { ""id"": ""e_backend_dmapi"", ""labels"": [ { ""text"": ""manage"" } ] } ] }, { ""id"": ""external_actors"", ""labels"": [ { ""text"": ""external_actors"" } ], ""children"": [ { ""id"": ""edge_devices"", ""labels"": [ { ""text"": ""edge_devices"" } ], ""children"": [ { ""id"": ""edge_device1"", ""labels"": [ { ""text"": ""Tractor Device"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""edge_device2"", ""labels"": [ { ""text"": ""Robot Device"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""edge_device3"", ""labels"": [ { ""text"": ""Factory Sensor"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""edge_device4"", ""labels"": [ { ""text"": ""Wind Turbine Sensor"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""edge_gateway"", ""labels"": [ { ""text"": ""edge_gateway"" } ], ""children"": [ { ""id"": ""certificate_store"", ""labels"": [ { ""text"": ""Certificate Store / SE"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""edge_software"", ""labels"": [ { ""text"": ""Edge Software"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_connectors"", ""labels"": [ { ""text"": ""Device Connectors"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""cloud_clients"", ""labels"": [ { ""text"": ""cloud_clients"" } ], ""children"": [ { ""id"": ""web_client"", ""labels"": [ { ""text"": ""Web Dashboard"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mobile_client"", ""labels"": [ { ""text"": ""Mobile App"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_manager_client"", ""labels"": [ { ""text"": ""Service Client"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_edge_mqtt_lb"", ""labels"": [ { ""text"": ""MQTT TLS"" } ] }, { ""id"": ""e_cert_lb"", ""labels"": [ { ""text"": ""authenticate"" } ] }, { ""id"": ""e_edgeSW_lb"", ""labels"": [ { ""text"": ""transmit"" } ] }, { ""id"": ""e_devConn_lb"", ""labels"": [ { ""text"": ""connect"" } ] }, { ""id"": ""e_web_api"", ""labels"": [ { ""text"": ""request"" } ] }, { ""id"": ""e_mobile_api"", ""labels"": [ { ""text"": ""request"" } ] }, { ""id"": ""e_service_api"", ""labels"": [ { ""text"": ""request"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/connected-devices/device-pubsub-architecture,"Device on Pub/Sub connection to Google Cloud bookmark_border Last reviewed 2024-08-09 UTC Rather than implementing a specific architecture to connect devices to analytics applications, some organizations might benefit from connecting directly to Pub/Sub from edge devices. We recommend this approach for organizations that have a small number of connected devices that aggregate data from a larger number of devices and sensors in a local or on-premises network. This approach is also recommended when your organization has connected devices that are in a more secure environment, such as a factory. This document outlines the high-level architectural considerations that you need to make to use this approach to connect devices to Google Cloud products. This document is part of a series of documents that provide information about IoT architectures on Google Cloud. The other documents in this series include the following: Connected device architectures on Google Cloud overview Standalone MQTT broker architecture on Google Cloud IoT platform product architecture on Google Cloud Best practices for running an IoT backend on Google Cloud Device on Pub/Sub architecture to Google Cloud (this document) Best practices for automatically provisioning and configuring edge and bare metal systems and servers Architecture The following diagram shows a connected aggregation device or gateway that connects directly to Pub/Sub. Aggregation device or gateway architecture connected to Pub/Sub (flow of events explained in following text). The flow of events in the preceding diagram is as follows: You use the Identity and Access Management API to create a new key pair for a service account. The public key is stored in IAM. However, you must download the private key securely and store it in the gateway device so that it can be used for authentication. The aggregation device collects data from multiple other remote devices and sensors located within a secure local network. The remote devices communicate with the gateway using a local edge protocol such as MODBUS, BACNET, OPC-UA, or another local protocol. The aggregation device sends data to Pub/Sub over either HTTPS or gRPC. These API calls are signed using the service account private key held on the aggregation device. Architectural considerations and choices Because Pub/Sub is a serverless data streaming service, you can use it to create bidirectional systems that are composed of event producers and consumers (known as publishers and subscribers). In some connected device scenarios, you only need a scalable publish and subscribe service to create an effective data architecture. The following sections describe the considerations and choices that you need to make when you implement a device to Pub/Sub architecture on Google Cloud. Ingestion endpoints Pub/Sub provides prebuilt client libraries in multiple languages that implement the REST and gRPC APIs. It supports two protocols for message ingestion: REST (HTTP) and gRPC. For a connected device to send and receive data through Pub/Sub, the device must be able to interact with one of these endpoints. Many software applications have built-in support for REST APIs, so connecting with the Pub/Sub REST API is often the easiest solution. In some use cases, however, gRPC can be a more efficient and faster alternative. Because it uses serialized protocol buffers for the message payload instead of JSON, XML, or another text-based format, gRPC is better suited for the low-bandwidth applications that are commonly found in connected device use cases. gRPC API connections are also faster than REST for data transmission, and gRPC supports simultaneous bidirectional communication. One study found that gRPC is up to seven times faster than REST. As a result, for many connected device scenarios, gRPC is a better option if a gRPC connector is available or can be implemented for the connected device application. Device authentication and credential management Pub/Sub supports a number of authentication methods for access from outside Google Cloud. If your architecture includes an external identity provider such as Active Directory or a local Kubernetes cluster, you can use workload identity federation to manage access to Pub/Sub. This approach lets you create short-lived access tokens for connected devices. You can also grant IAM roles to your connected devices, without the management and security overhead of using service account keys. In cases when an external identity provider is not available, service account keys are the only option for authentication. Service account keys can become a security risk if not managed correctly, so we recommend that you follow security best practices for deploying service account keys to connected devices. To learn more, see Best practices for managing service account keys. Service accounts are also a limited resource and any cloud project has a limited quota of user-managed service accounts. Consequently, this approach is only an option for deployments that have a small number of devices that need to be connected. Backend applications After data is ingested into a Pub/Sub topic, the data is available to any application that runs on Google Cloud that has the appropriate credentials and access privileges. No additional connectors are necessary other than the Pub/Sub API in your application. Messages can be made available to multiple applications across your backend infrastructure for parallel processing or alerting, as well as archival storage and other analytics. Use cases The following sections describe example scenarios where a direct connection from devices to Pub/Sub is well suited for connected device use cases. Bulk data ingestion from an on-premises data historian A device to Pub/Sub connection is best suited for applications which have a small number of endpoints that transmit large volumes of data. An operational data historian is a good example of an on-premises system that stores a lot of data which needs to be transmitted to Google Cloud. For this use case, a small number of endpoints must be authenticated, typically one to a few connected devices, which is within the typical parameters for service account authentication. These systems also commonly have modular architectures, which lets you implement the Pub/Sub API connection that you need to communicate with Google Cloud. Local gateway data aggregation for a factory Aggregation of factory sensor data in a local gateway is another use case well suited for a direct Pub/Sub connection. In this case, a local data management and aggregation system are deployed on a gateway device in the factory. This system is typically a software product that connects to a wide variety of local sensors and machines. The product collects the data and frequently transforms it into a standardized representation before passing it on to the cloud application. Many devices can be connected in this scenario. However, those devices are usually only connected to the local gateway and are managed by the software on that device, so there's no need for a cloud-based management application. Unlike in an MQTT broker architecture, in this use case, the gateway plays an active role in aggregating and transforming the data. When the gateway connects to Google Cloud, it authenticates with Pub/Sub through a service account key. The key sends the aggregated and transformed data to the cloud application for further processing. The number of connected gateways is also typically in the range of tens to hundreds of devices, which is within the typical range for service account authentication.","{ ""id"": ""root"", ""children"": [ { ""id"": ""secure_local_network"", ""labels"": [ { ""text"": ""secure_local_network"" } ], ""children"": [ { ""id"": ""edge_devices"", ""labels"": [ { ""text"": ""edge_devices"" } ], ""children"": [ { ""id"": ""edge_device_1"", ""labels"": [ { ""text"": ""Edge Device 1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""edge_device_2"", ""labels"": [ { ""text"": ""Edge Device 2"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""aggregation_gateway"", ""labels"": [ { ""text"": ""Aggregation Gateway"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_edge1"", ""labels"": [ { ""text"": ""local protocol"" } ] }, { ""id"": ""e_edge2"", ""labels"": [ { ""text"": ""local protocol"" } ] } ] }, { ""id"": ""google_cloud"", ""labels"": [ { ""text"": ""google_cloud"" } ], ""children"": [ { ""id"": ""messaging"", ""labels"": [ { ""text"": ""messaging"" } ], ""children"": [ { ""id"": ""cloud_pubsub"", ""labels"": [ { ""text"": ""Cloud Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""cloud_applications"", ""labels"": [ { ""text"": ""cloud_applications"" } ], ""children"": [ { ""id"": ""data_analytics"", ""labels"": [ { ""text"": ""Data Analytics"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device_twin"", ""labels"": [ { ""text"": ""Device Twin"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""alerting"", ""labels"": [ { ""text"": ""Alerting"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""identity"", ""labels"": [ { ""text"": ""identity"" } ], ""children"": [ { ""id"": ""cloud_iam"", ""labels"": [ { ""text"": ""Cloud IAM"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_pubsub_to_cloudapps_stream"", ""labels"": [ { ""text"": ""streams to"" } ] }, { ""id"": ""e_cloudapps_to_pubsub_consume"", ""labels"": [ { ""text"": ""consumes"" } ] } ] } ], ""edges"": [ { ""id"": ""e_gateway_pubsub_publish"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_pubsub_gateway_deliver"", ""labels"": [ { ""text"": ""delivers"" } ] }, { ""id"": ""e_iam_to_gateway_key"", ""labels"": [ { ""text"": ""provides key"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/application-development/ecommerce-serverless,"Jump Start Solution: Ecommerce platform with serverless computing bookmark_border Last reviewed 2023-08-24 UTC This guide helps you understand, deploy, and use the Ecommerce platform with serverless computing solution. This solution demonstrates how to build and run an ecommerce application for a retail organization, with a publicly visible online store website. It shows you how to create an application that scales to handle spikes in usage (for example, during peak scale events like a seasonal sale), and that can manage requests based on the visitor's location. This design helps the online store provide a consistent service to geographically distributed customers. This solution is a good starting point if you want to learn how to deploy scalable ecommerce web apps with serverless capabilities. If you want granular operational control, check out the Ecommerce web app deployed on Kubernetes solution. This document assumes that you're familiar with basic cloud concepts, though not necessarily Google Cloud. Experience with Terraform is helpful. Note: This solution helps you explore the capabilities of Google Cloud. The solution is not intended to be used as is for production environments. For information about designing and setting up production-grade environments in Google Cloud, see Landing zone design in Google Cloud and Google Cloud setup checklist. Objectives This solution guide helps you do the following: Learn how to design a system architecture for an ecommerce website. Optimize an ecommerce website for performance, scale, and responsiveness. Monitor and anticipate load limitations. Use tracing and error reporting to understand and manage problems. Products The solution uses the following Google Cloud products: Cloud Run: A fully managed service that lets you build and deploy serverless containerized apps. Google Cloud handles scaling and other infrastructure tasks so that you can focus on the business logic of your code. Cloud SQL: A cloud-based PostgreSQL database that's fully managed on the Google Cloud infrastructure. Secret Manager: A service that lets you store, manage, and access secrets as binary blobs or text strings. You can use Secret Manager to store database passwords, API keys, or TLS certificates that are needed by an application at runtime. Cloud Storage: An enterprise-ready service that provides low-cost, no-limit object storage for diverse data types. Data is accessible from within and outside of Google Cloud and is replicated geo-redundantly. Firebase Hosting: A fully managed hosting service to deploy and serve your web applications and static content. Cloud Logging: A service that lets you store, search, analyze, monitor, and alert on logging data and events from Google Cloud and other clouds. Cloud Trace: A distributed tracing system for Google Cloud that helps you understand how long it takes your application to handle incoming requests from users or other applications, and how long it takes to complete operations like RPC calls performed when handling the requests. Error Reporting: This service aggregates and displays errors produced in your running cloud services. Error Reporting groups errors which are considered to have the same root cause. Architecture The following diagram shows the architecture of the solution: Ecommerce web application deployed with Cloud Run Request flow The following is the request processing flow of the ecommerce platform. The steps in the flow are numbered as shown in the preceding architecture diagram. A Firebase Hosting client frontend. The frontend uses Lit and web components for client-side rendering of API data. The web client calls an API backend that is running as a Cloud Run service. The Cloud Run API server is written in Django using the Django REST Framework. The configuration and other secrets for the Python application are stored in Secret Manager. Static assets for the application are stored in Cloud Storage. A Cloud SQL database, using PostgreSQL, is used as the relational database backend for the Python application. Cloud Logging, Cloud Trace, and Error Reporting stores logs, OpenTelemetry traces, and error reports sent by other cloud products and the Cloud Run API server. This data enables monitoring for the correct application behavior and troubleshooting of unexpected behavior. Cost For an estimate of the cost of the Google Cloud resources that the Ecommerce platform with serverless computing solution uses, see the precalculated estimate in the Google Cloud Pricing Calculator. Use the estimate as a starting point to calculate the cost of your deployment. You can modify the estimate to reflect any configuration changes that you plan to make for the resources that are used in the solution. The precalculated estimate is based on assumptions for certain factors, including the following: The Google Cloud locations where the resources are deployed. The amount of time that the resources are used.","{ ""id"": ""root"", ""children"": [ { ""id"": ""consumers"", ""labels"": [ { ""text"": ""consumers"" } ], ""children"": [ { ""id"": ""computer_client"", ""labels"": [ { ""text"": ""Computer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""smartphone_client"", ""labels"": [ { ""text"": ""Smartphone"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_project"", ""labels"": [ { ""text"": ""gcp_project"" } ], ""children"": [ { ""id"": ""application"", ""labels"": [ { ""text"": ""application"" } ], ""children"": [ { ""id"": ""frontend"", ""labels"": [ { ""text"": ""frontend"" } ], ""children"": [ { ""id"": ""firebase_hosting"", ""labels"": [ { ""text"": ""Firebase Hosting"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""api"", ""labels"": [ { ""text"": ""api"" } ], ""children"": [ { ""id"": ""cloud_run_api"", ""labels"": [ { ""text"": ""Cloud Run - Django API"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""config_mgr"", ""labels"": [ { ""text"": ""config_mgr"" } ], ""children"": [ { ""id"": ""secret_manager"", ""labels"": [ { ""text"": ""Secret Manager"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_layer"", ""labels"": [ { ""text"": ""data_layer"" } ], ""children"": [ { ""id"": ""cloud_storage_static"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_sql_db"", ""labels"": [ { ""text"": ""Cloud SQL (PostgreSQL)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""observability"", ""labels"": [ { ""text"": ""observability"" } ], ""children"": [ { ""id"": ""cloud_logging_store"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_trace_traces"", ""labels"": [ { ""text"": ""Cloud Trace"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""error_reporting_logs"", ""labels"": [ { ""text"": ""Error Reporting"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_firebase_cloudrun"", ""labels"": [ { ""text"": ""route"" } ] }, { ""id"": ""e_cloudrun_secretmanager"", ""labels"": [ { ""text"": ""fetch config"" } ] }, { ""id"": ""e_cloudrun_storage"", ""labels"": [ { ""text"": ""store static"" } ] }, { ""id"": ""e_cloudrun_sql"", ""labels"": [ { ""text"": ""query"" } ] }, { ""id"": ""e_cloudrun_logging"", ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""e_cloudrun_trace"", ""labels"": [ { ""text"": ""traces"" } ] }, { ""id"": ""e_cloudrun_error"", ""labels"": [ { ""text"": ""reports"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_computer_firebase"", ""labels"": [ { ""text"": ""HTTPS"" } ] }, { ""id"": ""e_smartphone_firebase"", ""labels"": [ { ""text"": ""HTTPS"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/manage-and-scale-windows-networking,"Manage and scale networking for Windows applications that run on managed Kubernetes bookmark_border Release Notes Last reviewed 2024-08-14 UTC This reference architecture provides a highly available and scalable solution that uses Cloud Service Mesh and Envoy gateways to manage network traffic for Windows applications that run on Google Kubernetes Engine (GKE). It explains how to manage that network traffic by using a service that can route traffic to Pods and to an open-source xDS-compliant proxy. Using an architecture like this can help to reduce costs and improve network management. This document is intended for cloud architects, network administrators and IT professionals who are responsible for designing and managing Windows applications running on GKE. Architecture The following diagram shows an architecture for managing networking for Windows applications running on GKE using Cloud Service Mesh and Envoy gateways: Data flows through an internal Application Load Balancer and an Envoy gateway. The architecture includes the following components: A regional GKE cluster with both Windows and Linux node pools. Two Windows applications running in two separate GKE Pods. Each application is exposed by a ClusterIP-type Kubernetes Service and a network endpoint group (NEG). Cloud Service Mesh creates and manages the traffic routes to the NEGs for each GKE Pod. Each route is mapped to a specific scope. That scope uniquely identifies a Cloud Service Mesh ingress gateway. HTTP routes that map to the backend services for Cloud Service Mesh. Envoy container Pods that act as an Envoy Gateway to the GKE cluster. Envoy gateways that run on Linux nodes. The gateways are configured to direct traffic to the Windows applications through the services that correspond to those applications. Envoy is configured to use the scope parameter to load the configuration details of the relevant Cloud Service Mesh services. An internal Application Load Balancer that terminates SSL traffic and directs all external incoming traffic to the Envoy gateways. Products used This reference architecture uses the following Google Cloud and third-party products: Google Cloud products Cloud Load Balancing: A portfolio of high performance, scalable, global and regional load balancers. Google Kubernetes Engine (GKE): A Kubernetes service that you can use to deploy and operate containerized applications at scale using Google's infrastructure. Cloud Service Mesh: A suite of tools that helps you monitor and manage a reliable service mesh on-premises or on Google Cloud. Third-party products Envoy Gateway: Manages an Envoy proxy as a standalone or Kubernetes-based application gateway. Gateway API: An official Kubernetes project focused on L4 and L7 routing in Kubernetes. Use case The main use case for this reference architecture is to manage network traffic for Windows applications that run on GKE. This architecture provides the following benefits: Simplified network management: Cloud Service Mesh and Envoy gateways provide simplified network management through a centralized control plane that manages network traffic to applications. These applications can be either Linux or Windows applications that run on GKE or Compute Engine. Using this simplified network management scheme reduces the need for manual configuration. Enhanced scalability and availability: To meet your changing demands, use Cloud Service Mesh and Envoy gateways to scale your Linux and Windows applications. You can also use Envoy gateways to provide high availability for your applications by load balancing traffic across multiple Pods. Improved security: Use Envoy gateways to add security features to your Linux and Windows applications, such as SSL termination, authentication, and rate limiting. Reduced costs: Both Cloud Service Mesh and Envoy gateways can help reduce the costs of managing network traffic for Linux and Windows applications. Design considerations This section provides guidance to help you develop an architecture that meets your specific requirements for security, reliability, cost, and efficiency. Security Secured networking: The architecture uses an internal Application Load Balancer to encrypt incoming traffic to the Windows containers. Encryption in transit helps to prevent data leakage. Windows containers: Windows containers help provide a secure and isolated environment for containerized applications. Reliability Load balancing: The architecture uses multiple layers of Cloud Load Balancing to distribute traffic across the Envoy gateways and the Windows containers. Fault tolerance: This architecture is fault tolerant with no single point of failure. This design helps to ensure that it's always available, even if one or more of the components fails. Autoscaling: The architecture uses autoscaling to automatically scale the number of Envoy gateways and Windows containers based on the load. Autoscaling helps to ensure that the gateways, and the applications, can handle spikes in traffic without experiencing performance issues. Monitoring: The architecture uses Google Cloud Managed Service for Prometheus and Cloud Operations to monitor the health of the Envoy gateways and Windows containers. Monitoring helps you identify issues early and potentially prevent them from disrupting your applications. Cost optimization Choose the right instance types for your workloads: Consider the following factors when choosing instance types: The number of vCPUs and memory your applications require The expected traffic load for your applications The need for users to have highly available applications Use autoscaling: Autoscaling can help you save money by automatically scaling your Windows workloads vertically and horizontally. Vertical scaling tunes container requests and limits according to customer use. Automate vertical scaling with vertical Pod autoscaling. Horizontal scaling adds or removes Kubernetes Pods to meet demand. Automate horizontal scaling with horizontal Pod autoscaling. Use Cloud Service Mesh and Envoy gateways: Cloud Service Mesh and Envoy gateways can help you save money by efficiently routing traffic to your Windows applications. Using more efficient routing can help reduce the amount of bandwidth you must purchase. It can also help improve the performance of those applications. Use shared Virtual Private Cloud (VPC) networks: Shared Virtual Private Cloud networks let you share a single VPC across multiple projects. Sharing can help you save money by reducing the number of VPCs that you need to create and manage. Operational efficiency Multiple domains with a single internal load balancer: The architecture uses internal Application Load Balancers to offload SSL traffic. Each HTTPS target proxy can support multiple SSL certificates (up to the supported maximum) to manage multiple applications with different domains. Infrastructure as Code (IaC): To manage the infrastructure, the architecture can be deployed using IaC. IaC helps to ensure that your infrastructure is consistent and repeatable. Deployment To deploy this architecture, see Deploy Windows applications running on managed Kubernetes.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""client"", ""labels"": [ { ""text"": ""User"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""google_cloud"", ""labels"": [ { ""text"": ""google_cloud"" } ], ""children"": [ { ""id"": ""gateway_group"", ""labels"": [ { ""text"": ""gateway_group"" } ], ""children"": [ { ""id"": ""alb"", ""labels"": [ { ""text"": ""HTTPS Internal Load Balancer"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gke_cluster"", ""labels"": [ { ""text"": ""gke_cluster"" } ], ""children"": [ { ""id"": ""linux_nodepool"", ""labels"": [ { ""text"": ""linux_nodepool"" } ], ""children"": [ { ""id"": ""envoy_gateway"", ""labels"": [ { ""text"": ""Envoy Gateway"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""windows_nodepool"", ""labels"": [ { ""text"": ""windows_nodepool"" } ], ""children"": [ { ""id"": ""win_app1_service"", ""labels"": [ { ""text"": ""Win App1 Service (NEG 80)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""win_app1_pod"", ""labels"": [ { ""text"": ""Win App1 Pod"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""win_app2_service"", ""labels"": [ { ""text"": ""Win App2 Service (NEG 80)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""win_app2_pod"", ""labels"": [ { ""text"": ""Win App2 Pod"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_app1_service_pod"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""e_app2_service_pod"", ""labels"": [ { ""text"": ""serves"" } ] } ] } ], ""edges"": [ { ""id"": ""e_envoy_app1_service"", ""labels"": [ { ""text"": ""forwards"" } ] }, { ""id"": ""e_envoy_app2_service"", ""labels"": [ { ""text"": ""forwards"" } ] } ] }, { ""id"": ""control_plane"", ""labels"": [ { ""text"": ""control_plane"" } ], ""children"": [ { ""id"": ""gateway_cr"", ""labels"": [ { ""text"": ""Gateway (scope: gateway-proxy)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""http_route_app1"", ""labels"": [ { ""text"": ""HTTP Route App1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""http_route_app2"", ""labels"": [ { ""text"": ""HTTP Route App2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_service_mesh"", ""labels"": [ { ""text"": ""Cloud Service Mesh"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_gateway_route1"", ""labels"": [ { ""text"": ""configures"" } ] }, { ""id"": ""e_gateway_route2"", ""labels"": [ { ""text"": ""configures"" } ] }, { ""id"": ""e_route1_mesh"", ""labels"": [ { ""text"": ""registers"" } ] }, { ""id"": ""e_route2_mesh"", ""labels"": [ { ""text"": ""registers"" } ] } ] } ], ""edges"": [ { ""id"": ""e_alb_envoy"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_mesh_app1_service"", ""labels"": [ { ""text"": ""programs"" } ] }, { ""id"": ""e_mesh_app2_service"", ""labels"": [ { ""text"": ""programs"" } ] } ] } ], ""edges"": [ { ""id"": ""e_client_alb"", ""labels"": [ { ""text"": ""HTTPS requests"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/application-development/dynamic-app-python,"Jump Start Solution: Dynamic web application with Python and JavaScript bookmark_border Last reviewed 2024-05-10 UTC This guide helps you understand, deploy, and use the Dynamic web application with Python and JavaScript Jump Start Solution. This solution demonstrates how to build and run dynamic websites on Google Cloud. The deployment and management of this application serves as a sample real-world implementation of building a dynamic web application with many of the tools and products that you can use for serverless applications. You might deploy this solution if you want to learn about the following: Django REST framework (DRF) application Storefront application Database-backed website This solution deploys the Avocano app. This is a faux storefront, where users can attempt to purchase an item. However, if a user adds an item to their cart, the store reveals it is fake (Avoca—no!). Although users can't actually purchase an avocado, the app illustrates inventory management, by decreasing the amount of available product by one. Avocano showcases the combination of a Cloud Run-powered API, a Cloud SQL for PostgreSQL database, and a Firebase frontend. See the Avocano application source code. Note: This solution helps you explore the capabilities of Google Cloud. The solution is not intended to be used as is for production environments. For information about designing and setting up production-grade environments in Google Cloud, see Landing zone design in Google Cloud and Google Cloud setup checklist. Objectives This solution guide will help you learn how to use Google Cloud to perform the following tasks: Deploy a publicly accessible web application Connect an application to a Google Cloud database following Google Cloud's recommended security practices: Using Secret Manager to store passwords, keys, and certificates. Granting only the IAM permissions required to perform the task. This is also known as applying the principle of least privilege. Deploy and operate the backend services. Customize your application Make content changes or modify the application to add a feature. Build and redeploy securely. Products used The following is a summary of the Google Cloud products that this solution uses: Cloud Run: A fully managed service that lets you build and deploy serverless containerized apps. Google Cloud handles scaling and other infrastructure tasks so that you can focus on the business logic of your code. Jobs: Container-based task processing. Cloud Build: A service that lets you import source code from repositories or Cloud Storage spaces, execute a build, and produce artifacts such as Docker containers or Java archives. Cloud SQL for PostgreSQL: A cloud-based PostgreSQL database that's fully managed on the Google Cloud infrastructure. Secret Manager: A service that lets you store, manage, and access secrets as binary blobs or text strings. You can use Secret Manager to store database passwords, API keys, or TLS certificates that are needed by an application at runtime. Cloud Storage: An enterprise-ready service that provides low-cost, no-limit object storage for diverse data types. Data is accessible from within and outside of Google Cloud and is replicated geo-redundantly. Firebase: A development platform that lets you build, release, and monitor applications for iOS, Android, and the web. Architecture The following diagram shows the architecture of the solution: Dynamic web application using Django architecture diagram Components and configuration The architecture includes the following components: The web client is hosted on Firebase Hosting. The web client calls an API backend written in Python that runs as a service on Cloud Run. The configuration and other secrets for the Python application are stored in Secret Manager. Cloud SQL for PostgreSQL is used as the relational database backend for the Python application. Static assets for the application and container images are stored in Cloud Storage. Cost For an estimate of the cost of the Google Cloud resources that the dynamic web application solution uses, see the precalculated estimate in the Google Cloud Pricing Calculator. Use the estimate as a starting point to calculate the cost of your deployment. You can modify the estimate to reflect any configuration changes that you plan to make for the resources that are used in the solution. The precalculated estimate is based on assumptions for certain factors, including the following: The Google Cloud locations where the resources are deployed. The amount of time that the resources are used. Before you begin To deploy this solution, you first need a Google Cloud project and some IAM permissions. Create or choose a Google Cloud project When you deploy the solution, you choose the Google Cloud project where the resources are deployed. You can either create a new project or use an existing project for the deployment. If you want to create a new project, do so before you begin the deployment. Using a new project can help avoid conflicts with previously provisioned resources, such as resources that are used for production workloads. To create a project, complete the following steps: In the Google Cloud console, go to the project selector page. Go to project selector Click Create project. Name your project. Make a note of your generated project ID. Edit the other fields as needed. Click Create. Get the required IAM permissions To start the deployment process, you need the Identity and Access Management (IAM) permissions that are listed in the following table. If you created a new project for this solution, then you have the roles/owner basic role in that project and have all the necessary permissions. If you don't have the roles/owner role, then ask your administrator to grant these permissions (or the roles that include these permissions) to you. IAM permission required	Predefined role that includes the required permissions serviceusage.services.enable Service Usage Admin (roles/serviceusage.serviceUsageAdmin) iam.serviceAccounts.create Service Account Admin (roles/iam.serviceAccountAdmin) resourcemanager.projects.setIamPolicy Project IAM Admin (roles/resourcemanager.projectIamAdmin) config.deployments.create config.deployments.list	Cloud Infrastructure Manager Admin (roles/config.admin) iam.serviceAccount.actAs	Service Account User (roles/iam.serviceAccountUser) About temporary service account permissions If you start the deployment process through the console, Google creates a service account to deploy the solution on your behalf (and to delete the deployment later if you choose). This service account is assigned certain IAM permissions temporarily; that is, the permissions are revoked automatically after the solution deployment and deletion operations are completed. Google recommends that after you delete the deployment, you delete the service account, as described later in this guide. View the roles assigned to the service account Deploy the solution To help you deploy this solution with minimal effort, a Terraform configuration is provided in GitHub. The Terraform configuration defines all the Google Cloud resources that are required for the solution. You can deploy the solution by using one of the following methods: Through the console: Use this method if you want to try the solution with the default configuration and see how it works. Cloud Build deploys all the resources that are required for the solution. When you no longer need the deployed solution, you can delete it through the console. Any resources that you create after you deploy the solution might need to be deleted separately. To use this deployment method, follow the instructions in Deploy through the console. Using the Terraform CLI: Use this method if you want to customize the solution or if you want to automate the provisioning and management of the resources by using the infrastructure as code (IaC) approach. Download the Terraform configuration from GitHub, optionally customize the code as necessary, and then deploy the solution by using the Terraform CLI. After you deploy the solution, you can continue to use Terraform to manage the solution. To use this deployment method, follow the instructions in Deploy using the Terraform CLI.","{ ""id"": ""root"", ""children"": [ { ""id"": ""client"", ""labels"": [ { ""text"": ""client"" } ], ""children"": [ { ""id"": ""device1"", ""labels"": [ { ""text"": ""Device"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""device2"", ""labels"": [ { ""text"": ""Device"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_project"", ""labels"": [ { ""text"": ""gcp_project"" } ], ""children"": [ { ""id"": ""services_group"", ""labels"": [ { ""text"": ""services_group"" } ], ""children"": [ { ""id"": ""firebase"", ""labels"": [ { ""text"": ""Firebase Web"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_run"", ""labels"": [ { ""text"": ""Cloud Run (DRF API)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""secret_manager"", ""labels"": [ { ""text"": ""Secret Manager"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_firebase_cloudrun"", ""labels"": [ { ""text"": ""calls"" } ] }, { ""id"": ""edge_cloudrun_secret"", ""labels"": [ { ""text"": ""retrieves"" } ] } ] }, { ""id"": ""data_group"", ""labels"": [ { ""text"": ""data_group"" } ], ""children"": [ { ""id"": ""cloud_sql"", ""labels"": [ { ""text"": ""Cloud SQL (PostgreSQL)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_storage"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_cloudrun_sql"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""edge_cloudrun_storage"", ""labels"": [ { ""text"": ""stores"" } ] } ] } ], ""edges"": [ { ""id"": ""edge_device1_firebase"", ""labels"": [ { ""text"": ""requests"" } ] }, { ""id"": ""edge_device2_firebase"", ""labels"": [ { ""text"": ""requests"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/application-development/cloud-client-api,"Jump Start Solution: Cloud SDK Client Library bookmark_border Last reviewed 2024-03-21 UTC This guide helps you understand and deploy the Cloud SDK Client Library solution. This solution lets you to interact with Google Cloud by using the Google Cloud SDK Client Libraries to process and aggregate data, and then show a radar visualization. Use this app to identify trends and observations based on the aggregate data. This solution will help you learn key skills for successfully making API calls. This solution uses the Google Cloud SDK Client Libraries to access Google Cloud APIs programmatically, leveraging Google Cloud services (Cloud Run jobs and Cloud Storage) to reduce boilerplate code. In this solution, the code parses a sample dataset (the 2018 Central Park Squirrel Census) with Cloud Run jobs and Cloud Storage. All Google Cloud SDK Client requests are logged into Cloud Logging, using a common pattern to enable troubleshooting and observability so you can see how long those requests take and where a process may encounter an error. This solution will also guide you through the execution of a Cloud Run job to process and store the dataset. APIs are the fundamental mechanism that developers use to interact with Google Cloud products and services. Google Cloud SDK provides language-specific Cloud Client libraries supporting eight different languages and their conventions and styles. Use this solution to learn how to use Google Cloud SDK Client Libraries to process data and deploy a frontend application where you can view the results. Note: This solution helps you explore the capabilities of Google Cloud. The solution is not intended to be used as is for production environments. For information about designing and setting up production-grade environments in Google Cloud, see Landing zone design in Google Cloud and Google Cloud setup checklist. Objectives This solution guide helps you do the following: Learn how to use a client library for Google Cloud API calls. Deploy an interactive dataset using Cloud Run jobs and Cloud Storage. Explore Google Cloud API calls using Cloud Logging. View the Cloud Run application, service account configurations, and enabled APIs and their usage. Architecture This solution deploys the raw data to a bucket in Cloud Storage, configures a Cloud Run job to process the data and store to a separate bucket in Cloud Storage, and deploys a frontend service in Cloud Run that can view and interact with the processed data. The following diagram shows the architecture of the solution: Architecture of the infrastructure required for the Cloud SDK Client Library solution. The following section describes the Google Cloud resources that are shown in the diagram. Components and configuration The following is the request processing flow of this solution. The steps in the flow are numbered as shown in the preceding architecture diagram. Unprocessed data has been uploaded to a Cloud Storage bucket. A Cloud Run job transforms the raw data into a more structured format that the frontend service can understand. The Cloud Run job uploads the processed data in a second Cloud Storage bucket. The frontend, hosted as a Cloud Run service, pulls the processed data from the second Cloud Storage bucket. The user can visit the web application served by the frontend Cloud Run service. Products used The solution uses the following Google Cloud products: Cloud Storage: An enterprise-ready service that provides low-cost, no-limit object storage for diverse data types. Data is accessible from within and outside of Google Cloud and is replicated geo-redundantly. Cloud Logging: A service that lets you store, search, analyze, monitor, and alert on logging data and events from Google Cloud and other clouds. Cloud Run: A fully managed service that lets you build and deploy serverless containerized apps. Google Cloud handles scaling and other infrastructure tasks so that you can focus on the business logic of your code. Cost For an estimate of the cost of the Google Cloud resources that the Cloud SDK Client Library solution uses, see the precalculated estimate in the Google Cloud Pricing Calculator. Use the estimate as a starting point to calculate the cost of your deployment. You can modify the estimate to reflect any configuration changes that you plan to make for the resources that are used in the solution. The precalculated estimate is based on assumptions for certain factors, including the following: The Google Cloud locations where the resources are deployed. The amount of time that the resources are used. The Google Cloud locations where the resources are deployed. The amount of time that the resources are used.","{ ""id"": ""root"", ""children"": [ { ""id"": ""client_group"", ""labels"": [ { ""text"": ""client_group"" } ], ""children"": [ { ""id"": ""user"", ""labels"": [ { ""text"": ""User"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_project_group"", ""labels"": [ { ""text"": ""gcp_project_group"" } ], ""children"": [ { ""id"": ""frontend_group"", ""labels"": [ { ""text"": ""frontend_group"" } ], ""children"": [ { ""id"": ""cloud_run_frontend"", ""labels"": [ { ""text"": ""Cloud Run Frontend"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_processing_group"", ""labels"": [ { ""text"": ""data_processing_group"" } ], ""children"": [ { ""id"": ""raw_storage"", ""labels"": [ { ""text"": ""Cloud Storage Raw Data"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""batch_processing_job"", ""labels"": [ { ""text"": ""Cloud Run Batch Processing"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""processed_storage"", ""labels"": [ { ""text"": ""Cloud Storage Processed Data"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_raw_to_batch"", ""labels"": [ { ""text"": ""provides raw data"" } ] }, { ""id"": ""e_batch_to_processed"", ""labels"": [ { ""text"": ""stores processed data"" } ] } ] } ], ""edges"": [ { ""id"": ""e_frontend_to_batch"", ""labels"": [ { ""text"": ""triggers"" } ] } ] } ], ""edges"": [ { ""id"": ""e_user_frontend"", ""labels"": [ { ""text"": ""requests"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/application-development/three-tier-web-app,"Jump Start Solution: Three-tier web app bookmark_border Last reviewed 2024-10-08 UTC This guide helps you understand and deploy the Three-tier web app Jump Start Solution, which demonstrates how to quickly deploy a multi-tier web application stack to Google Cloud. The three-tier web app solution deploys a task-tracker app in Google Cloud. The app has a web-based frontend and an API layer in the middle tier. The frontend and API layer are containerized apps that are deployed as serverless services. The backend is a SQL database. The solution also includes an in-memory cache to serve frequently accessed data. Each tier in this solution is independent. You can develop, update, and scale any tier without affecting the other tiers. This architecture enables efficient app development and delivery. This guide is intended for developers who have some background with deploying multi-tier app stacks. It assumes that you're familiar with basic cloud concepts, though not necessarily Google Cloud. Experience with Terraform is helpful. Note: This solution helps you explore the capabilities of Google Cloud. The solution is not intended to be used as is for production environments. For information about designing and setting up production-grade environments in Google Cloud, see Landing zone design in Google Cloud and Google Cloud setup checklist. Products used The solution uses the following Google Cloud products: Cloud Run: A fully managed service that lets you build and deploy serverless containerized apps. Google Cloud handles scaling and other infrastructure tasks so that you can focus on the business logic of your code. Cloud SQL: A fully managed MySQL or PostgreSQL database in Google Cloud. Memorystore for Redis: A service that provides application caching using a scalable, secure, and highly available in-memory service for Redis and Memcached. Virtual Private Cloud (VPC) network: A global virtual network that spans all Google Cloud regions and that lets you interconnect your cloud resources. For information about how these products are configured and how they interact, see the next section. Architecture The example app that the three-tier web app solution deploys is a task-tracking app for which the code already exists. The following diagram shows the architecture of the infrastructure that the solution deploys: Architecture of the infrastructure required for the three-tier web app solution. The following subsections describe the request flow and the configuration of the Google Cloud resources that are shown in the diagram. Request flow The following is the request processing flow of the task-tracker app that this solution deploys. The steps in the flow are numbered as shown in the preceding architecture diagram. A web-based frontend receives requests from clients to the task-tracker app. The frontend is a Cloud Run service, which renders an HTML client in the user's browser. The frontend sends requests to an API layer, which is also deployed as a Cloud Run service. Data that is read frequently is cached in and served from a Memorystore for Redis instance. Requests that can't be served from the in-memory Redis cache are sent by the API layer to a Cloud SQL database. Resource configuration This section describes the configuration of the Cloud Run, Memorystore, Cloud SQL, and networking resources that the solution deploys. If you're familiar with the Terraform configuration language, you can change some of these settings, as described later in this guide. To view the configuration settings, click the following subsections: Cloud Run services Memorystore for Redis instance Cloud SQL database Networking resources Cost For an estimate of the cost of the Google Cloud resources that the three-tier web app solution uses, see the precalculated estimate in the Google Cloud Pricing Calculator. Use the estimate as a starting point to calculate the cost of your deployment. You can modify the estimate to reflect any configuration changes that you plan to make for the resources that are used in the solution. The precalculated estimate is based on assumptions for certain factors, including the following: The Google Cloud locations where the resources are deployed. The amount of time that the resources are used.","{ ""id"": ""root"", ""children"": [ { ""id"": ""clients"", ""labels"": [ { ""text"": ""clients"" } ], ""children"": [ { ""id"": ""web_client"", ""labels"": [ { ""text"": ""Web Client"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mobile_client"", ""labels"": [ { ""text"": ""Mobile Client"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_project"", ""labels"": [ { ""text"": ""gcp_project"" } ], ""children"": [ { ""id"": ""application"", ""labels"": [ { ""text"": ""application"" } ], ""children"": [ { ""id"": ""frontend_service"", ""labels"": [ { ""text"": ""frontend_service"" } ], ""children"": [ { ""id"": ""cloud_run_frontend"", ""labels"": [ { ""text"": ""Cloud Run Frontend"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""api_layer"", ""labels"": [ { ""text"": ""api_layer"" } ], ""children"": [ { ""id"": ""cloud_run_api"", ""labels"": [ { ""text"": ""Cloud Run API"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""cache"", ""labels"": [ { ""text"": ""cache"" } ], ""children"": [ { ""id"": ""memorystore_redis"", ""labels"": [ { ""text"": ""Memorystore (Redis)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""vpc"", ""labels"": [ { ""text"": ""vpc"" } ], ""children"": [ { ""id"": ""cloud_sql"", ""labels"": [ { ""text"": ""Cloud SQL"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_front_api"", ""labels"": [ { ""text"": ""invoke"" } ] }, { ""id"": ""e_api_cache"", ""labels"": [ { ""text"": ""cache"" } ] }, { ""id"": ""e_api_db"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_cache_db"", ""labels"": [ { ""text"": ""peer"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_web_front"", ""labels"": [ { ""text"": ""HTTP request"" } ] }, { ""id"": ""e_mobile_front"", ""labels"": [ { ""text"": ""HTTP request"" } ] } ] }"
gcp,app-dev,app-architectures,https://cloud.google.com/architecture/web-serving-overview,"Website hosting bookmark_border Last reviewed 2024-06-11 UTC This article discusses how to host a website on Google Cloud. Google Cloud provides a robust, flexible, reliable, and scalable platform for serving websites. Google built Google Cloud by using the same infrastructure that Google uses to serve content from sites such as Google.com, YouTube, and Gmail. You can host your website's content by using the type and design of infrastructure that best suits your needs. You might find this article useful if you are: Knowledgeable about how to create a website and have deployed and run some web-hosting infrastructure before. Evaluating whether and how to migrate your site to Google Cloud. If you want to build a simple website, consider using Google Sites, a structured wiki- and web page–creation tool. For more information, visit Sites help. Note: You might find it helpful to read the Concepts page of the Google Cloud overview before reading this article. Some of those concepts are referenced in this article without further explanation. If you're already a bit familiar with Google Cloud, you can skip this step. Choosing an option If you're new to using Google Cloud, it's a reasonable approach to start by using the kind of technology you're already familiar with. For example, if you currently use hardware servers or virtual machines (VMs) to host your site, perhaps with another cloud provider or on your own hardware, Compute Engine provides a familiar paradigm for you. If you prefer serverless computing, then Cloud Run is probably a good option for you. If you use a platform-as-a-service (PaaS) offering, such as Heroku or Engine Yard, then App Engine can be the best place to start. After you become more familiar with Google Cloud, you can explore the richness of products and services that Google Cloud provides. For example, if you started by using Compute Engine, you might augment your site's capabilities by using Google Kubernetes Engine (GKE) or migrate some or all of the functionality to App Engine and Cloud Run. The following table summarizes your hosting options on Google Cloud: Option	Product	Data storage	Load balancing	Scalability	Logging and monitoring Static website Cloud Storage Firebase Hosting Cloud Storage bucket HTTP(S) optional Automatically Cloud Logging Cloud Monitoring Virtual machines	Compute Engine Cloud SQL, Cloud Storage, Firestore, and Bigtable, or you can use another external storage provider. Hard-disk-based persistent disks, called standard persistent disks, and solid-state persistent disks (SSD). HTTP(S) TCP Proxy SSL Proxy IPv6 termination Network Cross-region Internal Automatically with managed instance groups Cloud Logging Cloud Monitoring Monitoring console Containers	GKE	Similar to Compute Engine but interacts with persistent disks differently Network HTTP(S) Cluster autoscaler Cloud Logging Cloud Monitoring Monitoring console Serverless Cloud Run Google Cloud services such as Cloud SQL, Firestore, Cloud Storage, and accessible third-party databases HTTP(S) Managed by Google Managed by Google Cloud Logging Cloud Monitoring Monitoring console Managed platform App Engine Google Cloud services such as Cloud SQL, Firestore, Cloud Storage, and accessible third-party databases HTTP(S) Managed by Google Managed by Google Cloud Logging Cloud Monitoring Monitoring console This article can help you to understand the main technologies that you can use for web hosting on Google Cloud and give you a glimpse of how the technologies work. The article provides links to complete documentation, tutorials, and solutions articles that can help you build a deeper understanding, when you're ready. Understanding costs Because there are so many variables and each implementation is different, it's beyond the scope of this article to provide specific advice about costs. To understand Google's principles about how pricing works on Google Cloud, see the pricing page. To understand pricing for individual services, see the product pricing section. You can also use the pricing calculator to estimate what your Google Cloud usage might look like. You can provide details about the services you want to use and then see a pricing estimate. Setting up domain name services Usually, you will want to register a domain name for your site. You can use a public domain name registrar to register a unique name for your site. If you want complete control of your own domain name system (DNS), you can use Cloud DNS to serve as your DNS provider. The Cloud DNS documentation includes a quickstart to get you going. If you have an existing DNS provider that you want to use, you generally need to create a couple of records with that provider. For a domain name such as example.com, you create an A record with your DNS provider. For the www.example.com sub-domain, you create a CNAME record for www to point it to the example.com domain. The A record maps a hostname to an IP address. The CNAME record creates an alias for the A record. If your domain name registrar is also your DNS provider, that's probably all you need to do. If you use separate providers for registration and DNS, make sure that your domain name registrar has the correct name servers associated with your domain. After making your DNS changes, the record updates can take some time to propagate depending on your time-to-live (TTL) values in your zone. If this is a new hostname, the changes go into effect quickly because the DNS resolvers don't have cached previous values and can contact the DNS provider to get the necessary information to route requests. Hosting a static website The simplest way to serve website content over HTTP(S) is to host static web pages. Static web pages are served unchanged, as they were written, usually by using HTML. Using a static website is a good option if your site's pages rarely change after they have been published, such as blog posts or pages that are part of a small-business website. You can do a lot with static web pages, but if you need your site to have robust interactions with users through server-side code, you should consider the other options discussed in this article. Hosting a static website with Cloud Storage Note: Though Cloud Storage serves content over HTTPS, it doesn't support end-to-end HTTPS for custom domains. If you need end-to-end HTTPS serving, check out Firebase hosting in the next section. Alternatively, you can use HTTP(S) load balancing with Cloud Storage to serve content from a custom domain over HTTPS. To host a static site in Cloud Storage, you need to create a Cloud Storage bucket, upload the content, and test your new site. You can serve your data directly from storage.googleapis.com, or you can verify that you own your domain and use your domain name. You can create your static web pages however you choose. For example, you could hand-author pages by using HTML and CSS. You can use a static-site generator, such as Jekyll, Ghost, or Hugo, to create the content. With static-site generators, you create a static website by authoring in markdown, and providing templates and tools. Site generators generally provide a local web server that you can use to preview your content. After your static site is working, you can update the static pages by using any process you like. That process can be as straightforward as hand-copying an updated page to the bucket. You might choose to use a more automated approach, such as storing your content on GitHub and then using a webhook to run a script that updates the bucket. An even more advanced system might use a continuous-integration/continuous-delivery (CI/CD) tool, such as Jenkins, to update the content in the bucket. Jenkins has a Cloud Storage plugin that provides a Google Cloud Storage Uploader post-build step to publish build artifacts to Cloud Storage. If you have a web app that needs to serve static content or user-uploaded static media, using Cloud Storage can be a cost-effective and efficient way to host and serve this content, while reducing the amount of dynamic requests to your web app. Additionally, Cloud Storage can directly accept user-submitted content. This feature lets users upload large media files directly and securely without proxying through your servers. To get the best performance from your static website, see Best practices for Cloud Storage. For more information, see the following pages: Hosting a static website J is for Jenkins (blog post) Band Aid 30 on Google Cloud (blog post) Cloud Storage documentation Hosting a static website with Firebase Hosting Firebase Hosting provides fast and secure static hosting for your web app. With Firebase Hosting, you can deploy web apps and static content to a global content-delivery network (CDN) by using a single command. Here are some benefits you get when you use Firebase Hosting: Zero-configuration SSL is built into Firebase Hosting. Provisions SSL certificates on custom domains for free. All of your content is served over HTTPS. Your content is delivered to your users from CDN edges around the world. Using the Firebase CLI, you can get your app up and running in seconds. Use command-line tools to add deployment targets into your build process. You get release management features, such as atomic deployment of new assets, full versioning, and one-click rollbacks. Hosting offers a configuration useful for single-page apps and other sites that are more app-like. Hosting is built to be used seamlessly with other Firebase features. For more information, see the following pages: Firebase Hosting guide Get started with Firebase Hosting Using virtual machines with Compute Engine For infrastructure as a service (IaaS) use cases, Google Cloud provides Compute Engine. Compute Engine provides a robust computing infrastructure, but you must choose and configure the platform components that you want to use. With Compute Engine, it's your responsibility to configure, administer, and monitor the systems. Google ensures that resources are available, reliable, and ready for you to use, but it's up to you to provision and manage them. The advantage, here, is that you have complete control of the systems and unlimited flexibility. Use Compute Engine to design and deploy nearly any website-hosting system you want. You can use VMs, called instances, to build your app, much like you would if you had your own hardware infrastructure. Compute Engine offers a variety of machine types to customize your configuration to meet your needs and your budget. You can choose which operating systems, development stacks, languages, frameworks, services, and other software technologies you prefer. Setting up automatically with Google Cloud Marketplace The easiest way to deploy a complete web-hosting stack is by using Google Cloud Marketplace. With just a few clicks, you can deploy any of over 100 fully realized solutions with Google Click to Deploy or Bitnami. Cloud Marketplace For example, you can set up a LAMP stack or WordPress with Cloud Marketplace. The system deploys a complete, working software stack in just a few minutes on a single instance. Before you deploy, Cloud Marketplace shows you cost estimates for running the site, gives you clear information about which versions of the software components it installs for you, and lets you customize your configuration by changing component instance names, choosing the machine type, and choosing a disk size. After you deploy, you have complete control over the Compute Engine instances, their configurations, and the software. Setting up manually You can also create your infrastructure on Compute Engine manually, either building your configuration from scratch or building on a Google Cloud Marketplace deployment. For example, you might want to use a version of a software component not offered by Cloud Marketplace, or perhaps you prefer to install and configure everything on your own. Providing a complete framework and best practices for setting up a website is beyond the scope of this article. But from a high-level view, the technical side of setting up a web-hosting infrastructure on Compute Engine requires that you: Understand the requirements. If you're building a new website, make sure you understand the components you need, such as instances, storage needs, and networking infrastructure. If you're migrating your app from an existing solution, you probably already understand these requirements, but you need think through how your existing setup maps to Google Cloud services. Plan the design. Think through your architecture and write down your design. Be as explicit as you can. Create the components. The components that you might usually think of as physical assets, such as computers and network switches, are provided through services in Compute Engine. For example, if you want a computer, you have to create a Compute Engine instance. If you want a persistent hard disk drive, you create that, too. Infrastructure as code tools, such as Terraform, makes this an easy and repeatable process. Configure and customize. After you have the components you want, you need to configure them, install and configure software, and write and deploy any customization code that you require. You can replicate the configuration by running shell scripts, which helps to speed future deployments. Terraform helps here, too, by providing declarative, flexible configuration templates for automatic deployment of resources. You can also take advantage of IT automation tools such as Puppet and Chef. Deploy the assets. Presumably, you have web pages and images. Test. Verify that everything works as you expect. Deploy to production. Open up your site for the world to see and use. Storing data with Compute Engine Most websites need some kind of storage. You might need storage for a variety of reasons, such as saving files that your users upload, and of course the assets that your site uses. Google Cloud provides a variety of managed storage services, including: A SQL database in Cloud SQL, which is a fully managed relational database service for MySQL, PostgreSQL, and SQL Server. Two options for NoSQL data storage: Firestore and Bigtable. Memorystore, which is a fully managed in-memory data store service for Redis and Memcached. Consistent, scalable, large-capacity object storage in Cloud Storage. Cloud Storage comes in several classes: Standard provides maximum availability. Nearline provides a low-cost choice ideal for data accessed less than once a month. Coldline provides a low-cost choice ideal for data accessed less than once a quarter. Archive provides the lowest-cost choice for archiving, backup, and disaster recovery. Persistent disks on Compute Engine for use as primary storage for your instances. Compute Engine offers both hard-disk-based persistent disks, called standard persistent disks, and solid-state persistent disks (SSD). You can also choose to set up your preferred storage technology on Compute Engine by using persistent disks. For example, you can set up PostgreSQL as your SQL database or MongoDB as your NoSQL storage. To understand the full range and benefits of storage services on Google Cloud, see Choosing a storage option. Load balancing with Compute Engine For any website that operates at scale, using load-balancing technologies to distribute the workload among servers is often a requirement. You have a variety of options when architecting your load-balanced web servers on Compute Engine, including: HTTP(S) load balancing. Explains the fundamentals of using Cloud Load Balancing. Content-based load balancing. Demonstrates how to distribute traffic to different instances based on the incoming URL. Cross-region load balancing. Demonstrates configuring VM instances in different regions and using HTTP or HTTPS load balancing to distribute traffic across the regions. TCP Proxy load balancing. Demonstrates setting up global TCP Proxy load balancing for a service that exists in multiple regions. SSL Proxy load balancing. Demonstrates setting up global SSL Proxy load balancing for a service that exists in multiple regions. IPv6 termination for HTTP(S), SSL Proxy, and TCP Proxy load balancing. Explains IPv6 termination and the options for configuring load balancers to handle IPv6 requests. Network load balancing. Shows a basic scenario that sets up a layer 3 load balancing configuration to distribute HTTP traffic across healthy instances. Cross-region load balancing using Microsoft IIS backends. Shows how to use the Compute Engine load balancer to distribute traffic to Microsoft Internet Information Services (IIS) servers. Setting up internal load balancing You can set up a load balancer that distributes network traffic on a private network that isn't exposed to the internet. Internal load balancing is useful not only for intranet apps where all traffic remains on a private network, but also for complex web apps where a frontend sends requests to backend servers by using a private network. Load balancing deployment is flexible, and you can use Compute Engine with your existing solutions. For example, HTTP(S) load balancing using Nginx is one possible solution that you could use in place of the Compute Engine load balancer. Content distribution with Compute Engine Because response time is a fundamental metric for any website, using a CDN to lower latency and increase performance is often a requirement, especially for a site with global web traffic. Cloud CDN uses Google's globally distributed edge points of presence to deliver content from cache locations closest to users. Cloud CDN works with HTTP(S) load balancing. To serve content out of Compute Engine, Cloud Storage, or both from a single IP address, enable Cloud CDN for an HTTP(S) load balancer. Autoscaling with Compute Engine You can set up your architecture to add and remove servers as demand varies. This approach can help to ensure that your site performs well under peak load, while keeping costs under control during more-typical demand periods. Compute Engine provides an autoscaler that you can use for this purpose. Autoscaling is a feature of managed instance groups. A managed instance group is a pool of homogeneous virtual machine instances, created from a common instance template. An autoscaler adds or remove instances in a managed instance group. Although Compute Engine has both managed and unmanaged instance groups, you can only use managed instance groups with an autoscaler. For more information, see autoscaling on Compute Engine. For an in-depth look at what it takes to build a scalable and resilient web-app solution, see Building scalable and resilient web apps. Logging and monitoring with Compute Engine Google Cloud includes features that you can use to keep tabs on what's happening with your website. Cloud Logging collects and stores logs from apps and services on Google Cloud. You can view or export logs and integrate third-party logs by using a logging agent. Logging Cloud Monitoring provides dashboards and alerts for your site. You configure Monitoring with the Google Cloud console. You can review performance metrics for cloud services, virtual machines, and common open source servers such as MongoDB, Apache, Nginx, and Elasticsearch. You can use the Cloud Monitoring API to retrieve monitoring data and create custom metrics. Cloud Monitoring also provides uptime checks, which send requests to your websites to see if they respond. You can monitor a website's availability by deploying an alerting policy that creates an incident if the uptime check fails. Managing DevOps with Compute Engine For information about managing DevOps with Compute Engine, see Distributed load testing using Kubernetes. Using containers with GKE You might already be using containers, such as Docker containers. For web hosting, containers offer several advantages, including: Componentization. You can use containers to separate the various components of your web app. For example, suppose your site runs a web server and a database. You can run these components in separate containers, modifying and updating one component without affecting the other. As your app's design becomes more complex, containers are a good fit for a service-oriented architecture, including microservices. This kind of design supports scalability, among other goals. Portability. A container has everything it needs to run—your app and its dependencies are bundled together. You can run your containers on a variety of platforms, without worrying about the underlying system details. Rapid deployment. When it's time to deploy, your system is built from a set of definitions and images, so the parts can be deployed quickly, reliably, and automatically. Containers are typically small and deploy much more quickly compared to, for example, virtual machines. Container computing on Google Cloud offers even more advantages for web hosting, including: Orchestration. GKE is a managed service built on Kubernetes, the open source container-orchestration system introduced by Google. With GKE, your code runs in containers that are part of a cluster that is composed of Compute Engine instances. Instead of administering individual containers or creating and shutting down each container manually, you can automatically manage the cluster through GKE, which uses the configuration you define. Image registration. Artifact Registry provides private storage for Docker images on Google Cloud. You can access the registry through an HTTPS endpoint, so you can pull images from any machine, whether it's a Compute Engine instance or your own hardware. The registry service hosts your custom images in Cloud Storage under your Google Cloud project. This approach ensures by default that your custom images are only accessible by principals that have access to your project. Mobility. This means that you have the flexibility to move and combine workloads with other cloud providers, or mix cloud computing workloads with on-premises implementations to create a hybrid solution. Storing data with GKE Because GKE runs on Google Cloud and uses Compute Engine instances as nodes, your storage options have a lot in common with storage on Compute Engine. You can access Cloud SQL, Cloud Storage, Firestore, and Bigtable through their APIs, or you can use another external storage provider if you choose. However, GKE does interact with Compute Engine persistent disks in a different way than a normal Compute Engine instance would. A Compute Engine instance includes an attached disk. When you use Compute Engine, as long as the instance exists, the disk volume remains with the instance. You can even detach the disk and use it with a different instance. But in a container, on-disk files are ephemeral. When a container restarts, such as after a crash, the on-disk files are lost. Kubernetes solves this issue by using volume and Storage Class abstractions. One type of storage class is GCE PD. This means that you can use Compute Engine persistent disks with containers to keep your data files from being deleted when you use GKE. To understand the features and benefits of a volume, you should first understand a bit about pods. You can think of a pod as an app-specific logical host for one or more containers. A pod runs on a node instance. When containers are members of a pod, they can share several resources, including a set of shared storage volumes. These volumes enable data to survive container restarts and to be shared among the containers within the pod. Of course, you can use a single container and volume in a pod, too, but the pod is a required abstraction to logically connect these resources to each other. For an example, see the tutorial Using persistent disks with WordPress and MySQL. Load balancing with GKE Many large web-hosting architectures need to have multiple servers running that can share the traffic demands. Because you can create and manage multiple containers, nodes, and pods with GKE, it's a natural fit for a load-balanced web-hosting system. Using network load balancing The easiest way to create a load balancer in GKE is to use Compute Engine's network load balancing. Network load balancing can balance the load of your systems based on incoming internet protocol data, such as the address, port, and protocol type. Network load balancing uses forwarding rules. These rules point to target pools that list which instances are available to be used for load balancing. With network load balancing, you can load balance additional TCP/UDP-based protocols such as SMTP traffic, and your app can directly inspect the packets. You can deploy network load balancing simply by adding the type: LoadBalancer field to your service configuration file. Using HTTP(S) load balancing If you need more advanced load-balancing features, such as HTTPS load balancing, content-based load balancing, or cross-region load balancing, you can integrate your GKE service with Compute Engine's HTTP/HTTPS load balancing feature. Kubernetes provides the Ingress resource that encapsulates a collection of rules for routing external traffic to Kubernetes endpoints. In GKE, an Ingress resource handles provisioning and configuring the Compute Engine HTTP/HTTPS load balancer. For more information about using HTTP/HTTPS load balancing in GKE, see Setting up HTTP load balancing with Ingress. Scaling with GKE For automatic resizing of clusters, you can use the Cluster Autoscaler. This feature periodically checks whether there are any pods that are waiting for a node with free resources but aren't being scheduled. If such pods exist, then the autoscaler resizes the node pool if resizing would allow the waiting pods to be scheduled. Cluster Autoscaler also monitors the usage of all nodes. If a node isn't needed for an extended period of time, and all of its pods can be scheduled elsewhere, then the node is deleted. For more information about the Cluster Autoscaler, its limitations, and best practices, see the Cluster Autoscaler documentation. Logging and monitoring with GKE Like on Compute Engine, Logging and Monitoring provide your logging and monitoring services. Logging collects and stores logs from apps and services. You can view or export logs and integrate third-party logs by using a logging agent. Monitoring provides dashboards and alerts for your site. You configure Monitoring with the Google Cloud console. You can review performance metrics for cloud services, virtual machines, and common open source servers such as MongoDB, Apache, Nginx, and Elasticsearch. You can use the Monitoring API to retrieve monitoring data and create custom metrics. Managing DevOps with GKE When you use GKE, you're already getting many of the benefits most people think of when they think of DevOps. This is especially true when it comes to ease of packaging, deployment, and management. For your CI/CD workflow needs, you can take advantage of tools that are built for the cloud, such as Cloud Build, and Cloud Deploy, or popular tools such as Jenkins. For more information, see the following articles: Jenkins on GKE Setting up Jenkins on GKE Configuring Jenkins for GKE Building on a serverless platform with Cloud Run Google Cloud's serverless platform lets you write code your way without worrying about the underlying infrastructure. You can build full-stack serverless applications with Google Cloud’s storage, databases, machine learning, and more. For your containerized websites, you can also deploy them to Cloud Run in addition to using GKE. Cloud Run is a fully managed serverless platform that lets you run highly scalable containerized applications on Google Cloud. You only pay for the time that your code runs. Using containers with Cloud Run, you can take advantage of mature technologies such as Nginx, Express.js, and Django to build your websites, access your SQL database on Cloud SQL, and render dynamic HTML pages. The Cloud Run documentation includes a quickstart to get you going. Storing data with Cloud Run Cloud Run containers are ephemeral and you need to understand their quotas and limits for your use cases. Files can be temporarily stored for processing in a container instance, but this storage comes out of the available memory for the service as described in the runtime contract. For persistent storage, similar to App Engine, you can choose Google Cloud's services such as Cloud Storage, Firestore or Cloud SQL. Alternatively, you can also use a third-party storage solution. Load balancing and autoscaling with Cloud Run By default, when you build on Cloud Run, it automatically routes incoming requests to appropriate back-end containers and do load balancing for you. However, if you want to take advantage of Google Cloud’s fully featured enterprise-grade HTTP(S) load balancing capabilities, you can use serverless network endpoint groups. With HTTP(S) load balancing, you can enable Cloud CDN or serve traffic from multiple regions. In addition, you can use middleware such as API Gateway to enhance your service. For Cloud Run, Google Cloud manages container instance autoscaling for you. Each revision is automatically scaled to the number of container instances needed to handle all incoming requests. When a revision doesn't receive any traffic, by default it's scaled to zero container instances. However, if desired, you can change this default to specify an instance to be kept idle or warm using the minimum instances setting. Logging and monitoring with Cloud Run Cloud Run has two types of logs, which are automatically sent to Cloud Logging: Request logs: logs of requests sent to Cloud Run services. These logs are created automatically. Container logs: logs emitted from the container instances, typically from your own code, written to supported locations as described in Writing container logs. You can view logs for your service in a couple of ways: Use the Cloud Run page in the Google Cloud console. Use Cloud Logging Logs Explorer in the Google Cloud console. Both of these viewing methods examine the same logs stored in Cloud Logging, but the Logs Explorer provides more details and more filtering capabilities. Cloud Monitoring provides Cloud Run performance monitoring, metrics, and uptime checks, along with alerts to send notifications when certain metric thresholds are exceeded. Google Cloud Observability pricing applies, which means there is no charge for metrics on the fully managed version of Cloud Run. Note that you can also use Cloud Monitoring custom metrics. Cloud Run is integrated with Cloud Monitoring with no setup or configuration required. This means that metrics of your Cloud Run services are automatically captured when they are running. Building on a managed platform with App Engine On Google Cloud, the managed platform as a service (PaaS) is called App Engine. When you build your website on App Engine, you get to focus on coding up your features and let Google worry about managing the supporting infrastructure. App Engine provides a wide range of features that make scalability, load balancing, logging, monitoring, and security much easier than if you had to build and manage them yourself. App Engine lets you code in a variety of programming languages, and it can use a variety of other Google Cloud services. App Engine provides the standard environment, which lets you run apps in a secure, sandboxed environment. The App Engine standard environment distributes requests across multiple servers, and scales servers to meet traffic demands. Your app runs in its own secure, reliable environment that's independent of the hardware, operating system, or physical location of the server. Web app uses App Engine and other components To give you more options, App Engine offers the flexible environment. When you use the flexible environment, your app runs on configurable Compute Engine instances, but App Engine manages the hosting environment for you. This means that you can use additional runtimes, including custom runtimes, for more programming language choices. You can also take advantage of some of the flexibility that Compute Engine offers, such as choosing from a variety of CPU and memory options. Programming languages The App Engine standard environment provides default runtimes, and you write source code in specific versions of the supported programming languages. With the flexible environment, you write source code in a version of any of the supported programming languages. You can customize these runtimes or provide your own runtime with a custom Docker image or Dockerfile. If the programming language you use is a primary concern, you need to decide whether the runtimes provided by the App Engine standard environment meet your requirements. If they don't, you should consider using the flexible environment. To determine which environment best meets your app's needs, see Choosing an App Engine environment. Getting started tutorials by language The following tutorials can help you get started using the App Engine standard environment: Hello World in Python Hello World in Java Hello World in PHP Hello World in Ruby Hello World in Go Hello World in Node.js The following tutorials can help you get started using the flexible environment: Getting started with Python Getting started with Java Getting started with PHP Getting started with Go Getting started with Node.js Getting started with Ruby Getting started with .NET Storing data with App Engine App Engine gives you options for storing your data: Name	Structure	Consistency Firestore	Schemaless	Strongly consistent. Cloud SQL	Relational	Strongly consistent. Cloud Storage	Files and their associated metadata	Strongly consistent except when performing list operations that get a list of buckets or objects. You can also use several third-party databases with the standard environment. For more details about storage in App Engine, see Choosing a storage option, and then select your preferred programming language. When you use the flexible environment, you can use all of the same storage options as you can with the standard environment, and a wider range of third-party databases as well. For more information about third-party databases in the flexible environment, see Using third-party databases. Load balancing and autoscaling with App Engine By default, App Engine automatically routes incoming requests to appropriate backend instances and does load balancing for you. However, if you want to take advantage of Google Cloud’s fully featured enterprise-grade HTTP(S) load balancing capabilities, you can use serverless network endpoint groups. For scaling, App Engine can automatically create and shut down instances as traffic fluctuates, or you can specify a number of instances to run regardless of the amount of traffic. Logging and monitoring with App Engine In App Engine, requests are logged automatically, and you can view these logs in the Google Cloud console. App Engine also works with standard, language-specific libraries that provide logging functionality and forwards the log entries to the logs in the Google Cloud console. For example, in Python you can use the standard Python logging module and in Java you can integrate the logback appender or java.util.logging with Cloud Logging. This approach enables the full features of Cloud Logging and requires only a few lines of Google Cloud-specific code. Cloud Monitoring provides features for monitoring your App Engine apps. Through the Google Cloud console, you can monitor incidents, uptime checks, and other details. Building content management systems Hosting a website means managing your website assets. Cloud Storage provides a global repository for these assets. One common architecture deploys static content to Cloud Storage and then syncs to Compute Engine to render dynamic pages. Cloud Storage works with many third-party content management systems, such as WordPress, Drupal, and Joomla. Cloud Storage also offers an Amazon S3 compatible API, so any system that works with Amazon S3 can work with Cloud Storage. The diagram below is a sample architecture for a content management system.","{ ""id"": ""root"", ""children"": [ { ""id"": ""end_user"", ""labels"": [ { ""text"": ""Users"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gcp_platform"", ""labels"": [ { ""text"": ""gcp_platform"" } ], ""children"": [ { ""id"": ""serving_static_content"", ""labels"": [ { ""text"": ""serving_static_content"" } ], ""children"": [ { ""id"": ""cloud_cdn"", ""labels"": [ { ""text"": ""Cloud CDN"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""app_engine"", ""labels"": [ { ""text"": ""App Engine"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_storage_static"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_cdn_to_app"", ""labels"": [ { ""text"": ""forwards"" } ] } ] }, { ""id"": ""serving_dynamic_content"", ""labels"": [ { ""text"": ""serving_dynamic_content"" } ], ""children"": [ { ""id"": ""memcache"", ""labels"": [ { ""text"": ""Memcache"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""task_queues"", ""labels"": [ { ""text"": ""Task Queues"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""datastore"", ""labels"": [ { ""text"": ""Datastore"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_taskqueue_to_datastore"", ""labels"": [ { ""text"": ""persist data"" } ] } ] }, { ""id"": ""log_processing"", ""labels"": [ { ""text"": ""log_processing"" } ], ""children"": [ { ""id"": ""pubsub"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataflow"", ""labels"": [ { ""text"": ""Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_storage_logs"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_pubsub_to_dataflow"", ""labels"": [ { ""text"": ""stream"" } ] }, { ""id"": ""e_dataflow_to_logs_storage"", ""labels"": [ { ""text"": ""store"" } ] }, { ""id"": ""e_logs_to_bigquery"", ""labels"": [ { ""text"": ""load"" } ] } ] }, { ""id"": ""monitoring_services"", ""labels"": [ { ""text"": ""monitoring_services"" } ], ""children"": [ { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_app_to_memcache"", ""labels"": [ { ""text"": ""cache queries"" } ] }, { ""id"": ""e_app_to_taskqueues"", ""labels"": [ { ""text"": ""enqueue tasks"" } ] }, { ""id"": ""e_app_to_pubsub"", ""labels"": [ { ""text"": ""publish logs"" } ] }, { ""id"": ""e_dataflow_to_logging"", ""labels"": [ { ""text"": ""write logs"" } ] }, { ""id"": ""e_dataflow_to_monitoring"", ""labels"": [ { ""text"": ""export metrics"" } ] }, { ""id"": ""e_storage_to_logging"", ""labels"": [ { ""text"": ""store logs"" } ] }, { ""id"": ""e_bigquery_to_monitoring"", ""labels"": [ { ""text"": ""analyze"" } ] } ] } ], ""edges"": [ { ""id"": ""e_user_to_cdn"", ""labels"": [ { ""text"": ""requests"" } ] }, { ""id"": ""e_user_to_app"", ""labels"": [ { ""text"": ""requests"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/big-data-analytics/analytics-lakehouse,"Jump Start Solution: Analytics lakehouse bookmark_border Release Notes Last reviewed 2025-01-13 UTC This guide helps you understand, deploy, and use the Analytics lakehouse Jump Start Solution. This solution demonstrates how you can unify data lakes and data warehouses by creating an analytics lakehouse to store, process, analyze, and activate data using a unified data stack. Common use cases for building an analytics lakehouse include the following: Large scale analysis of telemetry data combined with reporting data. Unifying structured and unstructured data analysis. Providing real-time analytics capabilities for a data warehouse. This document is intended for developers who have some background with data analysis and have used a database or data lake to perform an analysis. It assumes that you're familiar with basic cloud concepts, though not necessarily Google Cloud. Experience with Terraform is helpful. Note: This solution helps you explore the capabilities of Google Cloud. The solution is not intended to be used as is for production environments. For information about designing and setting up production-grade environments in Google Cloud, see Landing zone design in Google Cloud and Google Cloud setup checklist. Objectives Learn how to set up an analytics lakehouse. Secure an analytics lakehouse using a common governance layer. Build dashboards from the data to perform data analysis. Create a machine learning model to predict data values over time. Products used The solution uses the following Google Cloud products: BigQuery: A fully managed, highly scalable data warehouse with built-in machine learning capabilities. Dataproc: A fully managed service for data lake modernization, ETL, and secure data science, at scale. Looker Studio: Self-service business intelligence platform that helps you create and share data insights. Dataplex Universal Catalog: Centrally discover, manage, monitor, and govern data at scale. Cloud Storage: An enterprise-ready service that provides low-cost, no-limit object storage for diverse data types. Data is accessible from within and outside of Google Cloud and is replicated geo-redundantly. BigLake: BigLake is a storage engine that unifies data warehouses and lakes by enabling BigQuery and open source frameworks like Spark to access data with fine-grained access control. The following Google Cloud products are used to stage data in the solution for first use: Workflows: A fully managed orchestration platform that executes services in a specified order as a workflow. Workflows can combine services, including custom services hosted on Cloud Run or Cloud Run functions, Google Cloud services such as BigQuery, and any HTTP-based API. Architecture The example lakehouse architecture that this solution deploys analyzes an ecommerce dataset to understand a retailer's performance over time. The following diagram shows the architecture of the Google Cloud resources that the solution deploys. Architecture of the infrastructure for the data warehouse solution. Solution flow The architecture represents a common data flow to populate and transform data in an analytics lakehouse architecture: Data lands in Cloud Storage buckets. A data lake is created in Dataplex Universal Catalog. Data in the buckets are organized into entities, or tables, in the data lake. Tables in the data lake are immediately available in BigQuery as BigLake: tables. Data transformations using Dataproc or BigQuery, and using open file formats including Apache Iceberg. Data can be secured using policy tags and row access policies. Machine learning can be applied on the tables. Dashboards are created from the data to perform more analytics by using Looker Studio. Cost For an estimate of the cost of the Google Cloud resources that the analytics lakehouse solution uses, see the precalculated estimate in the Google Cloud Pricing Calculator. Use the estimate as a starting point to calculate the cost of your deployment. You can modify the estimate to reflect any configuration changes that you plan to make for the resources that are used in the solution. The precalculated estimate is based on assumptions for certain factors, including the following: The Google Cloud locations where the resources are deployed. The amount of time that the resources are used.","{ ""id"": ""root"", ""children"": [ { ""id"": ""data_sources"", ""labels"": [ { ""text"": ""data_sources"" } ], ""children"": [ { ""id"": ""data_source1"", ""labels"": [ { ""text"": ""Data Source 1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_source2"", ""labels"": [ { ""text"": ""Data Source 2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_source3"", ""labels"": [ { ""text"": ""Data Source 3"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_project"", ""labels"": [ { ""text"": ""gcp_project"" } ], ""children"": [ { ""id"": ""cloud_storage"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""analytics_components"", ""labels"": [ { ""text"": ""analytics_components"" } ], ""children"": [ { ""id"": ""lake_admin"", ""labels"": [ { ""text"": ""lake_admin"" } ], ""children"": [ { ""id"": ""dataplex_admin"", ""labels"": [ { ""text"": ""Dataplex Lake Admin"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery_biglake"", ""labels"": [ { ""text"": ""BigQuery BigLake Tables"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataplex_quality"", ""labels"": [ { ""text"": ""Dataplex Quality Rules"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_admin_manage_biglake"", ""labels"": [ { ""text"": ""manages"" } ] }, { ""id"": ""e_biglake_quality"", ""labels"": [ { ""text"": ""feeds"" } ] } ] }, { ""id"": ""data_processing"", ""labels"": [ { ""text"": ""data_processing"" } ], ""children"": [ { ""id"": ""dataproc_serverless"", ""labels"": [ { ""text"": ""Dataproc Serverless"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""machine_learning"", ""labels"": [ { ""text"": ""machine_learning"" } ], ""children"": [ { ""id"": ""vertex_ai"", ""labels"": [ { ""text"": ""Vertex AI"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""visualization"", ""labels"": [ { ""text"": ""visualization"" } ], ""children"": [ { ""id"": ""looker_studio"", ""labels"": [ { ""text"": ""Looker Studio"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_biglake_process_dataproc"", ""labels"": [ { ""text"": ""processes"" } ] }, { ""id"": ""e_biglake_train_vertex"", ""labels"": [ { ""text"": ""trains"" } ] }, { ""id"": ""e_biglake_visualizes"", ""labels"": [ { ""text"": ""analyzes"" } ] } ] } ], ""edges"": [ { ""id"": ""e_storage_ingest_biglake"", ""labels"": [ { ""text"": ""ingests"" } ] } ] } ], ""edges"": [ { ""id"": ""e_datasources_storage"", ""labels"": [ { ""text"": ""uploads"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/blueprints/confidential-data-warehouse-blueprint,"Import data into a secured BigQuery data warehouse bookmark_border Last reviewed 2025-06-15 UTC Many organizations deploy data warehouses that store sensitive data so that they can analyze the data for a variety of business purposes. This document is intended for data engineers and security administrators who deploy and secure data warehouses using BigQuery. It's part of a blueprint that's made up of the following: Two GitHub repositories (terraform-google-secured-data-warehouse and terraform-google-secured-data-warehouse-onprem-ingest) that contain Terraform configurations and scripts. The Terraform configuration sets up an environment in Google Cloud that supports a data warehouse that stores confidential data. A guide to the architecture, design, and security controls of this blueprint (this document). A walkthrough that deploys a sample environment. This document discusses the following: The architecture and Google Cloud services that you can use to help secure a data warehouse in a production environment. Best practices for importing data into BigQuery from an external network such as an on-premises environment. Best practices for data governance when creating, deploying, and operating a data warehouse in Google Cloud, including the following: data de-identification differential handling of confidential data column-level encryption column-level access controls This document assumes that you have already configured a foundational set of security controls as described in the enterprise foundations blueprint. It helps you to layer additional controls onto your existing security controls to help protect confidential data in a data warehouse. Data warehouse use cases The blueprint supports the following use cases: Use the terraform-google-secured-data-warehouse repository to import data from Google Cloud into a BigQuery data warehouse Use the terraform-google-secured-data-warehouse-onprem-ingest repository to import data from an on-premises environment or another cloud into a BigQuery data warehouse Overview Data warehouses such as BigQuery let businesses analyze their business data for insights. Analysts access the business data that is stored in data warehouses to create insights. If your data warehouse includes confidential data, you must take measures to preserve the security, confidentiality, integrity, and availability of the business data while it is stored, while it is in transit, or while it is being analyzed. In this blueprint, you do the following: When importing data from external data sources, encrypt your data that's located outside of Google Cloud (for example, in an on-premises environment) and import it into Google Cloud. Configure controls that help secure access to confidential data. Configure controls that help secure the data pipeline. Configure an appropriate separation of duties for different personas. When importing data from other sources located in Google Cloud (also known as internal data sources), set up templates to find and de-identify confidential data. Set up appropriate security controls and logging to help protect confidential data. Use data classification, policy tags, dynamic data masking, and column-level encryption to restrict access to specific columns in the data warehouse. Architecture To create a confidential data warehouse, you need to import data securely and then store the data in a VPC Service Controls perimeter. Architecture when importing data from Google Cloud The following image shows how ingested data is categorized, de-identified, and stored when you import source data from Google Cloud using the terraform-google-secured-data-warehouse repository. It also shows how you can re-identify confidential data on demand for analysis. The sensitive data warehouse architecture for internal sources. Architecture when importing data from external sources The following image shows how data is ingested and stored when you import data from an on-premises environment or another cloud into a BigQuery warehouse using the terraform-google-secured-data-warehouse-onprem-ingest repository. The sensitive data warehouse architecture for external networks. Google Cloud services and features The architectures use a combination of the following Google Cloud services and features: Service or feature	Description BigQuery Applicable to both internal and external data sources. However, different storage options exist, as follows: When importing data from Google Cloud, BigQuery stores the confidential data in the confidential data perimeter. When importing data from an external source, BigQuery stores the encrypted data and the wrapped encryption key in separate tables. BigQuery uses various security controls to help protect content, including access controls, column-level security for confidential data, and data encryption. Cloud Key Management Service (Cloud KMS) with Cloud HSM Applicable to both internal and external sources. However, an additional use case for external data sources exists. Cloud HSM is a cloud-based hardware security module (HSM) service that hosts the key encryption key (KEK). When importing data from an external source, you use Cloud HSM to generate the encryption key that you use to encrypt the data in your network before sending it to Google Cloud. Cloud Logging Applicable to both internal and external sources. Cloud Logging collects all the logs from Google Cloud services for storage and retrieval by your analysis and investigation tools. Cloud Monitoring Applicable to both internal and external sources. Cloud Monitoring collects and stores performance information and metrics about Google Cloud services. Cloud Run functions Applicable for external data sources only. Cloud Run functions is triggered by Cloud Storage and writes the data that Cloud Storage uploads to the ingestion bucket into BigQuery. Cloud Storage and Pub/Sub Applicable to both internal and external sources. Cloud Storage and Pub/Sub receive data as follows: Cloud Storage: receives and stores batch data. By default, Cloud Storage uses TLS to encrypt data in transit and AES-256 to encrypt data in storage. The encryption key is a customer-managed encryption key (CMEK). For more information about encryption, see Data encryption options. You can help to secure access to Cloud Storage buckets using security controls such as Identity and Access Management, access control lists (ACLs), and policy documents. For more information about supported access controls, see Overview of access control. Pub/Sub: receives and stores streaming data before de-identification. Pub/Sub uses authentication, access controls, and message-level encryption with a CMEK to protect your data. Data Profiler for BigQuery Applicable to both internal and external sources. Data Profiler for BigQuery automatically scans for sensitive data in all BigQuery tables and columns across the entire organization, including all folders and projects. Dataflow pipelines Applicable to both internal and external sources; however, different pipelines exist. Dataflow pipelines import data, as follows: When importing data from Google Cloud, two Dataflow pipelines de-identify and re-identify confidential data. The first pipeline de-identifies confidential data using pseudonymization. The second pipeline re-identifies confidential data when authorized users require access. When importing data from an external source, one Dataflow pipeline writes streaming data into BigQuery. Dataplex Universal Catalog Applicable to both internal and external sources. Dataplex Universal Catalog automatically categorizes confidential data with metadata, also known as policy tags, during ingestion. Dataplex Universal Catalog also uses metadata to manage access to confidential data. To control access to data within the data warehouse, you apply policy tags to columns that include confidential data. Dedicated Interconnect Applicable for external data sources only. Dedicated Interconnect lets you move data between your network and Google Cloud. You can use another connectivity option, as described in Choosing a Network Connectivity product. IAM and Resource Manager Applicable to both internal and external sources. Identity and Access Management (IAM) and Resource Manager restrict access and segment resources. The access controls and resource hierarchy follow the principle of least privilege. Security Command Center Applicable to both internal and external sources. Security Command Center monitors and reviews security findings from across your Google Cloud environment in a central location. Sensitive Data Protection Applicable to both internal and external sources; however, different scans occur. Sensitive Data Protection scans data, as follows: When importing data from Google Cloud, Sensitive Data Protection de-identifies confidential data during ingestion. Sensitive Data Protection de-identifies structured and unstructured data based on the infoTypes or records that are detected. When importing data from an external source, Sensitive Data Protection scans data that is stored in BigQuery to find any sensitive data that isn't protected. For more information, see Using Sensitive Data Protection to scan BigQuery data. VPC Service Controls Applicable to both internal and external sources; however, different perimeters exist. VPC Service Controls creates security perimeters that isolate services and resources by setting up authorization, access controls, and secure data exchange. The perimeters are as follows: A data ingestion perimeter accepts incoming data (in batch or stream) and de-identifies it. A separate landing zone helps to protect the rest of your workloads from incoming data. When importing data from Google Cloud, a confidential data perimeter can re-identify the confidential data and store it in a restricted area. When importing external data, a data perimeter isolates the encryption data from other workloads. A governance perimeter stores the encryption keys and defines what is considered confidential data. These perimeters are designed to protect incoming content, isolate confidential data by setting up additional access controls and monitoring, and separate your governance from the actual data in the warehouse. Your governance includes key management, data catalog management, and logging.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_cloud"", ""labels"": [ { ""text"": ""gcp_cloud"" } ], ""children"": [ { ""id"": ""interconnect_project"", ""labels"": [ { ""text"": ""interconnect_project"" } ], ""children"": [ { ""id"": ""dedicated_interconnect"", ""labels"": [ { ""text"": ""Dedicated Interconnect"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_projects_group"", ""labels"": [ { ""text"": ""data_projects_group"" } ], ""children"": [ { ""id"": ""data_ingestion_project"", ""labels"": [ { ""text"": ""data_ingestion_project"" } ], ""children"": [ { ""id"": ""cloud_storage_ingestion"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_functions_ingestion"", ""labels"": [ { ""text"": ""Cloud Functions"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_ingestion"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataflow_ingestion"", ""labels"": [ { ""text"": ""Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_storage_cfunctions"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_pubsub_dataflow"", ""labels"": [ { ""text"": ""streams"" } ] } ] }, { ""id"": ""data_project"", ""labels"": [ { ""text"": ""data_project"" } ], ""children"": [ { ""id"": ""bigquery_data_project"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""looker_studio_data_project"", ""labels"": [ { ""text"": ""Looker Studio"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""vertex_ai_notebooks"", ""labels"": [ { ""text"": ""Vertex AI Notebooks"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_bigquery_looker"", ""labels"": [ { ""text"": ""visualizes"" } ] }, { ""id"": ""e_bigquery_vertex"", ""labels"": [ { ""text"": ""analyses"" } ] } ] } ], ""edges"": [ { ""id"": ""e_cfunctions_bigquery"", ""labels"": [ { ""text"": ""loads"" } ] }, { ""id"": ""e_dataflow_bigquery"", ""labels"": [ { ""text"": ""loads"" } ] }, { ""id"": ""e_storage_bigquery"", ""labels"": [ { ""text"": ""loads"" } ] } ] }, { ""id"": ""data_governance_project"", ""labels"": [ { ""text"": ""data_governance_project"" } ], ""children"": [ { ""id"": ""governance_services"", ""labels"": [ { ""text"": ""governance_services"" } ], ""children"": [ { ""id"": ""kms"", ""labels"": [ { ""text"": ""Cloud KMS"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dlp"", ""labels"": [ { ""text"": ""Cloud DLP"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog"", ""labels"": [ { ""text"": ""Data Catalog"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""scc"", ""labels"": [ { ""text"": ""Security Command Center"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_dlp_catalog"", ""labels"": [ { ""text"": ""findings"" } ] } ] }, { ""id"": ""governance_storage"", ""labels"": [ { ""text"": ""governance_storage"" } ], ""children"": [ { ""id"": ""cloud_storage_dgov"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_logging_dgov"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_storage_logging"", ""labels"": [ { ""text"": ""logs"" } ] } ] }, { ""id"": ""governance_analytics"", ""labels"": [ { ""text"": ""governance_analytics"" } ], ""children"": [ { ""id"": ""bigquery_dgov"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""looker_studio_dgov"", ""labels"": [ { ""text"": ""Looker Studio"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_bigquerydgov_looker"", ""labels"": [ { ""text"": ""visualizes"" } ] } ] } ], ""edges"": [ { ""id"": ""e_scc_storage"", ""labels"": [ { ""text"": ""writes findings"" } ] }, { ""id"": ""e_logging_bigquerydgov"", ""labels"": [ { ""text"": ""exports"" } ] }, { ""id"": ""e_catalog_bigquerydgov"", ""labels"": [ { ""text"": ""catalog"" } ] } ] } ], ""edges"": [ { ""id"": ""e_interconnect_storage"", ""labels"": [ { ""text"": ""batch transfer"" } ] }, { ""id"": ""e_interconnect_pubsub"", ""labels"": [ { ""text"": ""stream transfer"" } ] }, { ""id"": ""e_catalog_bigquerydataproj"", ""labels"": [ { ""text"": ""profiles"" } ] } ] }, { ""id"": ""external_sources"", ""labels"": [ { ""text"": ""external_sources"" } ], ""children"": [ { ""id"": ""ext_storage"", ""labels"": [ { ""text"": ""Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""encryption_engine"", ""labels"": [ { ""text"": ""Encryption Engine"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""etl_pipeline"", ""labels"": [ { ""text"": ""ETL Pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gateway"", ""labels"": [ { ""text"": ""Gateway"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_storage_encrypt"", ""labels"": [ { ""text"": ""encrypts"" } ] }, { ""id"": ""e_encrypt_etl"", ""labels"": [ { ""text"": ""processes"" } ] }, { ""id"": ""e_etl_gateway"", ""labels"": [ { ""text"": ""deploys"" } ] } ] } ], ""edges"": [ { ""id"": ""e_gateway_interconnect"", ""labels"": [ { ""text"": ""transfers"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/blueprints/confidential-data-warehouse-blueprint,"Organization structure You group your organization's resources so that you can manage them and separate your testing environments from your production environment. Resource Manager lets you logically group resources by project, folder, and organization. The following diagrams show you a resource hierarchy with folders that represent different environments such as bootstrap, common, production, non-production (or staging), and development. You deploy most of the projects in the architecture into the production folder, and the data governance project in the common folder which is used for governance. Organization structure when importing data from Google Cloud The following diagram shows the organization structure when importing data from Google Cloud using the terraform-google-secured-data-warehouse repository. The resource hierarchy for a sensitive data warehouse for internal sources. Organization structure when importing data from external sources The following diagram shows the organization structure when importing data from an external source using the terraform-google-secured-data-warehouse-onprem-ingest repository. The resource hierarchy for a sensitive data warehouse for external sources. Folders You use folders to isolate your production environment and governance services from your non-production and testing environments. The following table describes the folders from the enterprise foundations blueprint that are used by this architecture. Folder	Description Bootstrap Contains resources required to deploy the enterprise foundations blueprint. Common Contains centralized services for the organization, such as the Data governance project. Production Contains projects that have cloud resources that have been tested and are ready to use. In this architecture, the Production folder contains the Data ingestion project and data-related projects. Non-production Contains projects that have cloud resources that are being tested and staged for release. In this architecture, the Non-production folder contains the Data ingestion project and data-related projects. Development Contains projects that have cloud resources that are being developed. In this architecture, the Development folder contains the Data ingestion project and data-related projects. You can change the names of these folders to align with your organization's folder structure, but we recommend that you maintain a similar structure. For more information, see the enterprise foundations blueprint. Projects You isolate parts of your environment using projects. The following table describes the projects that are needed within the organization. You create these projects when you run the Terraform code. You can change the names of these projects, but we recommend that you maintain a similar project structure. Project	Description Data ingestion Common project for both internal and external sources. Contains services that are required in order to receive data and de-identify confidential data. Data governance Common project for both internal and external sources. Contains services that provide key management, logging, and data cataloging capabilities. Non-confidential data Project for internal sources only. Contains services that are required in order to store data that has been de-identified. Confidential data Project for internal sources only. Contains services that are required in order to store and re-identify confidential data. Data Project for external sources only. Contains services that are required to store data. In addition to these projects, your environment must also include a project that hosts a Dataflow Flex Template job. The Flex Template job is required for the streaming data pipeline. Mapping roles and groups to projects You must give different user groups in your organization access to the projects that make up the confidential data warehouse. The following sections describe the architecture recommendations for user groups and role assignments in the projects that you create. You can customize the groups to match your organization's existing structure, but we recommend that you maintain a similar segregation of duties and role assignment. Data analyst group Data analysts analyze the data in the warehouse. In the terraform-google-secured-data-warehouse-onprem-ingest repository, this group can view data after it has been loaded into the data warehouse and perform the same operations as the Encrypted data viewer group. The following table describes the group's roles in different projects for the terraform-google-secured-data-warehouse repository (internal data sources only). Project mapping	Roles Data ingestion Dataflow Developer (roles/dataflow.developer) Dataflow Viewer (roles/dataflow.viewer) Logs Viewer (roles/logging.viewer) Additional role for data analysts that require access to confidential data: Fine-Grained Reader (roles/datacatalog.categoryFineGrainedReader) Confidential data BigQuery Data Viewer (roles/bigquery.dataViewer) BigQuery Job User (roles/bigquery.jobUser) BigQuery User (roles/bigquery.user) Dataflow Developer (roles/dataflow.developer) Dataflow Viewer (roles/dataflow.viewer) Logs Viewer (roles/logging.viewer) Non-confidential data BigQuery Data Viewer (roles/bigquery.dataViewer) BigQuery Job User (roles/bigquery.jobUser) BigQuery User (roles/bigquery.user) Logs Viewer (roles/logging.viewer) The following table describes the group's roles in different projects for the terraform-google-secured-data-warehouse-onprem-ingest repository (external data sources only). Scope of assignment	Roles Data ingestion project Dataflow Developer (roles/dataflow.developer) Dataflow Viewer (roles/dataflow.viewer) Logs Viewer (roles/logging.viewer) Data project BigQuery Data Viewer (roles/bigquery.dataViewer) BigQuery Job User (roles/bigquery.jobUser) BigQuery User (roles/bigquery.user) Dataflow Developer (roles/dataflow.developer) Dataflow Viewer (roles/dataflow.viewer) DLP Administrator (roles/dlp.admin) Logs Viewer (roles/logging.viewer) Data policy level Masked Reader (roles/bigquerydatapolicy.maskedReader) Encrypted data viewer group (external sources only) The Encrypted data viewer group in the terraform-google-secured-data-warehouse-onprem-ingest repository can view encrypted data from BigQuery reporting tables through Looker Studio and other reporting tools, such as SAP Business Objects. The encrypted data viewer group can't view cleartext data from encrypted columns. This group requires the BigQuery User (roles/bigquery.jobUser) role in the Data project. This group also requires the Masked Reader (roles/bigquerydatapolicy.maskedReader) role at the data policy level. Plaintext reader group (external sources only) The Plaintext reader group in the terraform-google-secured-data-warehouse-onprem-ingest repository has the required permission to call the decryption user-defined function (UDF) to view plaintext data and the additional permission to read unmasked data. This group requires the following roles in the Data project: BigQuery User (roles/bigquery.user) BigQuery User (roles/bigquery.jobUser) Cloud KMS Viewer (roles/cloudkms.viewer) In addition, this group requires the Fine-Grained Reader (roles/datacatalog.categoryFineGrainedReader) role at the Dataplex Universal Catalog level. Data engineer group Data engineers set up and maintain the data pipeline and warehouse. The following table describes the group's roles in different projects for the terraform-google-secured-data-warehouse repository. Score of assignment	Roles Data ingestion project Cloud Build Editor (roles/cloudbuild.builds.editor) Cloud KMS Viewer (roles/cloudkms.viewer) Composer User (roles/composer.user) Compute Network User (roles/compute.networkUser) Dataflow Admin (roles/dataflow.admin) Logs Viewer (roles/logging.viewer) Confidential data project BigQuery Data Editor (roles/bigquery.dataEditor) BigQuery Job User (roles/bigquery.jobUser) Cloud Build Editor (roles/cloudbuild.builds.editor) Cloud KMS Viewer (roles/cloudkms.viewer) Compute Network User (roles/compute.networkUser) Dataflow Admin (roles/dataflow.admin) Logs Viewer (roles/logging.viewer) Non-confidential data project BigQuery Data Editor (roles/bigquery.dataEditor) BigQuery Job User (roles/bigquery.jobUser) Cloud KMS Viewer (roles/cloudkms.viewer) Logs Viewer (roles/logging.viewer) The following table describes the group's roles in different projects for the terraform-google-secured-data-warehouse-onprem-ingest repository. Scope of assignment	Roles Data ingestion project Cloud Build Editor (roles/cloudbuild.builds.editor) Cloud KMS Viewer (roles/cloudkms.viewer) Composer User (roles/composer.user) Compute Network User (roles/compute.networkUser) Dataflow Admin (roles/dataflow.admin) Logs Viewer (roles/logging.viewer) Data project BigQuery Data Editor (roles/bigquery.dataEditor) BigQuery Job User (roles/bigquery.jobUser) Cloud Build Editor (roles/cloudbuild.builds.editor) Cloud KMS Viewer (roles/cloudkms.viewer) Compute Network User (roles/compute.networkUser) Dataflow Admin (roles/dataflow.admin) DLP Administrator (roles/dlp.admin) Logs Viewer (roles/logging.viewer) Network administrator group Network administrators configure the network. Typically, they are members of the networking team. Network administrators require the following roles at the organization level: Compute Admin (roles/compute.networkAdmin) Logs Viewer (roles/logging.viewer) Security administrator group Security administrators administer security controls such as access, keys, firewall rules, VPC Service Controls, and the Security Command Center. Security administrators require the following roles at the organization level: Access Context Manager Admin (roles/accesscontextmanager.policyAdmin) Cloud Asset Viewer (roles/cloudasset.viewer) Cloud KMS Admin (roles/cloudkms.admin) Compute Security Admin (roles/compute.securityAdmin) Data Catalog Admin (roles/datacatalog.admin) DLP Administrator (roles/dlp.admin) Logging Admin (roles/logging.admin) Organization Administrator (roles/orgpolicy.policyAdmin) Security Admin (roles/iam.securityAdmin) Security analyst group Security analysts monitor and respond to security incidents and Sensitive Data Protection findings. Security analysts require the following roles at the organization level: Access Context Manager Reader (roles/accesscontextmanager.policyReader) Compute Network Viewer (roles/compute.networkViewer) Data Catalog Viewer (roles/datacatalog.viewer) Cloud KMS Viewer (roles/cloudkms.viewer) Logs Viewer (roles/logging.viewer) Organization Policy Viewer (roles/orgpolicy.policyViewer) Security Center Admin Viewer (roles/securitycenter.adminViewer) Security Center Findings Editor(roles/securitycenter.findingsEditor) One of the following Security Command Center roles: Security Center Findings Bulk Mute Editor (roles/securitycenter.findingsBulkMuteEditor) Security Center Finders Mute Setter (roles/securitycenter.findingsMuteSetter) Security Center Findings State Setter (roles/securitycenter.findingsStateSetter)","{ ""id"": ""root"", ""children"": [ { ""id"": ""external_pipeline"", ""labels"": [ { ""text"": ""external_pipeline"" } ], ""children"": [ { ""id"": ""storage_ext"", ""labels"": [ { ""text"": ""Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""encryption_engine"", ""labels"": [ { ""text"": ""Encryption Engine"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""etl_pipeline"", ""labels"": [ { ""text"": ""ETL Pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""gateway"", ""labels"": [ { ""text"": ""Gateway"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_storage_encrypt"", ""labels"": [ { ""text"": ""encrypts"" } ] }, { ""id"": ""e_encrypt_etl"", ""labels"": [ { ""text"": ""processes"" } ] }, { ""id"": ""e_etl_gateway"", ""labels"": [ { ""text"": ""deploys"" } ] } ] }, { ""id"": ""gcp_main"", ""labels"": [ { ""text"": ""gcp_main"" } ], ""children"": [ { ""id"": ""data_project"", ""labels"": [ { ""text"": ""data_project"" } ], ""children"": [ { ""id"": ""bigquery_data"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""looker_data"", ""labels"": [ { ""text"": ""Looker Studio"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""vertex_notebook"", ""labels"": [ { ""text"": ""Vertex AI Notebooks"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_bq_looker_data"", ""labels"": [ { ""text"": ""visualizes"" } ] }, { ""id"": ""e_bq_notebook"", ""labels"": [ { ""text"": ""analyzes"" } ] } ] }, { ""id"": ""ingestion_project"", ""labels"": [ { ""text"": ""ingestion_project"" } ], ""children"": [ { ""id"": ""cloud_storage_ingest"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_functions_ingest"", ""labels"": [ { ""text"": ""Cloud Functions"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_ingest"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataflow_ingest"", ""labels"": [ { ""text"": ""Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_storage_cf"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_pubsub_dataflow"", ""labels"": [ { ""text"": ""streams"" } ] } ] }, { ""id"": ""interconnect_project"", ""labels"": [ { ""text"": ""interconnect_project"" } ], ""children"": [ { ""id"": ""dedicated_interconnect"", ""labels"": [ { ""text"": ""Dedicated Interconnect"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""governance_project"", ""labels"": [ { ""text"": ""governance_project"" } ], ""children"": [ { ""id"": ""kms"", ""labels"": [ { ""text"": ""Cloud KMS"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dlp"", ""labels"": [ { ""text"": ""Cloud DLP"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog"", ""labels"": [ { ""text"": ""Data Catalog"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""scc"", ""labels"": [ { ""text"": ""Security Command Center"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_storage_gov"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_logging_gov"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery_gov"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""looker_gov"", ""labels"": [ { ""text"": ""Looker Studio"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_scc_storage"", ""labels"": [ { ""text"": ""write findings"" } ] }, { ""id"": ""e_storage_logging"", ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""e_logging_bigquery_gov"", ""labels"": [ { ""text"": ""findings"" } ] }, { ""id"": ""e_bqgov_looker"", ""labels"": [ { ""text"": ""visualizes"" } ] } ] } ], ""edges"": [ { ""id"": ""e_cf_bq"", ""labels"": [ { ""text"": ""loads"" } ] }, { ""id"": ""e_dataflow_bq"", ""labels"": [ { ""text"": ""writes"" } ] }, { ""id"": ""e_interconnect_storage"", ""labels"": [ { ""text"": ""batch"" } ] }, { ""id"": ""e_interconnect_pubsub"", ""labels"": [ { ""text"": ""stream"" } ] }, { ""id"": ""e_dlp_bqdata"", ""labels"": [ { ""text"": ""scan"" } ] }, { ""id"": ""e_catalog_bqdata"", ""labels"": [ { ""text"": ""profile"" } ] } ] } ], ""edges"": [ { ""id"": ""e_gateway_interconnect"", ""labels"": [ { ""text"": ""deploys"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/data-mesh,"Architecture and functions in a data mesh bookmark_border Last reviewed 2024-09-03 UTC A data mesh is an architectural and organizational framework which treats data as a product (referred to in this document as data products). In this framework, data products are developed by the teams that best understand that data, and who follow an organization-wide set of data governance standards. Once data products are deployed to the data mesh, distributed teams in an organization can discover and access data that's relevant to their needs more quickly and efficiently. To achieve such a well-functioning data mesh, you must first establish the high-level architectural components and organizational roles that this document describes. This document is part of a series which describes how to implement a data mesh on Google Cloud. It assumes that you have read and are familiar with the concepts described in Build a modern, distributed Data Mesh with Google Cloud. The series has the following parts: Architecture and functions in a data mesh (this document) Design a self-service data platform for a data mesh Build data products in a data mesh Discover and consume data products in a data mesh In this series, the data mesh that's described is internal to an organization. Although it's possible to extend a data mesh architecture to provide data products to third-parties, this extended approach is outside the scope of this document. Extending a data mesh involves additional considerations beyond just the usage within an organization. Architecture The following key terms are used to define the architectural components which are described in this series: Data product: A data product is a logical container or grouping of one or more related data resources. Data resource: A data resource is a physical asset in a storage system which holds structured data or stores a query that yields structured data. Data attribute: A data attribute is a field or element of a data resource. The following diagram provides an overview of the key architectural components in a data mesh implemented on Google Cloud. Architectural components in a data mesh. The preceding diagram shows the following: Central services enable the creation and management of data products, including organizational policies that affect the data mesh participants, access controls (through Identity and Access Management groups), and the infrastructure-specific artifacts. Examples of such commitments and reservations, and infrastructure that facilitates the functioning of the data mesh are described in Create platform components and solutions. Central services primarily supply the Data Catalog for all the data products in the data mesh and the discovery mechanism for potential customers of these products. Data domains expose subsets of their data as data products through well-defined data consumption interfaces. These data products could be a table, view, structured file, topic, or stream. In BigQuery, it would be a dataset, and in Cloud Storage, it would be a folder or bucket. There can be different types of interfaces that can be exposed as a data product. An example of an interface is a BigQuery view over a BigQuery table. The types of interfaces most commonly used for analytical purposes are discussed in Build data products in a data mesh. Data mesh reference implementation You can find a reference implementation of this architecture in the data-mesh-demo repository. The Terraform scripts that are used in the reference implementation demonstrate data mesh concepts and are not intended for production use. By running these scripts, you'll learn how to do the following: Separate product definitions from the underlying data. Create Data Catalog templates for describing product interfaces. Tag product interfaces with these templates. Grant permissions to the product consumers. For the product interfaces, the reference implementation creates and uses the following interface types: Authorized views over BigQuery tables. Data streams based on Pub/Sub topics. For further details, refer to the README file in the repository. Functions in a data mesh For a data mesh to operate well, you must define clear roles for the people who perform tasks within the data mesh. Ownership is assigned to team archetypes, or functions. These functions hold the core user journeys for people who work in the data mesh. To clearly describe user journeys, they have been assigned to user roles. These user roles can be split and combined based on the circumstances of each enterprise. You don't need to map the roles directly with employees or teams in your organization. A data domain is aligned with a business unit (BU), or a function within an enterprise. Common examples of business domains might be the mortgage department in a bank, or the customer, distribution, finance, or HR departments of an enterprise. Conceptually, there are two domain-related functions in a data mesh: the data producer teams and the data consumer teams. It's important to understand that a single data domain is likely to serve both functions at once. A data domain team produces data products from data that it owns. The team also consumes data products for business insight, and to produce derived-data products for the use of other domains. In addition to the domain-based functions, a data mesh also has a set of functions that are performed by centralized teams within the organization. These central teams enable the operation of the data mesh by providing cross-domain oversight, services, and governance. They reduce the operational burden for data domains in producing and consuming data products, and facilitate the cross-domain relationships that are required for the data mesh to operate. This document only describes functions that have a data mesh-specific role. There are several other roles that are required in any enterprise, regardless of the architecture being employed for the platform. However, these other roles are out of scope for this document. The four main functions in a data mesh are as follows: Data domain-based producer teams: Create and maintain data products over their lifecycle. These teams are often referred to as the data producers. Data domain-based consumer teams: Discover data products and use them in various analytic applications. These teams might consume data products to create new data products. These teams are often referred to as the data consumers. Central data governance team: Defines and enforces data governance policies among data producers, ensuring high data quality and data trustworthiness for consumers. This team is often referred to as the data governance team. Central self-service data infrastructure platform team: Provides a self-service data platform for data producers. This team also provides the tooling for central data discovery and data product observability that both data consumers and data producers use. This team is often referred to as the data platform team. An optional extra function to consider is that of a Center of Excellence (COE) for the data mesh. The purpose of the COE is to provide management of the data mesh. The COE is also the designated arbitration team that resolves any conflicts raised by any of the other functions. This function is useful for helping to connect the other four functions. Data domain-based producer team Typically, data products are built on top of a physical repository of data (either single or multiple data warehouses, lakes, or streams). An organization needs traditional data platform roles to create and maintain these physical repositories. However, these traditional data platform roles are not typically the people who create the data product. To create data products from these physical repositories, an organization needs a mix of data practitioners, such as data engineers and data architects. The following table lists all the domain-specific user roles that are needed in data producer teams. Role Responsibilities Required skills Desired outcomes Data product owner Acts as the primary business point of contact for the data product. Is accountable for the definitions, policies, business decisions, and application of business rules for the data exposed as products. Acts as a point of contact for business questions. As such, the owner represents the data domain when meeting with the data consumer teams or the centralized teams (data governance and data infrastructure platform). Data analytics Data architecture Product management The data product is driving value for consumers. There's robust management of the lifecycle of the data product, including deciding when to retire a product or release a new version. There's coordination of universal data elements with other data domains. Data product technical lead Acts as the primary technical point of contact for the product. Is accountable for implementing and publishing product interfaces. Acts as a point of contact for technical questions. As such, the lead represents the data domain when meeting with the data consumer teams or the centralized teams (data governance and data infrastructure platform). Works with the data governance team to define and implement data mesh standards in the organization. Works with the data platform team to help to develop the platform in tandem with the technical needs that production and consumption generate. Data engineering Data architecture Software engineering The data product meets business requirements and adheres to the data mesh technical standards. The data consumer teams use the data product and it appears in the results generated by the data product discovery experience. The use of the data product can be analyzed (for example, the number of daily queries). Data product support Acts as the point of contact for production support. Is accountable for maintaining product Service Level Agreement (SLA). Software engineering Site reliability engineering (SRE) The data product is meeting the stated SLA. Data consumer questions about the use of the data product are addressed and resolved. Subject matter expert (SME) for data domain Represents the data domain when meeting with SMEs from other data domains to establish data element definitions and boundaries that are common across the organization. Helps new data producers within the domain define their product scopes. Data analytics Data architecture Collaborates with other SMEs from across data domains to establish and maintain comprehensive understanding of the data in the organization and the data models that it uses. Facilitates the creation of interoperable data products which match the overall data model of the organization. There are clear standards for data product creation and lifecycle management. The data products from the data domain provide business value. Data owner Is accountable for a content area. Is responsible for data quality and accuracy. Approves access requests. Contributes to data product documentation. Any skill, but must have full knowledge of the business function. Any skill, but must have full knowledge of what the data means and business rules around it. Any skill, but must be able to determine the best possible resolution to data quality issues. Data that cross-functional areas use is accurate. Stakeholders understand the data. Data use is in accordance with usage policies. Data domain-based consumer teams In a data mesh, the people that consume a data product are typically data users who are outside of the data product domain. These data consumers use a central data catalog to find data products that are relevant to their needs. Because it's possible that more than one data product might meet their needs, data consumers can end up subscribing to multiple data products. If data consumers are unable to find the required data product for their use case, it's their responsibility to consult directly with the data mesh COE. During that consultation, data consumers can raise their data needs and seek advice on how to get those needs met by one or more domains. When looking for a data product, data consumers are looking for data that help them achieve various use cases such as persistent analytics dashboards and reports, individual performance reports, and other business performance metrics. Alternatively, data consumers might be looking for data products that can be used in artificial intelligence (AI) and machine learning (ML) use cases. To achieve these various use cases, data consumers require a mix of data practitioner personas, which are as follows: Role Responsibilities Required skills Desired outcomes Data analyst Searches for, identifies, evaluates, and subscribes to single-domain or cross-domain data products to create a foundation for business intelligence frameworks to operate. Analytics engineering Business analytics Provides clean, curated, and aggregated datasets for data visualization specialists to consume. Creates best practices for how to use data products. Aggregates and curates cross-domain datasets to meet the analytical needs of their domain. Application developer Develops an application framework for consumption of data across one or more data products, either inside or outside of the domain. Application development Data engineering Creates, serves, and maintains applications that consume data from one or more data products. Creates data applications for end-user consumption. Data visualization specialist Translates data engineering and data analysis jargon into information which business stakeholders can understand. Defines processes to populate business reports from data products. Creates and monitors reports that describe strategic business goals. Collaborates with engineers within the organization to design datasets which are aggregated from consumed data products. Implements reporting solutions. Translates high-level business requirements into technical requirements. Requirement analysis Data visualization Provides valid, accurate datasets and reports to end users. Business requirements are met through the dashboards and reports that are developed. Data scientist Searches for, identifies, evaluates, and subscribes to data products for data science use cases. Extracts data products and metadata from multiple data domains. Trains predictive models and deploys those models to optimize domain business processes. Provides feedback on possible data curation and data annotation techniques for multiple data domains. ML engineering Analytics engineering Creates predictive and prescriptive models to optimize business processes. Model training and model deployment are done in a timely manner. Central data governance team The data governance team enables data producers and consumers to safely share, aggregate, and compute data in a self-service manner, without introducing compliance risks to the organization. To meet the compliance requirements of the organization, the data governance team is a mix of data practitioner personas, which are as follows: Role Responsibilities Required skills Desired outcomes Data governance specialist Provides oversight and coordinates a single view of compliance. Recommends mesh-wide privacy policies on data collection, data protection, and data retention. Ensures that data stewards know about policies and can access them. Informs and consults on the latest data privacy regulations as required. Informs and consults on security questions as required. Performs internal audits and shares regular reports on risk and control plans. Legal SME Security SME Data privacy SME Privacy regulations in policies are up to date. Data producers are informed of policy changes in a timely manner. Management receives timely and regular reports on policy compliance for all published data products. Data steward (sits within each domain) Codifies the policies created by the data governance specialists. Defines and updates the taxonomy that an organization uses for annotating data products, data resources, and data attributes with discovery and privacy-related metadata. Coordinates across various stakeholders inside and outside of their respective domain. Ensures that the data products in their domain meet the metadata standards and privacy policies of the organization. Provides guidance to the data governance engineers on how to design and prioritize data platform features. Data architecture Data stewardship Required metadata has been created for all data products in the domain, and the data products for the domain are described accurately. The self-service data infrastructure platform team is building the right tooling to automate metadata annotations of data products, policy creation and verification. Data governance engineer Develops tools which auto-generate data annotations and can be used by all data domains, and then uses these annotations for policy enforcement. Implements monitoring to check the consistency of annotations and alerts when problems are found. Ensures that employees in the organization are informed of the status of data products by implementing alerts, reporting, and dashboards. Software engineering Data governance annotations are automatically verified. Data products comply with data governance policies. Data product violations are detected in a timely fashion. Central self-service data infrastructure platform team The self-service data infrastructure platform team, or just the data platform team, is responsible for creating a set of data infrastructure components. Distributed data domain teams use these components to build and deploy their data products. The data platform team also promotes best practices and introduces tools and methodologies which help to reduce cognitive load for distributed teams when adopting new technology. Platform infrastructure should provide easy integration with operations toolings for global observability, instrumentation, and compliance automation. Alternatively, the infrastructure should facilitate such integration to set up distributed teams for success. The data platform team has a shared responsibility model that it uses with the distributed domain teams and the underlying infrastructure team. The model shows what responsibilities are expected from the consumers of the platform, and what platform components the data platform team supports. As the data platform is itself an internal product, the platform doesn't support every use case. Instead, the data platform team continuously releases new services and features according to a prioritized roadmap. The data platform team might have a standard set of components in place and in development. However, data domain teams might choose to use a different, unique set of components if the needs of a team don't align with those provided by the data platform. If data domain teams choose a different approach, they must ensure that any platform infrastructure that they build and maintain complies with organization-wide policies and guardrails for security and data governance. For data platform infrastructure that is developed outside of the central data platform team, the data platform team might either choose to co-invest or embed their own engineers into the domain teams. Whether the data platform team chooses to co-invest or embed engineers might depend on the strategic importance of the data domain platform infrastructure to the organization. By staying involved in the development of infrastructure by data domain teams, organizations can provide the alignment and technical expertise required to repackage any new platform infrastructure components that are in development for future reuse. You might need to limit autonomy in the early stages of building a data mesh if your initial goal is to get approval from stakeholders for scaling up the data mesh. However, limiting autonomy risks creating a bottleneck at the central data platform team. This bottleneck can inhibit the data mesh from scaling. So, any centralization decisions should be taken carefully. For data producers, making their technical choices from a limited set of available options might be preferable to evaluating and choosing from an unlimited list of options themselves. Promoting autonomy of data producers doesn't equate to creating an ungoverned technology landscape. Instead, the goal is to drive compliance and platform adoption by striking the right balance between freedom of choice and standardization. Finally, a good data platform team is a central source of education and best practices for the rest of the company. Some of the most impactful activities that we recommend central data platform teams undertake are as follows: Fostering regular architectural design reviews for new functional projects and proposing common ways of development across development teams. Sharing knowledge and experiences, and collectively defining best practices and architectural guidelines. Ensuring engineers have the right tools in place to validate and check for common pitfalls like issues with code, bugs, and performance degradations. Organizing internal hackathons so development teams can surface their requirements for internal tooling needs. Example roles and responsibilities for the central data platform team might include the following: Role	Responsibilities Required skills	Desired outcomes Data platform product owner Creates an ecosystem of data infrastructure and solutions to empower distributed teams to build data products. Lowers the technical barrier to entry, ensures that governance is embedded, and minimizes collective technical debt for data infrastructure. Interfaces with leadership, data domain owners, data governance team, and technology platform owners to set the strategy and roadmap for the data platform. Data strategy and operations Product management Stakeholder management Establishes an ecosystem of successful data products. There are robust numbers of data products in production. There's a reduction in time-to-minimum viable product and time-to-production for data product releases. A portfolio of generalized infrastructure and components is in place that addresses the most common needs for data producers and data consumers. There's a high satisfaction score from data producers and data consumers. Data platform engineer Creates reusable and self-service data infrastructure and solutions for data ingestion, storage, processing, and consumption through templates, deployable architecture blueprints, developer guides, and other documentation. Also creates Terraform templates, data pipeline templates, container templates, and orchestration tooling. Develops and maintains central data services and frameworks to standardize processes for cross-functional concerns such as data sharing, pipelines orchestration, logging and monitoring, data governance, continuous integration and continuous deployment (CI/CD) with embedded guardrails, security and compliance reporting, and FinOps reporting. Data engineering Software engineering There are standardized, reusable infrastructure components and solutions for data producers to do data ingestion, storage, processing, curation, and sharing, along with necessary documentation. Releases of components, solutions, and end-user documentation align with the roadmap. Users report a high level of customer satisfaction. There are robust shared services for all functions in the data mesh. There is high uptime for shared services. The support response time is short. Platform and security engineer (a representative from the central IT teams such as networking and security, who is embedded in the data platform team) Ensures that data platform abstractions are aligned to enterprise-wide technology frameworks and decisions. Supports engineering activities by building the technology solutions and services in their core team that are necessary for data platform delivery. Infrastructure engineering Software engineering Platform infrastructure components are developed for the data platform. Releases of components, solutions, and end-user documentation align with the roadmap. The central data platform engineers report a high level of customer satisfaction. The health of the infrastructure platform improves for components that are used by the data platform (for example, logging). Underlying technology components have a high uptime. When data platform engineers have issues, the support response time is short. Enterprise architect Aligns data mesh and data platform architecture with enterprise-wide technology and data strategy. Provides advisory and design authority and assurance for both data platform and data product architectures to ensure alignment with enterprise-level strategy and best-practices. Data architecture Solution iteration and problem solving Consensus building A successful ecosystem is built that includes robust numbers of data products for which there is a reduction in time to both create minimum viable products and to release those products into production. Architecture standards have been established for critical data journeys, such as by establishing common standards for metadata management and for data sharing architecture. Additional considerations for a data mesh There are multiple architectural options for an analytics data platform, each option with different prerequisites. To enable each data mesh architecture, we recommend that your organization follow the best practices described in this section. Acquire platform funding As explained in the blog post, ""If you want to transform start with finance"", the platform is never finished: it's always operating based on a prioritized roadmap. Therefore, the platform must be funded as a product, not as a project with a fixed endpoint. The first adopter of the data mesh bears the cost. Usually, the cost is shared between the business that forms the first data domain to initiate the data mesh, and the central technology team, which generally houses the central data platform team. To convince finance teams to approve funding for the central platform, we recommend that you make a business case for the value of the centralized platform being realized over time. That value comes from reimplementing the same components in individual delivery teams. Define the minimum viable platform for the data mesh To help you to define the minimum viable platform for the data mesh, we recommend that you pilot and iterate with one or more business cases. For your pilot, find use cases that are needed, and where there's a consumer ready to adopt the resulting data product. The use cases should already have funding to develop the data products, but there should be a need for input from technical teams. Make sure the team that is implementing the pilot understands the data mesh operating model as follows: The business (that is, the data producer team) owns the backlog, support, and maintenance. The central team defines the self-service patterns and helps the business build the data product, but passes the data product to the business to run and own when it's complete. The primary goal is to prove the business operating model (domains produce, domains consume). The secondary goal is to prove the technical operating model (self-service patterns developed by the central team). Because platform team resources are limited, use the trunk and branch teams model to pool knowledge but still allow for the development of specialized platform services and products. We also recommend that you do the following: Plan roadmaps rather than letting services and features evolve organically. Define minimum viable platform capabilities spanning ingest, storage, processing, analysis, and ML. Embed data governance in every step, not as a separate workstream. Put in place the minimum capabilities across governance, platform, value-stream, and change management. Minimum capabilities are those which meet 80% of business cases. Plan for the co-existence of the data mesh with an existing data platform Many organizations that want to implement a data mesh likely already have an existing data platform, such as a data lake, data warehouse, or a combination of both. Before implementing a data mesh, these organizations must make a plan for how their existing data platform can evolve as the data mesh grows. These organizations should consider factors such as the following: The data resources that are most effective on the data mesh. The assets that must stay within the existing data platform. Whether assets have to move, or whether they can be maintained on the existing platform and still participate in the data mesh.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_cloud_environment"", ""labels"": [ { ""text"": ""gcp_cloud_environment"" } ], ""children"": [ { ""id"": ""central_services"", ""labels"": [ { ""text"": ""central_services"" } ], ""children"": [ { ""id"": ""central_management"", ""labels"": [ { ""text"": ""central_management"" } ], ""children"": [ { ""id"": ""access_control_iam"", ""labels"": [ { ""text"": ""Access Control IAM"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""access_policies_dataplex"", ""labels"": [ { ""text"": ""Access Policies Dataplex"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""reservation_bigquery"", ""labels"": [ { ""text"": ""Reservation BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""discovery_services"", ""labels"": [ { ""text"": ""discovery_services"" } ], ""children"": [ { ""id"": ""catalog_dataplex"", ""labels"": [ { ""text"": ""Catalog Dataplex"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""search_ui_cloudrun"", ""labels"": [ { ""text"": ""Search UI Cloud Run"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""docs_google_drive"", ""labels"": [ { ""text"": ""Docs Google Drive"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] }, { ""id"": ""consumers"", ""labels"": [ { ""text"": ""consumers"" } ], ""children"": [ { ""id"": ""reports_consumer"", ""labels"": [ { ""text"": ""reports_consumer"" } ], ""children"": [ { ""id"": ""reports_sql_engine"", ""labels"": [ { ""text"": ""SQL Engine BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""looker"", ""labels"": [ { ""text"": ""Looker"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_reports_sql_to_looker"", ""labels"": [ { ""text"": ""queries"" } ] } ] }, { ""id"": ""data_science_consumer"", ""labels"": [ { ""text"": ""data_science_consumer"" } ], ""children"": [ { ""id"": ""ds_sql_engine"", ""labels"": [ { ""text"": ""SQL Engine BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""vertex_ai"", ""labels"": [ { ""text"": ""Data Science Vertex AI"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ds_sql_to_vertex"", ""labels"": [ { ""text"": ""feeds"" } ] } ] }, { ""id"": ""data_analytics_consumer"", ""labels"": [ { ""text"": ""data_analytics_consumer"" } ], ""children"": [ { ""id"": ""analytics_sql_engine"", ""labels"": [ { ""text"": ""SQL Engine BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""spark_job"", ""labels"": [ { ""text"": ""Spark Job Dataproc"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_analytics_sql_to_spark"", ""labels"": [ { ""text"": ""triggers"" } ] } ] } ], ""edges"": [] }, { ""id"": ""producers"", ""labels"": [ { ""text"": ""producers"" } ], ""children"": [ { ""id"": ""data_domain1"", ""labels"": [ { ""text"": ""data_domain1"" } ], ""children"": [ { ""id"": ""data_sources_domain1"", ""labels"": [ { ""text"": ""data_sources_domain1"" } ], ""children"": [ { ""id"": ""inventory_spanner"", ""labels"": [ { ""text"": ""Inventory Cloud Spanner"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_product_domain1"", ""labels"": [ { ""text"": ""data_product_domain1"" } ], ""children"": [ { ""id"": ""native_storage_bq1"", ""labels"": [ { ""text"": ""Native Storage BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""auth_dataset_bq1"", ""labels"": [ { ""text"": ""Auth Dataset BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_native1_to_auth1"", ""labels"": [ { ""text"": ""curates"" } ] } ] } ], ""edges"": [ { ""id"": ""e_inv1_to_native1"", ""labels"": [ { ""text"": ""flows to"" } ] } ] }, { ""id"": ""data_domain2"", ""labels"": [ { ""text"": ""data_domain2"" } ], ""children"": [ { ""id"": ""curation_mgmt_dataplex"", ""labels"": [ { ""text"": ""Curation & Mgmt Dataplex"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_sources_domain2"", ""labels"": [ { ""text"": ""data_sources_domain2"" } ], ""children"": [ { ""id"": ""raw_data_storage"", ""labels"": [ { ""text"": ""Raw Data Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_product_domain2"", ""labels"": [ { ""text"": ""data_product_domain2"" } ], ""children"": [ { ""id"": ""biglake_table"", ""labels"": [ { ""text"": ""BigLake Table Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""native_storage_bq2"", ""labels"": [ { ""text"": ""Native Storage BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""auth_dataset_bq2"", ""labels"": [ { ""text"": ""Auth Dataset BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""read_api_bq2"", ""labels"": [ { ""text"": ""Read API BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_biglake_to_auth2"", ""labels"": [ { ""text"": ""curates"" } ] }, { ""id"": ""e_native2_to_auth2"", ""labels"": [ { ""text"": ""curates"" } ] }, { ""id"": ""e_auth2_to_readapi"", ""labels"": [ { ""text"": ""serves"" } ] } ] } ], ""edges"": [ { ""id"": ""e_curation_to_raw"", ""labels"": [ { ""text"": ""curates"" } ] }, { ""id"": ""e_curation_to_biglake"", ""labels"": [ { ""text"": ""curates"" } ] }, { ""id"": ""e_raw_to_native2"", ""labels"": [ { ""text"": ""flows to"" } ] }, { ""id"": ""e_raw_to_biglake"", ""labels"": [ { ""text"": ""flows to"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_auth1_to_reports"", ""labels"": [ { ""text"": ""shares dataset"" } ] }, { ""id"": ""e_auth1_to_ds"", ""labels"": [ { ""text"": ""shares dataset"" } ] }, { ""id"": ""e_auth1_to_analytics"", ""labels"": [ { ""text"": ""shares dataset"" } ] }, { ""id"": ""e_auth2_to_reports"", ""labels"": [ { ""text"": ""shares dataset"" } ] }, { ""id"": ""e_auth2_to_ds"", ""labels"": [ { ""text"": ""shares dataset"" } ] }, { ""id"": ""e_auth2_to_analytics"", ""labels"": [ { ""text"": ""shares dataset"" } ] }, { ""id"": ""e_readapi_to_analytics"", ""labels"": [ { ""text"": ""provides interface"" } ] } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/data-mesh,"Architecture and functions in a data mesh bookmark_border Last reviewed 2024-09-03 UTC A data mesh is an architectural and organizational framework which treats data as a product (referred to in this document as data products). In this framework, data products are developed by the teams that best understand that data, and who follow an organization-wide set of data governance standards. Once data products are deployed to the data mesh, distributed teams in an organization can discover and access data that's relevant to their needs more quickly and efficiently. To achieve such a well-functioning data mesh, you must first establish the high-level architectural components and organizational roles that this document describes. This document is part of a series which describes how to implement a data mesh on Google Cloud. It assumes that you have read and are familiar with the concepts described in Build a modern, distributed Data Mesh with Google Cloud. The series has the following parts: Architecture and functions in a data mesh (this document) Design a self-service data platform for a data mesh Build data products in a data mesh Discover and consume data products in a data mesh In this series, the data mesh that's described is internal to an organization. Although it's possible to extend a data mesh architecture to provide data products to third-parties, this extended approach is outside the scope of this document. Extending a data mesh involves additional considerations beyond just the usage within an organization. Architecture The following key terms are used to define the architectural components which are described in this series: Data product: A data product is a logical container or grouping of one or more related data resources. Data resource: A data resource is a physical asset in a storage system which holds structured data or stores a query that yields structured data. Data attribute: A data attribute is a field or element of a data resource. The following diagram provides an overview of the key architectural components in a data mesh implemented on Google Cloud. Architectural components in a data mesh. The preceding diagram shows the following: Central services enable the creation and management of data products, including organizational policies that affect the data mesh participants, access controls (through Identity and Access Management groups), and the infrastructure-specific artifacts. Examples of such commitments and reservations, and infrastructure that facilitates the functioning of the data mesh are described in Create platform components and solutions. Central services primarily supply the Data Catalog for all the data products in the data mesh and the discovery mechanism for potential customers of these products. Data domains expose subsets of their data as data products through well-defined data consumption interfaces. These data products could be a table, view, structured file, topic, or stream. In BigQuery, it would be a dataset, and in Cloud Storage, it would be a folder or bucket. There can be different types of interfaces that can be exposed as a data product. An example of an interface is a BigQuery view over a BigQuery table. The types of interfaces most commonly used for analytical purposes are discussed in Build data products in a data mesh. Data mesh reference implementation You can find a reference implementation of this architecture in the data-mesh-demo repository. The Terraform scripts that are used in the reference implementation demonstrate data mesh concepts and are not intended for production use. By running these scripts, you'll learn how to do the following: Separate product definitions from the underlying data. Create Data Catalog templates for describing product interfaces. Tag product interfaces with these templates. Grant permissions to the product consumers. For the product interfaces, the reference implementation creates and uses the following interface types: Authorized views over BigQuery tables. Data streams based on Pub/Sub topics. For further details, refer to the README file in the repository. Functions in a data mesh For a data mesh to operate well, you must define clear roles for the people who perform tasks within the data mesh. Ownership is assigned to team archetypes, or functions. These functions hold the core user journeys for people who work in the data mesh. To clearly describe user journeys, they have been assigned to user roles. These user roles can be split and combined based on the circumstances of each enterprise. You don't need to map the roles directly with employees or teams in your organization. A data domain is aligned with a business unit (BU), or a function within an enterprise. Common examples of business domains might be the mortgage department in a bank, or the customer, distribution, finance, or HR departments of an enterprise. Conceptually, there are two domain-related functions in a data mesh: the data producer teams and the data consumer teams. It's important to understand that a single data domain is likely to serve both functions at once. A data domain team produces data products from data that it owns. The team also consumes data products for business insight, and to produce derived-data products for the use of other domains. In addition to the domain-based functions, a data mesh also has a set of functions that are performed by centralized teams within the organization. These central teams enable the operation of the data mesh by providing cross-domain oversight, services, and governance. They reduce the operational burden for data domains in producing and consuming data products, and facilitate the cross-domain relationships that are required for the data mesh to operate. This document only describes functions that have a data mesh-specific role. There are several other roles that are required in any enterprise, regardless of the architecture being employed for the platform. However, these other roles are out of scope for this document. The four main functions in a data mesh are as follows: Data domain-based producer teams: Create and maintain data products over their lifecycle. These teams are often referred to as the data producers. Data domain-based consumer teams: Discover data products and use them in various analytic applications. These teams might consume data products to create new data products. These teams are often referred to as the data consumers. Central data governance team: Defines and enforces data governance policies among data producers, ensuring high data quality and data trustworthiness for consumers. This team is often referred to as the data governance team. Central self-service data infrastructure platform team: Provides a self-service data platform for data producers. This team also provides the tooling for central data discovery and data product observability that both data consumers and data producers use. This team is often referred to as the data platform team. An optional extra function to consider is that of a Center of Excellence (COE) for the data mesh. The purpose of the COE is to provide management of the data mesh. The COE is also the designated arbitration team that resolves any conflicts raised by any of the other functions. This function is useful for helping to connect the other four functions. Data domain-based producer team Typically, data products are built on top of a physical repository of data (either single or multiple data warehouses, lakes, or streams). An organization needs traditional data platform roles to create and maintain these physical repositories. However, these traditional data platform roles are not typically the people who create the data product. To create data products from these physical repositories, an organization needs a mix of data practitioners, such as data engineers and data architects. The following table lists all the domain-specific user roles that are needed in data producer teams. Role Responsibilities Required skills Desired outcomes Data product owner Acts as the primary business point of contact for the data product. Is accountable for the definitions, policies, business decisions, and application of business rules for the data exposed as products. Acts as a point of contact for business questions. As such, the owner represents the data domain when meeting with the data consumer teams or the centralized teams (data governance and data infrastructure platform). Data analytics Data architecture Product management The data product is driving value for consumers. There's robust management of the lifecycle of the data product, including deciding when to retire a product or release a new version. There's coordination of universal data elements with other data domains. Data product technical lead Acts as the primary technical point of contact for the product. Is accountable for implementing and publishing product interfaces. Acts as a point of contact for technical questions. As such, the lead represents the data domain when meeting with the data consumer teams or the centralized teams (data governance and data infrastructure platform). Works with the data governance team to define and implement data mesh standards in the organization. Works with the data platform team to help to develop the platform in tandem with the technical needs that production and consumption generate. Data engineering Data architecture Software engineering The data product meets business requirements and adheres to the data mesh technical standards. The data consumer teams use the data product and it appears in the results generated by the data product discovery experience. The use of the data product can be analyzed (for example, the number of daily queries). Data product support Acts as the point of contact for production support. Is accountable for maintaining product Service Level Agreement (SLA). Software engineering Site reliability engineering (SRE) The data product is meeting the stated SLA. Data consumer questions about the use of the data product are addressed and resolved. Subject matter expert (SME) for data domain Represents the data domain when meeting with SMEs from other data domains to establish data element definitions and boundaries that are common across the organization. Helps new data producers within the domain define their product scopes. Data analytics Data architecture Collaborates with other SMEs from across data domains to establish and maintain comprehensive understanding of the data in the organization and the data models that it uses. Facilitates the creation of interoperable data products which match the overall data model of the organization. There are clear standards for data product creation and lifecycle management. The data products from the data domain provide business value. Data owner Is accountable for a content area. Is responsible for data quality and accuracy. Approves access requests. Contributes to data product documentation. Any skill, but must have full knowledge of the business function. Any skill, but must have full knowledge of what the data means and business rules around it. Any skill, but must be able to determine the best possible resolution to data quality issues. Data that cross-functional areas use is accurate. Stakeholders understand the data. Data use is in accordance with usage policies. Data domain-based consumer teams In a data mesh, the people that consume a data product are typically data users who are outside of the data product domain. These data consumers use a central data catalog to find data products that are relevant to their needs. Because it's possible that more than one data product might meet their needs, data consumers can end up subscribing to multiple data products. If data consumers are unable to find the required data product for their use case, it's their responsibility to consult directly with the data mesh COE. During that consultation, data consumers can raise their data needs and seek advice on how to get those needs met by one or more domains. When looking for a data product, data consumers are looking for data that help them achieve various use cases such as persistent analytics dashboards and reports, individual performance reports, and other business performance metrics. Alternatively, data consumers might be looking for data products that can be used in artificial intelligence (AI) and machine learning (ML) use cases. To achieve these various use cases, data consumers require a mix of data practitioner personas, which are as follows: Role Responsibilities Required skills Desired outcomes Data analyst Searches for, identifies, evaluates, and subscribes to single-domain or cross-domain data products to create a foundation for business intelligence frameworks to operate. Analytics engineering Business analytics Provides clean, curated, and aggregated datasets for data visualization specialists to consume. Creates best practices for how to use data products. Aggregates and curates cross-domain datasets to meet the analytical needs of their domain. Application developer Develops an application framework for consumption of data across one or more data products, either inside or outside of the domain. Application development Data engineering Creates, serves, and maintains applications that consume data from one or more data products. Creates data applications for end-user consumption. Data visualization specialist Translates data engineering and data analysis jargon into information which business stakeholders can understand. Defines processes to populate business reports from data products. Creates and monitors reports that describe strategic business goals. Collaborates with engineers within the organization to design datasets which are aggregated from consumed data products. Implements reporting solutions. Translates high-level business requirements into technical requirements. Requirement analysis Data visualization Provides valid, accurate datasets and reports to end users. Business requirements are met through the dashboards and reports that are developed. Data scientist Searches for, identifies, evaluates, and subscribes to data products for data science use cases. Extracts data products and metadata from multiple data domains. Trains predictive models and deploys those models to optimize domain business processes. Provides feedback on possible data curation and data annotation techniques for multiple data domains. ML engineering Analytics engineering Creates predictive and prescriptive models to optimize business processes. Model training and model deployment are done in a timely manner. Central data governance team The data governance team enables data producers and consumers to safely share, aggregate, and compute data in a self-service manner, without introducing compliance risks to the organization. To meet the compliance requirements of the organization, the data governance team is a mix of data practitioner personas, which are as follows: Role Responsibilities Required skills Desired outcomes Data governance specialist Provides oversight and coordinates a single view of compliance. Recommends mesh-wide privacy policies on data collection, data protection, and data retention. Ensures that data stewards know about policies and can access them. Informs and consults on the latest data privacy regulations as required. Informs and consults on security questions as required. Performs internal audits and shares regular reports on risk and control plans. Legal SME Security SME Data privacy SME Privacy regulations in policies are up to date. Data producers are informed of policy changes in a timely manner. Management receives timely and regular reports on policy compliance for all published data products. Data steward (sits within each domain) Codifies the policies created by the data governance specialists. Defines and updates the taxonomy that an organization uses for annotating data products, data resources, and data attributes with discovery and privacy-related metadata. Coordinates across various stakeholders inside and outside of their respective domain. Ensures that the data products in their domain meet the metadata standards and privacy policies of the organization. Provides guidance to the data governance engineers on how to design and prioritize data platform features. Data architecture Data stewardship Required metadata has been created for all data products in the domain, and the data products for the domain are described accurately. The self-service data infrastructure platform team is building the right tooling to automate metadata annotations of data products, policy creation and verification. Data governance engineer Develops tools which auto-generate data annotations and can be used by all data domains, and then uses these annotations for policy enforcement. Implements monitoring to check the consistency of annotations and alerts when problems are found. Ensures that employees in the organization are informed of the status of data products by implementing alerts, reporting, and dashboards. Software engineering Data governance annotations are automatically verified. Data products comply with data governance policies. Data product violations are detected in a timely fashion. Central self-service data infrastructure platform team The self-service data infrastructure platform team, or just the data platform team, is responsible for creating a set of data infrastructure components. Distributed data domain teams use these components to build and deploy their data products. The data platform team also promotes best practices and introduces tools and methodologies which help to reduce cognitive load for distributed teams when adopting new technology. Platform infrastructure should provide easy integration with operations toolings for global observability, instrumentation, and compliance automation. Alternatively, the infrastructure should facilitate such integration to set up distributed teams for success. The data platform team has a shared responsibility model that it uses with the distributed domain teams and the underlying infrastructure team. The model shows what responsibilities are expected from the consumers of the platform, and what platform components the data platform team supports. As the data platform is itself an internal product, the platform doesn't support every use case. Instead, the data platform team continuously releases new services and features according to a prioritized roadmap. The data platform team might have a standard set of components in place and in development. However, data domain teams might choose to use a different, unique set of components if the needs of a team don't align with those provided by the data platform. If data domain teams choose a different approach, they must ensure that any platform infrastructure that they build and maintain complies with organization-wide policies and guardrails for security and data governance. For data platform infrastructure that is developed outside of the central data platform team, the data platform team might either choose to co-invest or embed their own engineers into the domain teams. Whether the data platform team chooses to co-invest or embed engineers might depend on the strategic importance of the data domain platform infrastructure to the organization. By staying involved in the development of infrastructure by data domain teams, organizations can provide the alignment and technical expertise required to repackage any new platform infrastructure components that are in development for future reuse. You might need to limit autonomy in the early stages of building a data mesh if your initial goal is to get approval from stakeholders for scaling up the data mesh. However, limiting autonomy risks creating a bottleneck at the central data platform team. This bottleneck can inhibit the data mesh from scaling. So, any centralization decisions should be taken carefully. For data producers, making their technical choices from a limited set of available options might be preferable to evaluating and choosing from an unlimited list of options themselves. Promoting autonomy of data producers doesn't equate to creating an ungoverned technology landscape. Instead, the goal is to drive compliance and platform adoption by striking the right balance between freedom of choice and standardization. Finally, a good data platform team is a central source of education and best practices for the rest of the company. Some of the most impactful activities that we recommend central data platform teams undertake are as follows: Fostering regular architectural design reviews for new functional projects and proposing common ways of development across development teams. Sharing knowledge and experiences, and collectively defining best practices and architectural guidelines. Ensuring engineers have the right tools in place to validate and check for common pitfalls like issues with code, bugs, and performance degradations. Organizing internal hackathons so development teams can surface their requirements for internal tooling needs. Example roles and responsibilities for the central data platform team might include the following: Role	Responsibilities Required skills	Desired outcomes Data platform product owner Creates an ecosystem of data infrastructure and solutions to empower distributed teams to build data products. Lowers the technical barrier to entry, ensures that governance is embedded, and minimizes collective technical debt for data infrastructure. Interfaces with leadership, data domain owners, data governance team, and technology platform owners to set the strategy and roadmap for the data platform. Data strategy and operations Product management Stakeholder management Establishes an ecosystem of successful data products. There are robust numbers of data products in production. There's a reduction in time-to-minimum viable product and time-to-production for data product releases. A portfolio of generalized infrastructure and components is in place that addresses the most common needs for data producers and data consumers. There's a high satisfaction score from data producers and data consumers. Data platform engineer Creates reusable and self-service data infrastructure and solutions for data ingestion, storage, processing, and consumption through templates, deployable architecture blueprints, developer guides, and other documentation. Also creates Terraform templates, data pipeline templates, container templates, and orchestration tooling. Develops and maintains central data services and frameworks to standardize processes for cross-functional concerns such as data sharing, pipelines orchestration, logging and monitoring, data governance, continuous integration and continuous deployment (CI/CD) with embedded guardrails, security and compliance reporting, and FinOps reporting. Data engineering Software engineering There are standardized, reusable infrastructure components and solutions for data producers to do data ingestion, storage, processing, curation, and sharing, along with necessary documentation. Releases of components, solutions, and end-user documentation align with the roadmap. Users report a high level of customer satisfaction. There are robust shared services for all functions in the data mesh. There is high uptime for shared services. The support response time is short. Platform and security engineer (a representative from the central IT teams such as networking and security, who is embedded in the data platform team) Ensures that data platform abstractions are aligned to enterprise-wide technology frameworks and decisions. Supports engineering activities by building the technology solutions and services in their core team that are necessary for data platform delivery. Infrastructure engineering Software engineering Platform infrastructure components are developed for the data platform. Releases of components, solutions, and end-user documentation align with the roadmap. The central data platform engineers report a high level of customer satisfaction. The health of the infrastructure platform improves for components that are used by the data platform (for example, logging). Underlying technology components have a high uptime. When data platform engineers have issues, the support response time is short. Enterprise architect Aligns data mesh and data platform architecture with enterprise-wide technology and data strategy. Provides advisory and design authority and assurance for both data platform and data product architectures to ensure alignment with enterprise-level strategy and best-practices. Data architecture Solution iteration and problem solving Consensus building A successful ecosystem is built that includes robust numbers of data products for which there is a reduction in time to both create minimum viable products and to release those products into production. Architecture standards have been established for critical data journeys, such as by establishing common standards for metadata management and for data sharing architecture. Additional considerations for a data mesh There are multiple architectural options for an analytics data platform, each option with different prerequisites. To enable each data mesh architecture, we recommend that your organization follow the best practices described in this section. Acquire platform funding As explained in the blog post, ""If you want to transform start with finance"", the platform is never finished: it's always operating based on a prioritized roadmap. Therefore, the platform must be funded as a product, not as a project with a fixed endpoint. The first adopter of the data mesh bears the cost. Usually, the cost is shared between the business that forms the first data domain to initiate the data mesh, and the central technology team, which generally houses the central data platform team. To convince finance teams to approve funding for the central platform, we recommend that you make a business case for the value of the centralized platform being realized over time. That value comes from reimplementing the same components in individual delivery teams. Define the minimum viable platform for the data mesh To help you to define the minimum viable platform for the data mesh, we recommend that you pilot and iterate with one or more business cases. For your pilot, find use cases that are needed, and where there's a consumer ready to adopt the resulting data product. The use cases should already have funding to develop the data products, but there should be a need for input from technical teams. Make sure the team that is implementing the pilot understands the data mesh operating model as follows: The business (that is, the data producer team) owns the backlog, support, and maintenance. The central team defines the self-service patterns and helps the business build the data product, but passes the data product to the business to run and own when it's complete. The primary goal is to prove the business operating model (domains produce, domains consume). The secondary goal is to prove the technical operating model (self-service patterns developed by the central team). Because platform team resources are limited, use the trunk and branch teams model to pool knowledge but still allow for the development of specialized platform services and products. We also recommend that you do the following: Plan roadmaps rather than letting services and features evolve organically. Define minimum viable platform capabilities spanning ingest, storage, processing, analysis, and ML. Embed data governance in every step, not as a separate workstream. Put in place the minimum capabilities across governance, platform, value-stream, and change management. Minimum capabilities are those which meet 80% of business cases. Plan for the co-existence of the data mesh with an existing data platform Many organizations that want to implement a data mesh likely already have an existing data platform, such as a data lake, data warehouse, or a combination of both. Before implementing a data mesh, these organizations must make a plan for how their existing data platform can evolve as the data mesh grows. These organizations should consider factors such as the following: The data resources that are most effective on the data mesh. The assets that must stay within the existing data platform. Whether assets have to move, or whether they can be maintained on the existing platform and still participate in the data mesh.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp"", ""labels"": [ { ""text"": ""Google Cloud"" } ], ""children"": [ { ""id"": ""consumers"", ""labels"": [ { ""text"": ""consumers"" } ], ""children"": [ { ""id"": ""reports_consumer"", ""labels"": [ { ""text"": ""reports_consumer"" } ], ""children"": [ { ""id"": ""reports_sql"", ""labels"": [ { ""text"": ""SQL Engine (Reports)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""looker"", ""labels"": [ { ""text"": ""Looker"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_reports_sql_to_looker"", ""labels"": [ { ""text"": ""feeds"" } ] } ] }, { ""id"": ""ds_consumer"", ""labels"": [ { ""text"": ""ds_consumer"" } ], ""children"": [ { ""id"": ""ds_sql"", ""labels"": [ { ""text"": ""SQL Engine (Data Science)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""vertex_ai"", ""labels"": [ { ""text"": ""Vertex AI"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ds_sql_to_vertex"", ""labels"": [ { ""text"": ""feeds"" } ] } ] }, { ""id"": ""analytics_consumer"", ""labels"": [ { ""text"": ""analytics_consumer"" } ], ""children"": [ { ""id"": ""analytics_sql"", ""labels"": [ { ""text"": ""SQL Engine (Analytics)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""spark_job"", ""labels"": [ { ""text"": ""Spark Job"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_an_sql_to_spark"", ""labels"": [ { ""text"": ""feeds"" } ] } ] } ], ""edges"": [] }, { ""id"": ""central_services"", ""labels"": [ { ""text"": ""central_services"" } ], ""children"": [ { ""id"": ""management"", ""labels"": [ { ""text"": ""management"" } ], ""children"": [ { ""id"": ""access_control_iam"", ""labels"": [ { ""text"": ""Access Control (IAM)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""access_policies"", ""labels"": [ { ""text"": ""Access Policies (Dataplex)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""reservation_bq"", ""labels"": [ { ""text"": ""Reservation (BigQuery)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""discovery"", ""labels"": [ { ""text"": ""discovery"" } ], ""children"": [ { ""id"": ""catalog_dataplex"", ""labels"": [ { ""text"": ""Catalog (Dataplex)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""search_ui"", ""labels"": [ { ""text"": ""Search UI (Cloud Run)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""docs_drive"", ""labels"": [ { ""text"": ""Docs (Drive)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_searchui_queries_catalog"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_searchui_links_docs"", ""labels"": [ { ""text"": ""links"" } ] } ] } ], ""edges"": [] }, { ""id"": ""producers"", ""labels"": [ { ""text"": ""producers"" } ], ""children"": [ { ""id"": ""data_domain_1"", ""labels"": [ { ""text"": ""data_domain_1"" } ], ""children"": [ { ""id"": ""data_sources_dd1"", ""labels"": [ { ""text"": ""data_sources_dd1"" } ], ""children"": [ { ""id"": ""inventory_spanner"", ""labels"": [ { ""text"": ""Inventory (Spanner)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_product_dd1"", ""labels"": [ { ""text"": ""data_product_dd1"" } ], ""children"": [ { ""id"": ""native_storage1"", ""labels"": [ { ""text"": ""Native Storage (BQ)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""auth_dataset1"", ""labels"": [ { ""text"": ""Auth Dataset (BQ)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_native1_to_auth1"", ""labels"": [ { ""text"": ""derives"" } ] } ] } ], ""edges"": [ { ""id"": ""e_inv_to_native1"", ""labels"": [ { ""text"": ""loads"" } ] } ] }, { ""id"": ""data_domain_2"", ""labels"": [ { ""text"": ""data_domain_2"" } ], ""children"": [ { ""id"": ""curation_mgmt"", ""labels"": [ { ""text"": ""Curation & Management"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_sources_dd2"", ""labels"": [ { ""text"": ""data_sources_dd2"" } ], ""children"": [ { ""id"": ""raw_data"", ""labels"": [ { ""text"": ""Raw Data (CS)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_product_dd2"", ""labels"": [ { ""text"": ""data_product_dd2"" } ], ""children"": [ { ""id"": ""biglake_table"", ""labels"": [ { ""text"": ""BigLake Table (CS)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""native_storage2"", ""labels"": [ { ""text"": ""Native Storage (BQ)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""auth_dataset2"", ""labels"": [ { ""text"": ""Auth Dataset (BQ)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""read_api2"", ""labels"": [ { ""text"": ""Read API (BQ)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_biglake2_to_native2"", ""labels"": [ { ""text"": ""exports"" } ] }, { ""id"": ""e_native2_to_auth2"", ""labels"": [ { ""text"": ""derives"" } ] }, { ""id"": ""e_native2_to_readapi2"", ""labels"": [ { ""text"": ""exposes"" } ] } ] } ], ""edges"": [ { ""id"": ""e_raw_to_biglake2"", ""labels"": [ { ""text"": ""curates"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_auth1_to_sql_reports"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_auth1_to_sql_ds"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_auth1_to_sql_an"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_auth2_to_sql_reports"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_auth2_to_sql_ds"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_auth2_to_sql_an"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_readapi2_to_sql_reports"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_readapi2_to_sql_ds"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_readapi2_to_sql_an"", ""labels"": [ { ""text"": ""queries"" } ] } ], ""data"": {} } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/design-self-service-data-platform-data-mesh,"Design a self-service data platform for a data mesh bookmark_border Last reviewed 2024-09-03 UTC In a data mesh, a self-service data platform enables users to generate value from data by enabling them to autonomously build, share, and use data products. To fully realize these benefits, we recommend that your self-service data platform provide the capabilities described in this document. This document is part of a series which describes how to implement a data mesh on Google Cloud. It assumes that you have read and are familiar with the concepts described in Build a modern, distributed Data Mesh with Google Cloud and Architecture and functions in a data mesh. The series has the following parts: Architecture and functions in a data mesh Design a self-service data platform for a data mesh (this document) Build data products in a data mesh Discover and consume data products in a data mesh Data platform teams typically create central self-service data platforms, as described in this document. This team builds the solutions and components that domain teams (both data producers and data consumers) can use to both create and consume data products. Domain teams represent functional parts of a data mesh. By building these components, the data platform team enables a smooth development experience and reduces the complexity of building, deploying, and maintaining data products that are secure and interoperable. Ultimately, the data platform team should allow domain teams to move faster. They help increase the efficiency of domain teams by providing those teams with a limited set of tools that address their needs. In providing these tools, the data platform team removes the burden of having the domain team build and source these tools themselves. The tooling choices should be customizable to different needs and not force an inflexible way of working on the data domain teams. The data platform team shouldn't focus on building custom solutions for data pipeline orchestrators or for continuous integration and continuous deployment (CI/CD) systems. Solutions such as CI/CD systems are readily available as managed cloud services, for example, Cloud Build. Using managed cloud services can reduce operational overheads for the data platform team and let them focus on the specific needs of the data domain teams as the users of the platform. With reduced operational overhead, the data platform team can focus more time on addressing the specific needs of the data domain teams. Architecture The following diagram illustrates the architecture components of a self-service data platform. The diagram also shows how these components can support teams as they develop and consume data products across the data mesh. Self-service data platform components, as described in following text. As shown in the preceding diagram, the self-service data platform provides the following: Platform solutions: These solutions consist of composable components for provisioning Google Cloud projects and resources, which users select and assemble in different combinations to meet their specific requirements. Instead of directly interacting with the components, users of the platform can interact with platform solutions to help them to achieve a specific goal. Data domain teams should design platform solutions to solve common pain-points and friction areas that cause slowdowns in data product development and consumption. For example, data domain teams onboarding onto the data mesh can use an infrastructure-as-code (IaC) template. Using IaC templates lets them quickly create a set of Google Cloud projects with standard Identity and Access Management (IAM) permissions, networking, security policies, and relevant Google Cloud APIs enabled for data product development. We recommend that each solution is accompanied with documentation such as ""how to get started"" guidance and code samples. Data platform solutions and their components must be secure and compliant by default. Common services: These services provide data product discoverability, management, sharing, and observability. These services facilitate data consumers' trust in data products, and are an effective way for data producers to alert data consumers to issues with their data products. Data platform solutions and common services might include the following: IaC templates to set up foundational data product development workspace environments, which include the following: IAM Logging and monitoring Networking Security and compliance guardrails Resource tagging for billing attribution Data product storage, transformation, and publishing Data product registration, cataloging, and metadata tagging IaC templates which follow organizational security guardrails and best practices that can be used to deploy Google Cloud resources into existing data product development workspaces. Application and data pipeline templates that can be used to bootstrap new projects or used as reference for existing projects. Examples of such templates include the following: Usage of common libraries and frameworks Integration with platform logging, monitoring, and observability tooling Build and test tooling Configuration management Packaging and CI/CD pipelines for deployment Authentication, deployment, and management of credentials Common services to provide data product observability and governance which can include the following: Uptime checks to show the overall state of data products. Custom metrics to give helpful indicators about data products. Operational support by the central team such that data consumer teams are alerted of changes in data products they use. Product scorecards to show how data products are performing. A metadata catalog for discovering data products. A centrally defined set of computational policies that can be applied globally across the data mesh. A data marketplace to facilitate data sharing across domain teams. Create platform components and solutions using IaC templates discusses the advantages of IaC templates to expose and deploy data products. Provide common services discusses why it's helpful to provide domain teams with common infrastructure components that have been built and are managed by the data platform team. Create platform components and solutions using IaC templates The goal of data platform teams is to set up self-service data platforms to get more value from data. To build these platforms, they create and provide domain teams with vetted, secure, and self-serviceable infrastructure templates. Domain teams use these templates to deploy their data development and data consumption environments. IaC templates help data platform teams achieve that goal and enable scale. Using vetted and trusted IaC templates simplifies the resource deployment process for domain teams by allowing those teams to reuse existing CI/CD pipelines. This approach lets domain teams quickly get started and become productive within the data mesh. IaC templates can be created using an IaC tool. Although there are multiple IaC tools, including Cloud Config Connector, Pulumi, Chef, and Ansible, this document provides examples for Terraform-based IaC tools. Terraform is an open source IaC tool that allows the data platform team to efficiently create composable platform components and solutions for Google Cloud resources. Using Terraform, the data platform team writes code that specifies the chosen end-state and lets the tool figure out how to achieve that state. This declarative approach lets the data platform team treat infrastructure resources as immutable artifacts for deployment across environments. It also helps to reduce the risk of inconsistencies arising between deployed resources and the declared code in source control (referred to as configuration drift). Configuration drift caused by ad hoc and manual changes to infrastructure hinders safe and repeatable deployment of IaC components into production environments. Common IaC templates for composable platform components include using Terraform modules for deploying resources such as a BigQuery dataset, Cloud Storage bucket, or Cloud SQL database. Terraform modules can be combined into end-to-end solutions for deploying complete Google Cloud projects, including relevant resources deployed using the composable modules. Example Terraform modules can be found in the Terraform blueprints for Google Cloud. Each Terraform module should by default satisfy security guardrails and compliance policies that your organization uses. These guardrails and policies can also be expressed as code and be automated using automated compliance verification tooling such as Google Cloud policy validation tool. Your organization should continuously test the platform-provided Terraform modules, using the same automated compliance guardrails that it uses to promote changes into production. To make IaC components and solutions discoverable and consumable for domain teams that have minimal experience with Terraform, we recommend that you use services such as Service Catalog. Users who have significant customization requirements should be allowed to create their own deployment solutions from the same composable Terraform templates used by existing solutions. When using Terraform, we recommend that you follow the Google Cloud best-practices as outlined in Best practices for using Terraform. To illustrate how Terraform can be used to create platform components, the following sections discuss examples of how Terraform can be used to expose consumption interfaces and to consume a data product. Expose a consumption interface A consumption interface for a data product is a set of guarantees on the data quality and operational parameters provided by the data domain team to enable other teams to discover and use their data products. Each consumption interface also includes a product support model and product documentation. A data product may have different types of consumption interfaces, such as APIs or streams, as described in Build data products in a data mesh. The most common consumption interface might be a BigQuery authorized dataset, authorized view, or authorized function. This interface exposes a read-only virtual table, which is expressed as a query into the data mesh. The interface does not grant reader permissions to directly access the underlying data. Google provides an example Terraform module for creating authorized views without granting teams permissions to the underlying authorized datasets. The following code from this Terraform module grants these IAM permissions on the dataset_id authorized view: module ""add_authorization"" { source = ""terraform-google-modules/bigquery/google//modules/authorization"" version = ""~> 4.1"" dataset_id = module.dataset.bigquery_dataset.dataset_id project_id = module.dataset.bigquery_dataset.project roles = [ { role           = ""roles/bigquery.dataEditor"" group_by_email = ""ops@mycompany.com"" } ] authorized_views = [ { project_id = ""view_project"" dataset_id = ""view_dataset"" table_id   = ""view_id"" } ] authorized_datasets = [ { project_id = ""auth_dataset_project"" dataset_id = ""auth_dataset"" } ] } If you need to grant users access to multiple views, granting access to each authorized view can be both time consuming and harder to maintain. Instead of creating multiple authorized views, you can use an authorized dataset to automatically authorize any views created in the authorized dataset. Consume a data product For most analytics use cases, consumption patterns are determined by the application that the data is being used in. The main use of a centrally provided consumption environment is for data exploration before the data is used within the consuming application. As discussed in Discover and consume products in a data mesh, SQL is the most commonly used method for querying data products. For this reason, the data platform should provide data consumers with a SQL application for exploration of the data. Depending on the analytics use case, you may be able to use Terraform to deploy the consumption environment for data consumers. For example, data science is a common use case for data consumers. You can use Terraform to deploy Vertex AI user-managed notebooks to be used as a data science development environment. From the data science notebooks, data consumers can use their credentials to sign in to the data mesh to explore data to which they have access and develop ML models based on this data. To learn how to use Terraform to deploy and help to secure a notebook environment on Google Cloud, see Build and deploy generative AI and machine learning models in an enterprise.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_platform"", ""labels"": [ { ""text"": ""gcp_platform"" } ], ""children"": [ { ""id"": ""common_services"", ""labels"": [ { ""text"": ""common_services"" } ], ""children"": [ { ""id"": ""metadata_discovery"", ""labels"": [ { ""text"": ""metadata_discovery"" } ], ""children"": [ { ""id"": ""data_catalog_tag_template"", ""labels"": [ { ""text"": ""Data Catalog Tag Template"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog_lineage"", ""labels"": [ { ""text"": ""Data Catalog Lineage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataplex_logical_org"", ""labels"": [ { ""text"": ""Dataplex Logical Org"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_tag_lineage"", ""labels"": [ { ""text"": ""enriches"" } ] }, { ""id"": ""edge_dataplex_tag"", ""labels"": [ { ""text"": ""organizes"" } ] } ] }, { ""id"": ""central_policy_management"", ""labels"": [ { ""text"": ""central_policy_management"" } ], ""children"": [ { ""id"": ""dataplex_policy_management"", ""labels"": [ { ""text"": ""Dataplex Policy Management"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog_policy_tag"", ""labels"": [ { ""text"": ""Data Catalog Policy Tag"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_policy_to_tag"", ""labels"": [ { ""text"": ""manages"" } ] } ] }, { ""id"": ""operations_observability"", ""labels"": [ { ""text"": ""operations_observability"" } ], ""children"": [ { ""id"": ""cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataplex_data_quality"", ""labels"": [ { ""text"": ""Dataplex Data Quality"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_billing"", ""labels"": [ { ""text"": ""Cloud Billing"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_logging_to_monitoring"", ""labels"": [ { ""text"": ""feeds"" } ] }, { ""id"": ""edge_data_quality_to_monitoring"", ""labels"": [ { ""text"": ""alerts"" } ] }, { ""id"": ""edge_billing_to_logging"", ""labels"": [ { ""text"": ""records"" } ] } ] }, { ""id"": ""data_marketplace"", ""labels"": [ { ""text"": ""data_marketplace"" } ], ""children"": [ { ""id"": ""analytics_hub"", ""labels"": [ { ""text"": ""Analytics Hub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_policy_governs_metadata"", ""labels"": [ { ""text"": ""governs"" } ] }, { ""id"": ""edge_observability_to_metadata"", ""labels"": [ { ""text"": ""observes"" } ] }, { ""id"": ""edge_marketplace_to_metadata"", ""labels"": [ { ""text"": ""publishes"" } ] } ] }, { ""id"": ""dev_env_template"", ""labels"": [ { ""text"": ""dev_env_template"" } ], ""children"": [ { ""id"": ""data_operations"", ""labels"": [ { ""text"": ""data_operations"" } ], ""children"": [ { ""id"": ""service_accounts"", ""labels"": [ { ""text"": ""Service Accounts"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""default_iam_groups"", ""labels"": [ { ""text"": ""Default IAM Groups"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dev_cloud_logging"", ""labels"": [ { ""text"": ""Cloud Logging (Dev)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dev_cloud_monitoring"", ""labels"": [ { ""text"": ""Cloud Monitoring (Dev)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_sa_to_iam"", ""labels"": [ { ""text"": ""defines"" } ] }, { ""id"": ""edge_sa_to_logging"", ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""edge_logging_to_monitoring_dev"", ""labels"": [ { ""text"": ""monitors"" } ] } ] }, { ""id"": ""ml_operations"", ""labels"": [ { ""text"": ""ml_operations"" } ], ""children"": [ { ""id"": ""vertex_ai_mlop"", ""labels"": [ { ""text"": ""Vertex AI"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_storage"", ""labels"": [ { ""text"": ""data_storage"" } ], ""children"": [ { ""id"": ""cloud_storage_ds"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery_ds"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigtable_store"", ""labels"": [ { ""text"": ""Cloud Bigtable"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_store"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_processing"", ""labels"": [ { ""text"": ""data_processing"" } ], ""children"": [ { ""id"": ""dataflow_proc"", ""labels"": [ { ""text"": ""Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataproc_proc"", ""labels"": [ { ""text"": ""Dataproc"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_interface"", ""labels"": [ { ""text"": ""data_interface"" } ], ""children"": [ { ""id"": ""bigquery_interface"", ""labels"": [ { ""text"": ""BigQuery Interface"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""vertex_ai_interface"", ""labels"": [ { ""text"": ""Vertex AI Interface"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_run_interface"", ""labels"": [ { ""text"": ""Cloud Run"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_governance"", ""labels"": [ { ""text"": ""data_governance"" } ], ""children"": [ { ""id"": ""dataplex_governance"", ""labels"": [ { ""text"": ""Dataplex Governance"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog_governance"", ""labels"": [ { ""text"": ""Data Catalog"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] }, { ""id"": ""data_products"", ""labels"": [ { ""text"": ""data_products"" } ], ""children"": [ { ""id"": ""data_product_1"", ""labels"": [ { ""text"": ""data_product_1"" } ], ""children"": [ { ""id"": ""pubsub_ingest1"", ""labels"": [ { ""text"": ""Pub/Sub Ingest"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery_storage1"", ""labels"": [ { ""text"": ""BigQuery Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery_views1"", ""labels"": [ { ""text"": ""Authorized Views"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog_tags1"", ""labels"": [ { ""text"": ""Data Catalog Tags"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_ingest1_to_storage1"", ""labels"": [ { ""text"": ""writes"" } ] }, { ""id"": ""edge_storage1_to_views1"", ""labels"": [ { ""text"": ""exposes"" } ] }, { ""id"": ""edge_metadata1_to_views1"", ""labels"": [ { ""text"": ""tags"" } ] } ] }, { ""id"": ""data_product_2"", ""labels"": [ { ""text"": ""data_product_2"" } ], ""children"": [ { ""id"": ""dataproc_ingest2"", ""labels"": [ { ""text"": ""Dataproc Serverless"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_storage_ext2"", ""labels"": [ { ""text"": ""Cloud Storage (External Table)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery_views2"", ""labels"": [ { ""text"": ""Authorized Views"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog_tags2"", ""labels"": [ { ""text"": ""Data Catalog Tags"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_ingest2_to_storage2"", ""labels"": [ { ""text"": ""writes"" } ] }, { ""id"": ""edge_storage2_to_views2"", ""labels"": [ { ""text"": ""exposes"" } ] }, { ""id"": ""edge_metadata2_to_views2"", ""labels"": [ { ""text"": ""tags"" } ] } ] }, { ""id"": ""data_product_3"", ""labels"": [ { ""text"": ""data_product_3"" } ], ""children"": [ { ""id"": ""vertex_ai_pipeline3"", ""labels"": [ { ""text"": ""Vertex AI Pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""vertex_ai_registry3"", ""labels"": [ { ""text"": ""Model & Feature Store"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""vertex_ai_prediction3"", ""labels"": [ { ""text"": ""Prediction API"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog_tags3"", ""labels"": [ { ""text"": ""Data Catalog Tags"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_pipeline3_to_registry3"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""edge_registry3_to_prediction3"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""edge_metadata3_to_prediction3"", ""labels"": [ { ""text"": ""tags"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_data_ops_to_observability"", ""labels"": [ { ""text"": ""extends"" } ] }, { ""id"": ""edge_governance_to_policy"", ""labels"": [ { ""text"": ""complies"" } ] }, { ""id"": ""edge_dev_template_to_metadata"", ""labels"": [ { ""text"": ""leverages"" } ] }, { ""id"": ""edge_dev_template_to_dp1"", ""labels"": [ { ""text"": ""instantiates"" } ] }, { ""id"": ""edge_dev_template_to_dp2"", ""labels"": [ { ""text"": ""instantiates"" } ] }, { ""id"": ""edge_dev_template_to_dp3"", ""labels"": [ { ""text"": ""instantiates"" } ] } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/design-self-service-data-platform-data-mesh#data_observability,"Data observability Each data domain should implement its own monitoring and alerting mechanisms, ideally using a standardized approach. Each domain can apply the monitoring practices described in Concepts in service monitoring, making the necessary adjustments to the data domains. Observability is a large topic, and is outside of the scope of this document. This section only addresses patterns which are useful in data mesh implementations. For products with multiple data consumers, providing timely information to each consumer about the status of the product can become an operational burden. Basic solutions, such as manually managed email distributions, are typically prone to error. They can be helpful for notifying consumers of planned outages, upcoming product launches, and deprecations, but they don't provide real-time operational awareness. Central services can play an important role in monitoring the health and quality of the products in the data mesh. Although not a prerequisite for a successful implementation of the data mesh, implementing observability features can improve satisfaction of the data producers and consumers, and reduce overall operational and support costs. The following diagram shows an architecture of data mesh observability based on Cloud Monitoring. Data mesh observability. The following sections describe the components shown in the diagram, which are as follows: Uptime checks to show the overall state of data products. Custom metrics to give helpful indicators about data products. Operational support by the central data platform team to alert data consumers of changes in the data products that they use. Product scorecards and dashboards to show how data products are performing. Uptime checks Data products can create simple custom applications that implement uptime checks. These checks can serve as high-level indicators of the overall state of the product. For example, if the data product team discovers a sudden drop in data quality of its product, the team can mark that product unhealthy. Uptime checks that are close to real time are especially important to data consumers who have derived products that rely on the constant availability of the data in the upstream data product. Data producers should build their uptime checks to include checking their upstream dependencies, thus providing an accurate picture of the health of their product to their data consumers. Data consumers can include product uptime checks into their processing. For example, a composer job that generates a report based on the data provided by a data product can, as the first step, validate whether the product is in the ""running"" state. We recommend that your uptime check application returns a structured payload in the message body of its HTTP response. This structured payload should indicate whether there's a problem, the root cause of the problem in human readable form, and if possible, the estimated time to restore the service. This structured payload can also provide more fine-grained information about the state of the product. For example, it can contain the health information for each of the views in the authorized dataset exposed as a product. Custom metrics Data products can have various custom metrics to measure their usefulness. Data producer teams can publish these custom metrics to their designated domain-specific Google Cloud projects. To create a unified monitoring experience across all data products, a central data mesh monitoring project can be given access to those domain-specific projects. Each type of data product consumption interface has different metrics to measure its usefulness. Metrics can also be specific to the business domain. For example, the metrics for BigQuery tables exposed through views or through the Storage Read API can be as follows: The number of rows. Data freshness (expressed as the number of seconds before the measurement time). The data quality score. The data that's available. This metric can indicate that the data is available for querying. An alternative is to use the uptime checks mentioned earlier in this document. These metrics can be viewed as service level indicators (SLI) for a particular product. For data streams (implemented as Pub/Sub topics), this list can be the standard Pub/Sub metrics, which are available through topics. Operational support by the central data platform team The central data platform team can expose custom dashboards to display different levels of details to the data consumers. A simple status dashboard that lists the products in the data mesh and uptime status for those products can help answer multiple end-user requests. The central team can also serve as a notification distribution hub to notify data consumers about various events in the data products they use. Typically, this hub is made by creating alerting policies. Centralizing this function can reduce the work that must be done by each data producer team. Creating these policies doesn't require knowledge of the data domains and should help avoid bottlenecks in data consumption. An ideal end state for data mesh monitoring is for the data product tag template to expose the SLIs and service-level objectives (SLOs) that the product supports when the product becomes available. The central team can then automatically deploy the corresponding alerting using service monitoring with the Monitoring API. Product scorecards As part of the central governance agreement, the four functions in a data mesh can define the criteria to create scorecards for data products. These scorecards can become an objective measurement of data product performance. Many of the variables used to calculate the scorecards are the percentage of time that data products are meeting their SLO. Useful criteria can be the percentage of uptime, average data quality scores, and percentage of products with data freshness that does not fall below a threshold. To calculate these metrics automatically using Monitoring Query Language (MQL), the custom metrics and the results of the uptime checks from the central monitoring project should be sufficient.","{ ""id"": ""root"", ""children"": [ { ""id"": ""central_metrics"", ""labels"": [ { ""text"": ""central_metrics"" } ], ""children"": [ { ""id"": ""uptime_checks"", ""labels"": [ { ""text"": ""Uptime checks"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""alerts"", ""labels"": [ { ""text"": ""Alerts"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dashboard"", ""labels"": [ { ""text"": ""Dashboard"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""producers"", ""labels"": [ { ""text"": ""producers"" } ], ""children"": [ { ""id"": ""domain1_metrics"", ""labels"": [ { ""text"": ""domain1_metrics"" } ], ""children"": [ { ""id"": ""monitor1"", ""labels"": [ { ""text"": ""Monitor"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""custom_metrics1"", ""labels"": [ { ""text"": ""Custom metrics"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""uptime_provider1"", ""labels"": [ { ""text"": ""Uptime provider"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_monitor1_custom"", ""labels"": [ { ""text"": ""collects"" } ] } ] }, { ""id"": ""domain1_resources"", ""labels"": [ { ""text"": ""domain1_resources"" } ], ""children"": [ { ""id"": ""bigquery1"", ""labels"": [ { ""text"": ""Data BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataflow1"", ""labels"": [ { ""text"": ""Processes Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""domain2_metrics"", ""labels"": [ { ""text"": ""domain2_metrics"" } ], ""children"": [ { ""id"": ""monitor2"", ""labels"": [ { ""text"": ""Monitor"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""custom_metrics2"", ""labels"": [ { ""text"": ""Custom metrics"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""uptime_provider2"", ""labels"": [ { ""text"": ""Uptime provider"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_monitor2_custom"", ""labels"": [ { ""text"": ""collects"" } ] } ] }, { ""id"": ""domain2_resources"", ""labels"": [ { ""text"": ""domain2_resources"" } ], ""children"": [ { ""id"": ""bigquery2"", ""labels"": [ { ""text"": ""Data BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataflow2"", ""labels"": [ { ""text"": ""Processes Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_bq1_monitor"", ""labels"": [ { ""text"": ""exports metrics"" } ] }, { ""id"": ""e_flow1_monitor"", ""labels"": [ { ""text"": ""exports metrics"" } ] }, { ""id"": ""e_bq2_monitor"", ""labels"": [ { ""text"": ""exports metrics"" } ] }, { ""id"": ""e_flow2_monitor"", ""labels"": [ { ""text"": ""exports metrics"" } ] } ] }, { ""id"": ""data_consumer1"", ""labels"": [ { ""text"": ""data_consumer1"" } ], ""children"": [ { ""id"": ""analysts"", ""labels"": [ { ""text"": ""Analysts"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_consumer2"", ""labels"": [ { ""text"": ""data_consumer2"" } ], ""children"": [ { ""id"": ""ml_engineers"", ""labels"": [ { ""text"": ""ML engineers"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_consumer3"", ""labels"": [ { ""text"": ""data_consumer3"" } ], ""children"": [ { ""id"": ""support_user"", ""labels"": [ { ""text"": ""Support"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""composer_support"", ""labels"": [ { ""text"": ""Cloud Composer"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_composer_support"", ""labels"": [ { ""text"": ""assists"" } ] } ] } ], ""edges"": [ { ""id"": ""e_alerts_analysts"", ""labels"": [ { ""text"": ""notifies"" } ] }, { ""id"": ""e_alerts_ml"", ""labels"": [ { ""text"": ""notifies"" } ] }, { ""id"": ""e_alerts_support"", ""labels"": [ { ""text"": ""notifies"" } ] }, { ""id"": ""e_dashboard_composer"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_custom1_dashboard"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_uptime1_checks"", ""labels"": [ { ""text"": ""reports"" } ] }, { ""id"": ""e_custom2_dashboard"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_uptime2_checks"", ""labels"": [ { ""text"": ""reports"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/build-data-products-data-mesh,"Build data products in a data mesh bookmark_border Last reviewed 2024-09-03 UTC To ensure that the use cases of data consumers are met, it's essential that data products in a data mesh are designed and built with care. The design of a data product starts with the definition of how data consumers would use that product, and how that product then gets exposed to consumers. Data products in a data mesh are built on top of a datastore (for example, a domain data warehouse or data lake). When you create data products in a data mesh, there are some key factors that we recommend you consider throughout this process. These considerations are described in this document. This document is part of a series which describes how to implement a data mesh on Google Cloud. It assumes that you have read and are familiar with the concepts described in Architecture and functions in a data mesh and Build a modern, distributed Data Mesh with Google Cloud. The series has the following parts: Architecture and functions in a data mesh Design a self-service data platform for a data mesh Build data products in a data mesh (this document) Discover and consume data products in a data mesh When creating data products from a domain data warehouse, we recommend that data producers carefully design analytical (consumption) interfaces for those products. These consumption interfaces are a set of guarantees on the data quality and operational parameters, along with a product support model and product documentation. The cost of changing consumption interfaces is usually high because of the need for both the data producer and potentially multiple data consumers to change their consuming processes and applications. Given that the data consumers are most likely to be in organizational units which are separate to that of the data producers, coordinating the changes can be difficult. The following sections provide background information on what you must consider when creating a domain warehouse, defining consumption interfaces, and exposing those interfaces to data consumers. Create a domain data warehouse There's no fundamental difference between building a standalone data warehouse and building a domain data warehouse from which the data producer team creates data products. The only real difference between the two is that the latter exposes a subset of its data through the consumption interfaces. In many data warehouses, the raw data ingested from operational data sources goes through the process of enrichment and data quality verification (curation). In Dataplex Universal Catalog-managed data lakes, curated data typically is stored in designated curated zones. When curation is complete, a subset of the data should be ready for external-to-the-domain consumption through several types of interfaces. To define those consumption interfaces, an organization should provide a set of tools to domain teams who are new to adopting a data mesh approach. These tools let data producers create new data products on a self-service basis. For recommended practices, see Design a self-service data platform. Additionally, data products must meet centrally defined data governance requirements. These requirements affect data quality, data availability, and lifecycle management. Because these requirements build the trust of data consumers in the data products and encourage data product usage, the benefits of implementing these requirements are worth the effort in supporting them. Define consumption interfaces We recommend that data producers use multiple types of interfaces, instead of defining just one or two. Each interface type in data analytics has advantages and disadvantages, and there's no single type of interface that excels at everything. When data producers assess the suitability of each interface type, they must consider the following: Ability to perform the data processing needed. Scalability to support current and future data consumer use cases. Performance required by data consumers. Cost of development and maintenance. Cost of running the interface. Support by the languages and tools that your organization uses. Support for separation of storage and compute. For example, if the business requirement is to be able to run analytical queries over a petabyte-size dataset, then the only practical interface is a BigQuery view. But if the requirements are to provide near real-time streaming data, then a Pub/Sub-based interface is more appropriate. Many of these interfaces don't require you to copy or replicate existing data. Most of them also let you separate storage and compute, a critical feature of Google Cloud analytical tools. Consumers of data exposed through these interfaces process the data using the compute resources available to them. There's no need for data producers to do any additional infrastructure provisioning. There's a wide variety of consumption interfaces. The following interfaces are the most common ones used in a data mesh and are discussed in the following sections: Authorized views and functions Direct read APIs Data as streams Data access API Looker blocks Machine learning (ML) models The list of interfaces in this document is not exhaustive. There are also other options that you might consider for your consumption interfaces (for example, BigQuery sharing (formerly Analytics Hub)). However, these other interfaces are outside of the scope of this document. Authorized views and functions As much as possible, data products should be exposed through authorized views and authorized functions, including table-valued functions. Authorized datasets provide a convenient way to authorize several views automatically. Using authorized views prevents direct access to the base tables, and lets you optimize the underlying tables and queries against them, without affecting consumer use of these views. Consumers of this interface use SQL to query the data. The following diagram illustrates the use of authorized datasets as the consumption interface. Consumption interfaces. Authorized datasets and views help to enable easy versioning of interfaces. As shown in the following diagram, there are two primary versioning approaches that data producers can take: Dataset and view versioning. The approaches can be summarized as follows: Dataset versioning: In this approach, you version the dataset name. You don't version the views and functions inside the dataset. You keep the same names for the views and functions regardless of version. For example, the first version of a sales dataset is defined in a dataset named sales_v1 with two views, catalog and orders. For its second version, the sales dataset has been renamed sales_v2, and any previous views in the dataset keep their previous names but have new schemas. The second version of the dataset might also have new views added to it, or may remove any of the previous views. View versioning: In this approach, the views inside the dataset are versioned instead of the dataset itself. For example, the sales dataset keeps the name of sales regardless of version. However, the names of the views inside the dataset change to reflect each new version of the view (such as catalog_v1, catalog_v2, orders_v1, orders_v2, and orders_v3). The best versioning approach for your organization depends on your organization's policies and the number of views that are rendered obsolete with the update to the underlying data. Dataset versioning is best when a major product update is needed and most views must change. View versioning leads to fewer identically named views in different datasets, but can lead to ambiguities, for example, how to tell if a join between datasets works correctly. A hybrid approach can be a good compromise. In a hybrid approach, compatible schema changes are allowed within a single dataset, and incompatible changes require a new dataset. BigLake table considerations Authorized views can be created not only on BigQuery tables, but also on BigLake tables. BigLake tables let consumers query the data stored in Cloud Storage by using the BigQuery SQL interface. BigLake tables support fine-grained access control without the need for data consumers to have read permissions for the underlying Cloud Storage bucket. Data producers must consider the following for BigLake tables: The design of the file formats and the data layout influences the performance of the queries. Column-based formats, for example, Parquet or ORC, generally perform much better for analytic queries than JSON or CSV formats. A Hive partitioned layout lets you prune partitions and speeds up queries which use partitioning columns. The number of files and the preferred query performance for the file size must also be taken into account in the design stage. If queries using BigLake tables don't meet service-level agreement (SLA) requirements for the interface and can't be tuned, then we recommend the following actions: For data that must be exposed to the data consumer, convert that data to BigQuery storage. Redefine the authorized views to use the BigQuery tables. Generally, this approach does not cause any disruption to the data consumers, or require any changes to their queries. The queries in BigQuery storage can be optimized using techniques that aren't possible with BigLake tables. For example, with BigQuery storage, consumers can query materialized views that have different partitioning and clustering than the base tables, and they can use the BigQuery BI Engine.","{ ""id"": ""root"", ""children"": [ { ""id"": ""data_domain"", ""labels"": [ { ""text"": ""data_domain"" } ], ""children"": [ { ""id"": ""operational_data"", ""labels"": [ { ""text"": ""operational_data"" } ], ""children"": [ { ""id"": ""inventory"", ""labels"": [ { ""text"": ""Inventory"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""raw_events"", ""labels"": [ { ""text"": ""Raw events"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_ingest"", ""labels"": [ { ""text"": ""data_ingest"" } ], ""children"": [ { ""id"": ""ingest_pipeline"", ""labels"": [ { ""text"": ""Ingest pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""curated_data"", ""labels"": [ { ""text"": ""curated_data"" } ], ""children"": [ { ""id"": ""base_tables"", ""labels"": [ { ""text"": ""Base tables"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""material_views"", ""labels"": [ { ""text"": ""Material views"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""biglake_tables"", ""labels"": [ { ""text"": ""BigLake tables"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""curated_storage"", ""labels"": [ { ""text"": ""Curated storage"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_biglake_storage"", ""labels"": [ { ""text"": ""store"" } ] } ] }, { ""id"": ""product_interface"", ""labels"": [ { ""text"": ""product_interface"" } ], ""children"": [ { ""id"": ""dataset_v1"", ""labels"": [ { ""text"": ""Authorized dataset v1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dataset_v2"", ""labels"": [ { ""text"": ""Authorized dataset v2"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_base_dataset1"", ""labels"": [ { ""text"": ""derive"" } ] }, { ""id"": ""e_material_dataset1"", ""labels"": [ { ""text"": ""derive"" } ] }, { ""id"": ""e_biglake_dataset2"", ""labels"": [ { ""text"": ""derive"" } ] }, { ""id"": ""e_ingest_biglake"", ""labels"": [ { ""text"": ""load"" } ] }, { ""id"": ""e_raw_ingest"", ""labels"": [ { ""text"": ""stream"" } ] }, { ""id"": ""e_inventory_base"", ""labels"": [ { ""text"": ""replicate"" } ] } ] }, { ""id"": ""consumers"", ""labels"": [ { ""text"": ""consumers"" } ], ""children"": [ { ""id"": ""reports_group"", ""labels"": [ { ""text"": ""reports_group"" } ], ""children"": [ { ""id"": ""bi_tools"", ""labels"": [ { ""text"": ""BI tools"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""sql_engine_reports"", ""labels"": [ { ""text"": ""SQL engine"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_bi_sql"", ""labels"": [ { ""text"": ""query"" } ] } ] }, { ""id"": ""analytics_group"", ""labels"": [ { ""text"": ""analytics_group"" } ], ""children"": [ { ""id"": ""data_analytics"", ""labels"": [ { ""text"": ""Data analytics"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""sql_engine_analytics"", ""labels"": [ { ""text"": ""SQL engine"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_data_sql"", ""labels"": [ { ""text"": ""process"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_dataset1_report"", ""labels"": [ { ""text"": ""provide data"" } ] }, { ""id"": ""e_dataset2_analytics"", ""labels"": [ { ""text"": ""provide data"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/build-data-products-data-mesh,"Direct read APIs Although we don't generally recommend that data producers give data consumers direct read access to the base tables, it might occasionally be practical to allow such access for reasons such as performance and cost. In such cases, extra care should be taken to ensure that the table schema is stable. There are two ways to directly access data in a typical warehouse. Data producers can either use the BigQuery Storage Read API, or the Cloud Storage JSON or XML APIs. The following diagram illustrates two examples of consumers using these APIs. One is a machine learning (ML) use case, and the other is a data processing job. ML and data processing use cases, explained in following text. Versioning a direct-read interface is complex. Typically, data producers must create another table with a different schema. They must also maintain two versions of the table, until all the data consumers of the deprecated version migrate to the new one. If the consumers can tolerate the disruption of rebuilding the table and switching to the new schema, then it's possible to avoid the data duplication. In cases where schema changes can be backward compatible, the migration of the base table can be avoided. For example, you don't have to migrate the base table if only new columns are added and the data in these columns is backfilled for all the rows. The following is a summary of the differences between the Storage Read API and Cloud Storage API. In general, whenever possible, we recommend that data producers use BigQuery API for analytical applications. Storage Read API: Storage Read API can be used to read data in BigQuery tables and to read BigLake tables. This API supports filtering and fine-grained access control, and can be a good option for stable data analytics or ML consumers. Cloud Storage API: Data producers might need to share a particular Cloud Storage bucket directly with data consumers. For example, data producers can share the bucket if data consumers can't use the SQL interface for some reason, or the bucket has data formats that aren't supported by Storage Read API. In general, we don't recommend that data producers allow direct access through the storage APIs because direct access doesn't allow for filtering and fine-grained access control. However, the direct access approach can be a viable choice for stable, small-sized (gigabytes) datasets. Allowing Pub/Sub access to the bucket gives data consumers an easy way to copy the data into their projects and process it there. In general, we don't recommend data copying if it can be avoided. Multiple copies of data increase storage cost, and add to the maintenance and lineage tracking overhead.","{ ""id"": ""root"", ""children"": [ { ""id"": ""consumers"", ""labels"": [ { ""text"": ""consumers"" } ], ""children"": [ { ""id"": ""ml_modeling"", ""labels"": [ { ""text"": ""ml_modeling"" } ], ""children"": [ { ""id"": ""notebook"", ""labels"": [ { ""text"": ""Notebook"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""analytics_group"", ""labels"": [ { ""text"": ""analytics_group"" } ], ""children"": [ { ""id"": ""data_analytics"", ""labels"": [ { ""text"": ""Data analytics"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [] }, { ""id"": ""data_domain"", ""labels"": [ { ""text"": ""data_domain"" } ], ""children"": [ { ""id"": ""operational_source_data"", ""labels"": [ { ""text"": ""operational_source_data"" } ], ""children"": [ { ""id"": ""inventory"", ""labels"": [ { ""text"": ""Inventory"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""raw_events"", ""labels"": [ { ""text"": ""Raw events"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_ingest"", ""labels"": [ { ""text"": ""data_ingest"" } ], ""children"": [ { ""id"": ""ingest_pipeline"", ""labels"": [ { ""text"": ""Ingest pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""curated_data"", ""labels"": [ { ""text"": ""curated_data"" } ], ""children"": [ { ""id"": ""base_tables"", ""labels"": [ { ""text"": ""Base tables"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""material_views"", ""labels"": [ { ""text"": ""Material views"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""biglake_tables"", ""labels"": [ { ""text"": ""BigLake tables"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""curated_storage"", ""labels"": [ { ""text"": ""Curated storage"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_bl_write_storage"", ""labels"": [ { ""text"": ""writes"" } ] } ] }, { ""id"": ""product_interface"", ""labels"": [ { ""text"": ""product_interface"" } ], ""children"": [ { ""id"": ""storage_read_api"", ""labels"": [ { ""text"": ""Storage Read API"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""storage_api"", ""labels"": [ { ""text"": ""Storage API"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_curated_srapi"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""e_curatedstorage_sapi"", ""labels"": [ { ""text"": ""provides"" } ] }, { ""id"": ""e_ingest_curated"", ""labels"": [ { ""text"": ""loads"" } ] }, { ""id"": ""e_raw_ingest"", ""labels"": [ { ""text"": ""streams"" } ] }, { ""id"": ""e_inventory_curated"", ""labels"": [ { ""text"": ""exports"" } ] } ] } ], ""edges"": [ { ""id"": ""e_sr_notebook"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""e_sr_dataanalytics"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""e_sa_notebook"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""e_sa_dataanalytics"", ""labels"": [ { ""text"": ""serves"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/build-data-products-data-mesh#data_as_streams,"Data as streams A domain can expose streaming data by publishing that data to a Pub/Sub topic. Subscribers who want to consume the data create subscriptions to consume the messages published to that topic. Each subscriber receives and consumes data independently. The following diagram shows an example of such data streams. Data streams to receive and consume data. In the diagram, the ingest pipeline reads raw events, enriches (curates) them, and saves this curated data to the analytical datastore (BigQuery base table). At the same time, the pipeline publishes the enriched events to a dedicated topic. This topic is consumed by multiple subscribers, each of whom may be potentially filtering these events to get only the ones relevant to them. The pipeline also aggregates and publishes event statistics to its own topic to be processed by another data consumer. The following are example use cases for Pub/Sub subscriptions: Enriched events, such as providing full customer profile information along with data on a particular customer order. Close-to-real-time aggregation notifications, such as total order statistics for the last 15 minutes. Business-level alerts, such as generating an alert if order volume dropped by 20% compared to a similar period on the previous day. Data change notifications (similar in concept to change data capture notifications), such as a particular order changes status. The data format that data producers use for Pub/Sub messages affects costs and how these messages are processed. For high-volume streams in a data mesh architecture, Avro or Protobuf formats are good options. If data producers use these formats, they can assign schemas to Pub/Sub topics. The schemas help to ensure that the consumers receive well-formed messages. Because a streaming data structure can be constantly changing, versioning of this interface requires coordination between the data producers and the data consumers. There are several common approaches data producers can take, which are as follows: A new topic is created every time the message structure changes. This topic often has an explicit Pub/Sub schema. Data consumers who need the new interface can start to consume the new data. The message version is implied by the name of the topic, for example, click_events_v1. Message formats are strongly typed. There's no variation on the message format between messages in the same topic. The disadvantage of this approach is that there might be data consumers who can't switch to the new subscription. In this case, the data producer must continue publishing events to all active topics for some time, and data consumers who subscribe to the topic must either deal with a gap in message flow, or de-duplicate the messages. Data is always published to the same topic. However, the structure of the message can change. A Pub/Sub message attribute (separate from the payload) defines the version of the message. For example, v=1.0. This approach removes the need to deal with gaps or duplicates; however, all data consumers must be ready to receive messages of a new type. Data producers also can't use Pub/Sub topic schemas for this approach. A hybrid approach. The message schema can have an arbitrary data section that can be used for new fields. This approach can provide a reasonable balance between having strongly typed data, and frequent and complex version changes.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_platform"", ""labels"": [ { ""text"": ""gcp_platform"" } ], ""children"": [ { ""id"": ""data_domain"", ""labels"": [ { ""text"": ""data_domain"" } ], ""children"": [ { ""id"": ""operational_source_data"", ""labels"": [ { ""text"": ""operational_source_data"" } ], ""children"": [ { ""id"": ""raw_events_pubsub"", ""labels"": [ { ""text"": ""Raw events"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_ingest"", ""labels"": [ { ""text"": ""data_ingest"" } ], ""children"": [ { ""id"": ""ingest_pipeline_dataflow"", ""labels"": [ { ""text"": ""Ingest pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""all_analytics_data"", ""labels"": [ { ""text"": ""all_analytics_data"" } ], ""children"": [ { ""id"": ""base_tables_bigquery"", ""labels"": [ { ""text"": ""Base tables"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""product_interface"", ""labels"": [ { ""text"": ""product_interface"" } ], ""children"": [ { ""id"": ""event_topic_pubsub"", ""labels"": [ { ""text"": ""Event topic"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""aggregated_topic_pubsub"", ""labels"": [ { ""text"": ""Aggregated topic"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_raw_to_ingest"", ""labels"": [ { ""text"": ""streams"" } ] }, { ""id"": ""e_ingest_to_base"", ""labels"": [ { ""text"": ""writes"" } ] }, { ""id"": ""e_ingest_to_event"", ""labels"": [ { ""text"": ""publishes enriched events"" } ] }, { ""id"": ""e_ingest_to_agg"", ""labels"": [ { ""text"": ""publishes aggregated events"" } ] } ] }, { ""id"": ""consumers"", ""labels"": [ { ""text"": ""consumers"" } ], ""children"": [ { ""id"": ""consumer1"", ""labels"": [ { ""text"": ""consumer1"" } ], ""children"": [ { ""id"": ""c1_event_subscription"", ""labels"": [ { ""text"": ""Event subscription"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""c1_data_analytics"", ""labels"": [ { ""text"": ""Data analytics"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_c1_streams"", ""labels"": [ { ""text"": ""streams"" } ] } ] }, { ""id"": ""consumer2"", ""labels"": [ { ""text"": ""consumer2"" } ], ""children"": [ { ""id"": ""c2_event_subscription"", ""labels"": [ { ""text"": ""Event subscription"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""c2_data_analytics"", ""labels"": [ { ""text"": ""Data analytics"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_c2_streams"", ""labels"": [ { ""text"": ""streams"" } ] } ] }, { ""id"": ""consumer3"", ""labels"": [ { ""text"": ""consumer3"" } ], ""children"": [ { ""id"": ""c3_aggregated_subscription"", ""labels"": [ { ""text"": ""Aggregated subscription"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""c3_organization_alerts"", ""labels"": [ { ""text"": ""Organization alerts"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_c3_streams"", ""labels"": [ { ""text"": ""streams"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_event_to_sub1"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_event_to_sub2"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_agg_to_sub3"", ""labels"": [ { ""text"": ""publishes"" } ] } ] } ], ""edges"": [] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/discover-consume-data-products-data-mesh,"Discover and consume data products in a data mesh bookmark_border Last reviewed 2024-09-03 UTC We recommend that you design your data mesh to support a wide variety of use cases for data consumption. The most common data consumption use cases in an organization are described in this document. The document also discusses what information data consumers must consider when determining the right data product for their use case, and how they discover and use data products. Understanding these factors can help organizations to ensure that they have the right guidance and tooling in place to support data consumers. This document is part of a series which describes how to implement a data mesh on Google Cloud. It assumes that you have read and are familiar with the concepts described in Architecture and functions in a data mesh and Build a modern, distributed Data Mesh with Google Cloud. The series has the following parts: Architecture and functions in a data mesh Design a self-service data platform for a data mesh Build data products in a data mesh Discover and consume data products in a data mesh (this document) The design of a data consumption layer, specifically, how the data domain-based consumers use data products, depends on the data consumer requirements. As a prerequisite, it's assumed that consumers have a use case in mind. It's assumed that they have identified the data that they require, and can search the central data product catalog to find it. If that data is not in the catalog or is not in the preferred state (for example, if the interface is not appropriate, or the SLAs are insufficient), the consumer must contact the data producer. Alternatively, the consumer can contact the center of excellence (COE) for the data mesh for advice on which domain is the best suited to produce that data product. The data consumers can also ask how to make their request. If your organization is large, there should be a process to make data product requests in a self-service manner. Data consumers use data products through the applications that they run. The type of insights required drives the choice of design of the data-consuming application. When they develop the design of the application, the data consumer also identifies their preferred use of data products in the application. They establish the confidence that they need to have in the trustworthiness and reliability of that data. The data consumers can then establish a view on the data product interfaces and SLAs that the application requires. Data consumption use cases For data consumers to create data applications, sources could be one or more data products and, perhaps, the data from the data consumer's own domain. As described in Build data products in a data mesh, analytical data products could be made from data products which are based on various physical data repositories. Although data consumption can happen within the same domain, the most common consumption patterns are those that search for the right data product, regardless of domain, as the source for the application. When the right data product exists in another domain, the consumption pattern requires you to set up the subsequent mechanism for access and usage of the data across domains. The consumption of data products created in domains other than the consuming domain is discussed in Data consumption steps. Architecture The following diagram shows an example scenario in which consumers use data products through a range of interfaces, including authorized datasets and APIs. Data consumption scenarios, explained in following text. As shown in the preceding diagram, the data producer has exposed four data product interfaces: two BigQuery authorized datasets, a BigQuery dataset exposed by the BigQuery storage read API, and data access APIs hosted on Google Kubernetes Engine. In using the data products, data consumers use a range of applications that query or directly access the data resources within the data products. For this scenario, data consumers access data resources in one of two different ways based on their specific data access requirements. In the first way, Looker uses BigQuery SQL to query an authorized dataset. In the second way, Dataproc directly accesses a dataset through the BigQuery API and then processes that ingested data to train a machine learning (ML) model. The use of a data consumption application might not always result in a business intelligence (BI) report or a BI dashboard. Consumption of data from a domain can also result in ML models that further enrich analytical products, are used in data analysis, or are a part of operational processes, for example, fraud detection. Some typical data product consumption use cases are as follows: BI reporting and data analysis: In this case, data applications are built to consume data from multiple data products. For example, data consumers from the customer relationship management (CRM) team need access to data from multiple domains such as sales, customers, and finance. The CRM application that is developed by these data consumers might need to query both a BigQuery authorized view in one domain and extract data from a Cloud Storage Read API in another domain. For data consumers, the optimizing factors that influence their preferred consumption interface are computing costs and any additional data processing that is required after they query the data product. In BI and data analysis use cases, BigQuery authorized views are likely to be most commonly used. Data science use cases and model training: In this case, the data consuming team is using the data products from other domains to enrich their own analytical data product such as an ML model. By using Dataproc Serverless for Spark, Google Cloud provides data pre-processing and feature engineering capabilities to enable data enrichment before running ML tasks. The key considerations are availability of sufficient amounts of training data at a reasonable cost, and confidence that the training data is the appropriate data. To keep costs down, the preferred consumption interfaces are likely to be direct read APIs. It's possible for a data consuming team to build an ML model as a data product, and in turn, that data consuming team also becomes a new data producing team. Operator processes: Consumption is a part of the operational process within the data consuming domain. For example, a data consumer in a team that deals with fraud might be using transaction data coming from operational data sources in the merchant domain. By using a data integration method like change data capture, this transaction data is intercepted at near real time. You can then use Pub/Sub to define a schema for this data and expose that information as events. In this case, the appropriate interfaces would be data exposed as Pub/Sub topics. Data consumption steps Data producers document their data product in the central catalog, including guidance on how to consume the data. For an organization with multiple domains, this documentation approach creates an architecture that's different from the traditional centrally built ELT/ETL pipeline, where processors create outputs without the boundary of business domains. Data consumers in a data mesh must have a well-designed discovery and consumption layer to create a data consumption lifecycle. The layer should include the following: Step 1: Discover data products through declarative search and exploration of data product specifications: Data consumers are free to search for any data product that data producers have registered in the central catalog. For all data products, the data product tag specifies how to make data access requests and the mode to consume data from the required data product interface. The fields in the data product tags are searchable using a search application. Data product interfaces implement data URIs, which means data does not need to be moved to a separate consumption zone to service consumers. In situations when real-time data isn't needed, consumers query data products and create reports with the results that are generated. Step 2: Exploring data through interactive data access and prototyping: Data consumers use interactive tools like BigQuery Studio and Jupyter Notebooks to interpret and experiment with the data to refine the queries that they need for production use. Interactive querying enables data consumers to explore newer dimensions of data and improve the correctness of insights generated in production scenarios. Step 3: Consuming data product through an application, with programmatic access and production: BI reports. Batch and near-real time reports and dashboards are the most common group of analytic use cases required by data consumers. Reports might require cross-data product access to help facilitate decision making. For example, a customer data platform requires programmatically querying both orders and CRM data products in a scheduled fashion. The results from such an approach provide a holistic customer view to the business users who consume the data. AI/ML model for batch and real-time prediction. Data scientists use common MLOps principles to build and service ML models that consume data products made available by the data product teams. ML models provide real-time inference capabilities for transactional use-cases like fraud detection. Similarly, with exploratory data analysis, data consumers can enrich source data. For example, exploratory data analysis on sales and marketing campaigns data shows demographic customer segments where sales are expected to be highest and hence where campaigns should be run.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_environment"", ""labels"": [ { ""text"": ""gcp_environment"" } ], ""children"": [ { ""id"": ""data_domain"", ""labels"": [ { ""text"": ""data_domain"" } ], ""children"": [ { ""id"": ""operational_source_data"", ""labels"": [ { ""text"": ""operational_source_data"" } ], ""children"": [ { ""id"": ""inventory_spanner"", ""labels"": [ { ""text"": ""Inventory Cloud Spanner"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""web_events_pubsub"", ""labels"": [ { ""text"": ""Web Events Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_ingest"", ""labels"": [ { ""text"": ""data_ingest"" } ], ""children"": [ { ""id"": ""ingest_pipeline_dataflow"", ""labels"": [ { ""text"": ""Ingest Pipeline Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""all_domain_data"", ""labels"": [ { ""text"": ""all_domain_data"" } ], ""children"": [ { ""id"": ""base_tables_bigquery"", ""labels"": [ { ""text"": ""Base tables BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""materialized_views_bigquery"", ""labels"": [ { ""text"": ""Mater. views BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_base_to_material"", ""labels"": [ { ""text"": ""transform"" } ] } ] }, { ""id"": ""product_interface"", ""labels"": [ { ""text"": ""product_interface"" } ], ""children"": [ { ""id"": ""auth_dataset_v1"", ""labels"": [ { ""text"": ""Auth dataset v1 BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""auth_dataset_v2"", ""labels"": [ { ""text"": ""Auth dataset v2 BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""storage_read_api"", ""labels"": [ { ""text"": ""Storage Read API BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_access_apis_gke"", ""labels"": [ { ""text"": ""Data Access APIs GKE"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_inv_ingest"", ""labels"": [ { ""text"": ""streams"" } ] }, { ""id"": ""edge_events_ingest"", ""labels"": [ { ""text"": ""streams"" } ] }, { ""id"": ""edge_ingest_base"", ""labels"": [ { ""text"": ""load"" } ] }, { ""id"": ""edge_domain_authv1"", ""labels"": [ { ""text"": ""publish"" } ] }, { ""id"": ""edge_domain_authv2"", ""labels"": [ { ""text"": ""publish"" } ] }, { ""id"": ""edge_domain_storage"", ""labels"": [ { ""text"": ""serve"" } ] }, { ""id"": ""edge_domain_dataapis"", ""labels"": [ { ""text"": ""serve"" } ] } ] }, { ""id"": ""consumers_domain"", ""labels"": [ { ""text"": ""consumers_domain"" } ], ""children"": [ { ""id"": ""consumers_bi"", ""labels"": [ { ""text"": ""consumers_bi"" } ], ""children"": [ { ""id"": ""bi_tools_looker"", ""labels"": [ { ""text"": ""BI Tools Looker"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""sql_engine_bigquery"", ""labels"": [ { ""text"": ""SQL Engine BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_bi_queries"", ""labels"": [ { ""text"": ""queries"" } ] } ] }, { ""id"": ""consumers_big_data"", ""labels"": [ { ""text"": ""consumers_big_data"" } ], ""children"": [ { ""id"": ""data_analytics_dataproc"", ""labels"": [ { ""text"": ""Data Analytics Dataproc"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ml_models_vertexai"", ""labels"": [ { ""text"": ""ML Models Vertex AI"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""edge_data_to_ml"", ""labels"": [ { ""text"": ""train"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""edge_authv1_sql"", ""labels"": [ { ""text"": ""feed"" } ] }, { ""id"": ""edge_authv2_sql"", ""labels"": [ { ""text"": ""feed"" } ] }, { ""id"": ""edge_storage_to_data"", ""labels"": [ { ""text"": ""feed"" } ] }, { ""id"": ""edge_dataapis_to_data"", ""labels"": [ { ""text"": ""provide"" } ] } ] } ], ""edges"": [] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/blueprints/deploy_enterprise_data_mesh,"Deploy an enterprise data management and analytics platform bookmark_border Last reviewed 2025-04-04 UTC An enterprise data management and analytics platform provides an enclave where you can store, analyze, and manipulate sensitive information while maintaining security controls. You can use the enterprise data mesh architecture to deploy a platform on Google Cloud for data management and analytics. The architecture is designed to work in a hybrid environment, where Google Cloud components interact with your existing on-premises components and operating processes. The enterprise data mesh architecture includes the following: A GitHub repository that contains a set of Terraform configurations, scripts, and code to build the following: A governance project that lets you use Google's implementation of the Cloud Data Management Capabilities (CDMS) Key Controls Framework. A data platform example that supports interactive and production workflows. A producer environment within the data platform that supports multiple data domains. Data domains are logical groupings of data elements. A consumer environment within the data platform that supports multiple consumer projects. A data transfer service that uses Workload Identity Federation and the Tink encryption library to help you transfer data into Google Cloud in a secure manner. A data domain example that contains ingestion, non-confidential, and confidential projects. An example of a data access system that lets data consumers request access to data sets and data owners grant access to those data sets. The example also includes a workflow manager that changes the IAM permissions of those data sets accordingly. A guide to the architecture, design, security controls, and operational processes that you use this architecture to implement (this document). The enterprise data mesh architecture is designed to be compatible with the enterprise foundations blueprint. The enterprise foundations blueprint provides a number of base-level services that this architecture relies on, such as VPC networks and logging. You can deploy this architecture without deploying the enterprise foundations blueprint if your Google Cloud environment provides the necessary functionality. This document is intended for cloud architects, data scientists, data engineers, and security architects who can use the architecture to build and deploy comprehensive data services on Google Cloud. This document assumes that you are familiar with the concepts of data meshes, Google Cloud data services, and the Google Cloud implementation of the CDMC framework. Architecture The enterprise data mesh architecture takes a layered approach to provide the capabilities that enable data ingestion, data processing, and governance. The architecture is intended to be deployed and controlled through a CI/CD workflow. The following diagram shows how the data layer that is deployed by this architecture relates to other layers in your environment. Data mesh architecture. This diagram includes the following: Google Cloud infrastructure provides security capabilities such as encryption at rest and encryption in transit, as well as basic building blocks such as compute and storage. The enterprise foundation provides a baseline of resources such as identity, networking, logging, monitoring, and deployment systems that enable you to adopt Google Cloud for your data workloads. The data layer provides various capabilities such as data ingestion, data storage, data access control, data governance, data monitoring, and data sharing. The application layer represents various different applications that use the data layer assets. CI/CD provides the tools to automate the provision, configuration, management, and deployment of infrastructure, workflows, and software components. These components help you ensure consistent, reliable, and auditable deployments; minimize manual errors; and accelerate the overall development cycle. To show how the data environment is used, the architecture includes a sample data workflow. The sample data workflow takes you through the following processes: data governance, data ingestion, data processing, data sharing, and data consumption. Key architectural decisions The following table summarizes the high-level decisions of the architecture. Decision area	Decision Google Cloud architecture Resource hierarchy The architecture uses the resource hierarchy from the enterprise foundations blueprint. Networking The architecture includes an example data transfer service that uses Workload Identity Federation and a Tink library. Roles and IAM permissions The architecture includes segmented data producer roles, data consumer roles, data governance roles, and data platform roles. Common data services Metadata The architecture uses Data Catalog to manage data metadata. Central policy management To manage policies, the architecture uses Google Cloud's implementation of the CDMC framework. Data access management To control access to data, the architecture includes an independent process that requires data consumers to request access to data assets from the data owner. Data quality The architecture uses the Cloud Data Quality Engine to define and run data quality rules on specified table columns, measuring data quality based on metrics like correctness and completeness. Data security The architecture uses tagging, encryption, masking, tokenization, and IAM controls to provide data security. Data domain Data environments The architecture includes three environments. Two environments (non-production and production) are operational environments that are driven by pipelines. One environment (development) is an interactive environment. Data owners Data owners ingest, process, expose, and grant access to data assets. Data consumers Data consumers request access to data assets. Onboarding and operations Pipelines The architecture uses the following pipelines to deploy resources: Foundation pipeline Infrastructure pipeline Artifact pipelines Service Catalog pipeline Repositories Each pipeline uses a separate repository to enable segregation of responsibility. Process flow The process requires that changes to the production environment include a submitter and an approver. Cloud operations Data product scorecards The Report Engine generates data product scorecards. Cloud Logging The architecture uses the logging infrastructure from the enterprise foundations blueprint. Cloud Monitoring The architecture uses the monitoring infrastructure from the enterprise foundations blueprint. Identity: Mapping roles to groups The data mesh leverages the enterprise foundations blueprint's existing identity lifecycle management, authorization, and authentication architecture. Users are not assigned roles directly; instead groups are the primary method of assigning roles and permission in IAM. IAM roles and permissions are assigned during project creation through the foundation pipeline. The data mesh associates groups with one of four key areas: infrastructure, data governance, domain-based data producers, and domain-based consumers. The permission scopes for these groups are the following: The infrastructure group's permission scope is the data mesh as a whole. The data governance groups' permission scope is the data governance project. Domain-based producers and consumers permissions are scoped to their data domain. The following tables show the various roles used in this data mesh implementation and their associated permissions. Infrastructure Group	Description	Roles data-mesh-ops@example.com Overall administrators of the data mesh roles/owner (data platform) Data governance Group	Description	Roles gcp-dm-governance-admins@example.com Administrators of the data governance project roles/owner on the data governance project gcp-dm-governance-developers@example.com Developers who build and maintain the data governance components Multiple roles on the data governance project, including roles/viewer, BigQuery roles, and Data Catalog roles gcp-dm-governance-data-readers@example.com Readers of data governance information roles/viewer gcp-dm-governance-security-administrator@example.com Security administrators of the governance project roles/orgpolicy.policyAdmin and roles/iam.securityReviewer gcp-dm-governance-tag-template-users@example.com Group with permission to use tag templates roles/datacatalog.tagTemplateUser gcp-dm-governance-tag-users@example.com Group with permission to use tag templates and add tags roles/datacatalog.tagTemplateUser and roles/datacatalog.tagEditor gcp-dm-governance-scc-notifications@example.com Service account group for Security Command Center notifications None. This is a group for membership, and a service account is created with this name, which has the necessary permissions. Domain-based data producers Group	Description	Roles gcp-dm-{data_domain_name}-admins@example.com Administrators of a specific data domain roles/owner on the data domain project gcp-dm-{data_domain_name}-developers@example.com Developers who build and maintain data products within a data domain Multiple roles on the data domain project, including roles/viewer, BigQuery roles, and Cloud Storage roles gcp-dm-{data_domain_name}-data-readers@example.com Readers of the data domain information roles/viewer gcp-dm-{data_domain_name}-metadata-editors@{var.domain} Editors of Data Catalog entries Roles to edit Data Catalog entries gcp-dm-{data_domain_name}-data-stewards@example.com Data stewards for the data domain Roles to manage metadata and data governance aspects Domain-based data consumers Group	Description	Roles gcp-dm-consumer-{project_name}-admins@example.com Administrators of a specific consumer project roles/owner on consumer project gcp-dm-consumer-{project_name}-developers@example.com Developers working within a consumer project Multiple roles on the consumer project, including roles/viewer and BigQuery roles gcp-dm-consumer-{project_name}-data-readers@example.com Readers of the consumer project information roles/viewer Organization structure To differentiate between production operations and production data, the architecture uses different environments to develop and release workflows. Production operations include the governance, traceability, and repeatability of a workflow and the auditability of the results of the workflow. Production data refers to possibly sensitive data that you need to run your organization. All environments are designed to have security controls that let you ingest and operate your data. To help data scientists and engineers, the architecture includes an interactive environment, where developers can work with the environment directly and add services through a curated catalog of solutions. Operational environments are driven through pipelines which have codified architecture and configuration. This architecture uses the organizational structure of the enterprise foundations blueprint as a basis for deploying data workloads. The following diagram shows the top-level folders and projects used in the enterprise data mesh architecture. Data mesh organization structure. The following table describes the top-level folders and projects that are part of the architecture. Folder	Component	Description common prj-c-artifact-pipeline Contains the deployment pipeline that's used to build out the code artifacts of the architecture. prj-c-service-catalog Contains the infrastructure used by the Service Catalog to deploy resources in the interactive environment. prj-c-datagovernance Contains all the resources used by Google Cloud's implementation of the CDMC framework. development fldr-d-dataplatform Contains the projects and resources of the data platform for developing use cases in interactive mode. non-production fldr-n-dataplatform Contains the projects and resources of the data platform for testing use cases that you want to deploy in an operational environment. production fldr-p-dataplatform Contains the projects and resources of the data platform for deployment into production. Data platform folder The data platform folder contains all the data plane components and some of the CDMC resources. In addition, the data platform folder and the data governance project contain the CDMC resources. The following diagram shows the folders and projects that are deployed in the data platform folder. The data platform folder Each data platform folder includes an environment folder (production, non-production, and development). The following table describes the folders within each data platform folder. Folders	Description Producers Contains the data domains. Consumers Contains the consumer projects. Data domain Contains the projects associated with a particular domain. Producers folder Each producers folder includes one or more data domains. A data domain refers to a logical grouping of data elements that share a common meaning, purpose, or business context. Data domains let you categorize and organize data assets within an organization. The following diagram shows the structure of a data domain. The architecture deploys projects in the data platform folder for each environment.","{ ""id"": ""root"", ""children"": [ { ""id"": ""multiple_data_sources"", ""labels"": [ { ""text"": ""Multiple Data Sources"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_transfer_service"", ""labels"": [ { ""text"": ""data_transfer_service"" } ], ""children"": [ { ""id"": ""tink_library"", ""labels"": [ { ""text"": ""Tink Library"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""workload_identity"", ""labels"": [ { ""text"": ""Workload Identity"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""google_cloud"", ""labels"": [ { ""text"": ""google_cloud"" } ], ""children"": [ { ""id"": ""data_governance_project"", ""labels"": [ { ""text"": ""data_governance_project"" } ], ""children"": [ { ""id"": ""data_governance_service"", ""labels"": [ { ""text"": ""Data Governance Service"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""environment"", ""labels"": [ { ""text"": ""environment"" } ], ""children"": [ { ""id"": ""data_domains"", ""labels"": [ { ""text"": ""data_domains"" } ], ""children"": [ { ""id"": ""ingestion_project"", ""labels"": [ { ""text"": ""ingestion_project"" } ], ""children"": [ { ""id"": ""data_landing_services"", ""labels"": [ { ""text"": ""data_landing_services"" } ], ""children"": [ { ""id"": ""bigquery_landing"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_storage_landing"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_landing"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""dataflow"", ""labels"": [ { ""text"": ""Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_composer"", ""labels"": [ { ""text"": ""Cloud Composer"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_dataflow_storage"", ""labels"": [ { ""text"": ""reads"" } ] }, { ""id"": ""e_dataflow_bigquery"", ""labels"": [ { ""text"": ""loads"" } ] }, { ""id"": ""e_composer_dataflow"", ""labels"": [ { ""text"": ""orchestrates"" } ] } ] }, { ""id"": ""non_confidential_project"", ""labels"": [ { ""text"": ""non_confidential_project"" } ], ""children"": [ { ""id"": ""bigquery_nonconf"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""confidential_project"", ""labels"": [ { ""text"": ""confidential_project"" } ], ""children"": [ { ""id"": ""bigquery_conf"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_landing_to_nonconf"", ""labels"": [ { ""text"": ""shares"" } ] }, { ""id"": ""e_landing_to_conf"", ""labels"": [ { ""text"": ""shares_confidential"" } ] } ] }, { ""id"": ""shared_vpc"", ""labels"": [ { ""text"": ""shared_vpc"" } ], ""children"": [ { ""id"": ""private_service_connect"", ""labels"": [ { ""text"": ""Private Service Connect"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""consumer_projects"", ""labels"": [ { ""text"": ""consumer_projects"" } ], ""children"": [ { ""id"": ""consumer_project_1"", ""labels"": [ { ""text"": ""Consumer Project 1"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""consumer_project_2"", ""labels"": [ { ""text"": ""Consumer Project 2"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""consumer_project_n"", ""labels"": [ { ""text"": ""Consumer Project N"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_psc_storage"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""e_nonconf_cp1"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""e_nonconf_cp2"", ""labels"": [ { ""text"": ""serves"" } ] }, { ""id"": ""e_nonconf_cpN"", ""labels"": [ { ""text"": ""serves"" } ] } ] } ], ""edges"": [ { ""id"": ""e_governance_bigquery"", ""labels"": [ { ""text"": ""governs"" } ] } ] } ], ""edges"": [ { ""id"": ""e_sources_transfer"", ""labels"": [ { ""text"": ""transfers"" } ] }, { ""id"": ""e_transfer_to_psc"", ""labels"": [ { ""text"": ""ingests"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/blueprints/deploy_enterprise_data_mesh,"Data access management The architecture's access to data is controlled through an independent process which separates operational control (for example, running Dataflow jobs) from data access control. A user's access to a Google Cloud service is defined by an environmental or operational concern and is provisioned and approved by a cloud engineering group. A user's access to Google Cloud data assets (for example, a BigQuery table) is a privacy, regulatory, or governance concern and is subject to an access agreement between the producing and consuming parties and controlled through the following processes. The following diagram shows how data access is provisioned through the interaction of different software components. Data access management As shown in the previous diagram, onboarding of data accesses is handled by the following processes: Cloud data assets are collected and inventoried by Data Catalog. The workflow manager retrieves the data assets from Data Catalog. Data owners are onboarded to workflow manager. The operation of the data access management is as follows: A data consumer makes a request for a specific asset. The data owner of the asset is alerted to the request. The data owner approves or rejects the request. If the request is approved, the workflow manager passes the group, asset, and associated tag to the IAM mapper. The IAM mapper translates the workflow manager tags into IAM permissions, and gives the specified group IAM permissions for the data asset. When a user wants to access the data asset, IAM evaluates access to the Google Cloud asset based on the permissions of the group. If permitted, the user accesses the data asset. Networking The data security process initiates at the source application, which might reside on-premises or in another environment external to the target Google Cloud project. Before any network transfer occurs, this application uses Workload Identity Federation to securely authenticate itself to Google Cloud APIs. Using these credentials, it interacts with Cloud KMS to obtain or wrap the necessary keys and then employs the Tink library to perform initial encryption and de-identification on the sensitive data payload according to predefined templates. After the data payload is protected, the payload must be securely transferred into the Google Cloud ingestion project. For on-premise applications, you can use Cloud Interconnect or potentially Cloud VPN. Within the Google Cloud network, use Private Service Connect to route the data towards the ingestion endpoint within the target project's VPC network. Private Service Connect lets the source application connect to Google APIs using private IP addresses, ensuring traffic isn't exposed to the internet. The entire network path and the target ingestion services (Cloud Storage, BigQuery, and Pub/Sub) within the ingestion project are secured by a VPC Service Controls perimeter. This perimeter enforces a security boundary, ensuring that the protected data originating from the source can only be ingested into the authorized Google Cloud services within that specific project.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""data_owner"", ""labels"": [ { ""text"": ""Data owner"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_consumer"", ""labels"": [ { ""text"": ""Data consumer"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_access_app"", ""labels"": [ { ""text"": ""data_access_app"" } ], ""children"": [ { ""id"": ""api_gateway"", ""labels"": [ { ""text"": ""api_gateway"" } ], ""children"": [ { ""id"": ""workflow_manager"", ""labels"": [ { ""text"": ""Workflow Manager"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""backend_services"", ""labels"": [ { ""text"": ""backend_services"" } ], ""children"": [ { ""id"": ""iam_mapper"", ""labels"": [ { ""text"": ""IAM Mapper"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""microsoft_ad"", ""labels"": [ { ""text"": ""Microsoft AD"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_ad_to_mapper"", ""labels"": [ { ""text"": ""groups"" } ] } ] } ], ""edges"": [ { ""id"": ""e_ad_to_wf"", ""labels"": [ { ""text"": ""groups"" } ] } ] }, { ""id"": ""google_cloud"", ""labels"": [ { ""text"": ""google_cloud"" } ], ""children"": [ { ""id"": ""google_cloud_iam"", ""labels"": [ { ""text"": ""Google Cloud IAM"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_catalog"", ""labels"": [ { ""text"": ""Data Catalog"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_layer"", ""labels"": [ { ""text"": ""data_layer"" } ], ""children"": [ { ""id"": ""cloud_storage"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_iam_to_cs"", ""labels"": [ { ""text"": ""permissions"" } ] }, { ""id"": ""e_iam_to_bq"", ""labels"": [ { ""text"": ""permissions"" } ] }, { ""id"": ""e_cs_to_catalog"", ""labels"": [ { ""text"": ""assets"" } ] }, { ""id"": ""e_bq_to_catalog"", ""labels"": [ { ""text"": ""assets"" } ] } ] } ], ""edges"": [ { ""id"": ""e_owner_approve"", ""labels"": [ { ""text"": ""access approval"" } ] }, { ""id"": ""e_consumer_request"", ""labels"": [ { ""text"": ""access request"" } ] }, { ""id"": ""e_catalog_assets_to_wf"", ""labels"": [ { ""text"": ""assets"" } ] }, { ""id"": ""e_mapper_to_iam"", ""labels"": [ { ""text"": ""groups, permissions, assets"" } ] }, { ""id"": ""e_ad_to_gcp_iam"", ""labels"": [ { ""text"": ""groups"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/blueprints/deploy_enterprise_data_mesh#pipelines,"Pipelines The enterprise data mesh architecture uses a series of pipelines to provision the infrastructure, orchestration, data sets, data pipelines, and application components. The architecture's resource deployment pipelines use Terraform as the infrastructure as code (IaC) tool and Cloud Build as the CI/CD service to deploy the Terraform configurations into the architecture environment. The following diagram shows the relationship between the pipelines. Pipeline relationships The foundation pipeline and the infrastructure pipeline are part of the enterprise foundations blueprint. The following table describes the purpose of the pipelines and the resources that they provision. Pipeline	Provisioned by	Resources Foundation pipeline Bootstrap Data platform folder and subfolders Common projects Infrastructure pipeline service account Cloud Build trigger for the Infrastructure pipeline Shared VPC VPC Service Control perimeter Infrastructure pipeline Foundation pipeline Consumer projects Service Catalog service account The Cloud Build trigger for the Service Catalog pipeline Artifact pipeline service account The Cloud Build trigger for the artifact pipeline Service Catalog pipeline Infrastructure pipeline Resources deployed in the Service Catalog bucket Artifact pipelines Infrastructure pipeline Artifact pipelines produce the various containers and other components of the codebase used by the data mesh. Each pipeline has its own set of repositories that it pulls code and configuration files from. Each repository has a separation of duties where submitters and approvals of operational code deployments are the responsibilities of different groups.","{ ""id"": ""root"", ""children"": [ { ""id"": ""gcp_environment"", ""labels"": [ { ""text"": ""gcp_environment"" } ], ""children"": [ { ""id"": ""artifact_outputs_group"", ""labels"": [ { ""text"": ""artifact_outputs_group"" } ], ""children"": [ { ""id"": ""containers"", ""labels"": [ { ""text"": ""Containers"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""python_code"", ""labels"": [ { ""text"": ""Python code"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""other_artifact"", ""labels"": [ { ""text"": ""Other"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""project_infrastructure_group"", ""labels"": [ { ""text"": ""project_infrastructure_group"" } ], ""children"": [ { ""id"": ""project_infrastructure"", ""labels"": [ { ""text"": ""Project infrastructure"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""deployment_templates_group"", ""labels"": [ { ""text"": ""deployment_templates_group"" } ], ""children"": [ { ""id"": ""deployment_templates_storage"", ""labels"": [ { ""text"": ""Deployment templates"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""artifact_pipelines_group"", ""labels"": [ { ""text"": ""artifact_pipelines_group"" } ], ""children"": [ { ""id"": ""artifact_pipelines_cb"", ""labels"": [ { ""text"": ""Artifact pipelines"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""service_catalog_pipeline_group"", ""labels"": [ { ""text"": ""service_catalog_pipeline_group"" } ], ""children"": [ { ""id"": ""service_catalog_pipeline_cb"", ""labels"": [ { ""text"": ""Service catalog pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""foundation_pipeline_group"", ""labels"": [ { ""text"": ""foundation_pipeline_group"" } ], ""children"": [ { ""id"": ""foundation_pipeline_cb"", ""labels"": [ { ""text"": ""Foundation pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""infrastructure_pipeline_group"", ""labels"": [ { ""text"": ""infrastructure_pipeline_group"" } ], ""children"": [ { ""id"": ""infrastructure_pipeline_cb"", ""labels"": [ { ""text"": ""Infrastructure pipeline"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_ap_containers"", ""labels"": [ { ""text"": ""packages"" } ] }, { ""id"": ""e_ap_python"", ""labels"": [ { ""text"": ""packages"" } ] }, { ""id"": ""e_ap_other"", ""labels"": [ { ""text"": ""packages"" } ] }, { ""id"": ""e_sc_templates"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""e_foundation_infra"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_infra_artifact"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_infra_project"", ""labels"": [ { ""text"": ""deploys"" } ] }, { ""id"": ""e_infra_service_catalog"", ""labels"": [ { ""text"": ""triggers"" } ] } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/blueprints/deploy_enterprise_data_mesh#interactive-deployment,"Interactive deployment through Service Catalog Interactive environments are the development environment within the architecture and exist under the development folder. The main interface for the interactive environment is Service Catalog, which lets developers use preconfigured templates to instantiate Google services. These preconfigured templates are known as service templates. Service templates help you to enforce your security posture, such as making CMEK encryption mandatory, and also prevents your users from having direct access to Google APIs. The following diagram shows the components of the interactive environment and how data scientists deploy resources. Interactive environment with Service Catalog. To deploy resources using the Service Catalog, the following steps occur: The MLOps engineer puts a Terraform resource template for Google Cloud into a Git repository. The Git Commit command triggers a Cloud Build pipeline. Cloud Build copies the template and any associated configuration files to Cloud Storage. The MLOps engineer sets up the Service Catalog solutions and Service Catalog manually. The engineer then shares the Service Catalog with a service project in the interactive environment. The data scientist selects a resource from the Service Catalog. Service Catalog deploys the template into the interactive environment. The resource pulls any necessary configuration scripts. The data scientist interacts with the resources. Artifact pipelines The data ingestion process uses Cloud Composer and Dataflow to orchestrate the movement and transformation of data within the data domain. The artifact pipeline builds all necessary resources for data ingestion and moves the resources to the appropriate location for the services to access them. The artifact pipeline creates the container artifacts that the orchestrator uses. Security controls The enterprise data mesh architecture uses a layered defense-in-depth security model that includes default Google Cloud capabilities, Google Cloud services, and security capabilities that are configured through the enterprise foundations blueprint. The following diagram shows the layering of the various security controls for the architecture. Security controls in the data mesh architecture. The following table describes the security controls that are associated with the resources in each layer. Layer	Resource	Security control CDMC framework Google Cloud CDMC implementation Provides a governance framework that helps secure, manage and control your data assets. See CDMC Key Controls Framework for more information. Deployment Infrastructure pipeline Provides a series of pipelines that deploy infrastructure, build containers, and create data pipelines. The use of pipelines allows for auditability, traceability, and repeatability. Artifact pipeline Deploys various components not deployed by the infrastructure pipeline. Terraform templates Builds out the system infrastructure. Open Policy Agent Helps ensure that the platform conforms to selected policies. Network Private Service Connect Provides data exfiltration protections around the architecture resources at the API layer and the IP layer. Lets you communicate with Google Cloud APIs using private IP addresses so that you can avoid exposing traffic to the internet. VPC network with private IP addresses Helps remove exposure to internet-facing threats. VPC Service Controls Helps protect sensitive resources against data exfiltration. Firewall Helps protect the VPC network against unauthorized access. Access management Access Context Manager Controls who can access what resources and helps prevent unauthorized use of your resources. Workload Identity Federation Removes the need for external credentials to transfer data onto the platform from on-premises environments. Data Catalog Provides an index of assets available to users. IAM Provides fine-grained access. Encryption Cloud KMS Lets you manage your encryption keys and secrets, and help protect your data through encryption at rest and encryption in transit. Secrets Manager Provides a secret store for pipelines that are controlled by IAM. Encryption at rest By default, Google Cloud encrypts data at rest. Encryption in transit By default, Google Cloud encrypts data in transit. Detective Security Command Center Helps you to detect misconfigurations and malicious activity in your Google Cloud organization. Continuous architecture Continually checks your Google Cloud organization against a series of OPA policies that you have defined. IAM Recommender Analyzes user permissions and provides suggestions about reducing permissions to help enforce the principle of least privilege. Firewall Insights Analyzes firewall rules, identifies overly-permissive firewall rules, and suggests more restrictive firewalls to help strengthen your overall security posture. Cloud Logging Provides visibility into system activity and helps enable the detection of anomalies and malicious activity. Cloud Monitoring Tracks key signals and events that can help identify suspicious activity. Preventative Organization Policy Lets you control and restrict actions within your Google Cloud organization","{ ""id"": ""root"", ""children"": [ { ""id"": ""frontend_users"", ""labels"": [ { ""text"": ""frontend_users"" } ], ""children"": [ { ""id"": ""user"", ""labels"": [ { ""text"": ""User"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""git_repo"", ""labels"": [ { ""text"": ""Git"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_user_git"", ""labels"": [ { ""text"": ""commits"" } ] } ] }, { ""id"": ""google_cloud"", ""labels"": [ { ""text"": ""google_cloud"" } ], ""children"": [ { ""id"": ""gcp_projects_collection"", ""labels"": [ { ""text"": ""gcp_projects_collection"" } ], ""children"": [ { ""id"": ""private_catalog_project"", ""labels"": [ { ""text"": ""private_catalog_project"" } ], ""children"": [ { ""id"": ""cloud_build"", ""labels"": [ { ""text"": ""Cloud Build"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""service_catalog"", ""labels"": [ { ""text"": ""Service Catalog"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""config_scripts_store"", ""labels"": [ { ""text"": ""Config Scripts (Cloud Storage)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""templates_store"", ""labels"": [ { ""text"": ""Templates (Cloud Storage)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_build_config"", ""labels"": [ { ""text"": ""stores scripts"" } ] }, { ""id"": ""e_build_templates"", ""labels"": [ { ""text"": ""stores templates"" } ] } ] }, { ""id"": ""data_project_1"", ""labels"": [ { ""text"": ""data_project_1"" } ], ""children"": [ { ""id"": ""pubsub"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_project_2"", ""labels"": [ { ""text"": ""data_project_2"" } ], ""children"": [ { ""id"": ""dataflow"", ""labels"": [ { ""text"": ""Dataflow"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""data_project_n"", ""labels"": [ { ""text"": ""data_project_n"" } ], ""children"": [ { ""id"": ""cloud_composer"", ""labels"": [ { ""text"": ""Cloud Composer"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_dataflow_service"", ""labels"": [ { ""text"": ""requests"" } ] }, { ""id"": ""e_composer_service"", ""labels"": [ { ""text"": ""requests"" } ] } ] } ], ""edges"": [] }, { ""id"": ""protected_endpoint"", ""labels"": [ { ""text"": ""protected_endpoint"" } ], ""children"": [ { ""id"": ""notebooks"", ""labels"": [ { ""text"": ""Notebooks"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""restricted_console"", ""labels"": [ { ""text"": ""Restricted Console"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_git_build"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_service_console"", ""labels"": [ { ""text"": ""exposes"" } ] }, { ""id"": ""e_pubsub_notebooks"", ""labels"": [ { ""text"": ""delivers"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/blueprints/deploy_enterprise_data_mesh#data-producer,"Data producer workflow The following diagram shows how data is protected as it is transferred to BigQuery. Data producer workflow The workflow for data transfer is the following: An application that is integrated with Workload Identity Federation uses Cloud KMS to decrypt a wrapped encryption key. The application uses the Tink library to de-identify or encrypt the data using a template. The application transfers data to the ingestion project in Google Cloud. The data arrives in Cloud Storage, BigQuery, or Pub/Sub. In the ingestion project, the data is decrypted or re-identified using a template. The decrypted data is encrypted or masked based on another de-identification template, then placed in the non-confidential project. Tags are applied by the tagging engine as appropriate. Data from the non-confidential project is transferred over to the confidential project and re-identified. The following data access is permitted: Users who have access to the confidential project can access all the raw plaintext data. Users who have access to the non-confidential project can access masked, tokenized, or encrypted data based on the tags associated with the data and their permissions. Data consumer workflow The following steps describe how a consumer can access data that is stored in BigQuery. The data consumer searches for data assets using Data Catalog. After the consumer finds the assets that they are looking for, the data consumer requests access to the data assets. The data owner decides whether to provide access to the assets. If the consumer obtains access, the consumer can use a notebook and the Solution Catalog to create an environment in which they can analyze and transform the data assets. Bringing it all together The GitHub repository provides you with detailed instructions on deploying the data mesh on Google Cloud after you deployed the enterprise foundation. The process to deploy the architecture involves modifying your existing infrastructure repositories and deploying new data mesh specific components. Complete the following: Complete all prerequisites, including the following: Install Google Cloud CLI, Terraform, Tink, Java, and Go. Deploy the enterprise foundations blueprint (v4.1). Maintain the following local repositories: gcp-data-mesh-foundations gcp-bootstrap gcp-environments gcp-networks gcp-org gcp-projects Modify the existing foundation blueprint and then deploy the data mesh applications. For each item, complete the following: In your target repository, check out the Plan branch. To add data mesh components, copy the relevant files and directories from gcp-data-mesh-foundations into the appropriate foundation directory. Overwrite files when required. Update the data mesh variables, roles, and settings in the Terraform files (for example, *.tfvars and *.tf). Set the GitHub tokens as environment variables. Perform the Terraform initialize, plan, and apply operations on each repository. Commit your changes, push the code to your remote repository, create pull requests and merge to your development, nonproduction, and production environments.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""user_process"", ""labels"": [ { ""text"": ""User/Application Process"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""deterministic_templates"", ""labels"": [ { ""text"": ""deterministic_templates"" } ], ""children"": [ { ""id"": ""deterministic_encryption_template"", ""labels"": [ { ""text"": ""Deterministic Encryption Template"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""deterministic_decryption_template"", ""labels"": [ { ""text"": ""Deterministic Decryption Template"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_enc_to_dec"", ""labels"": [ { ""text"": ""maps to"" } ] } ] }, { ""id"": ""consumer_groups"", ""labels"": [ { ""text"": ""consumer_groups"" } ], ""children"": [ { ""id"": ""full_access_consumer"", ""labels"": [ { ""text"": ""Full Access Consumer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""mixed_access_consumer"", ""labels"": [ { ""text"": ""Mixed Access Consumer"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""limited_access_consumer"", ""labels"": [ { ""text"": ""Limited Access Consumer"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_environment"", ""labels"": [ { ""text"": ""gcp_environment"" } ], ""children"": [ { ""id"": ""ingestion_project"", ""labels"": [ { ""text"": ""ingestion_project"" } ], ""children"": [ { ""id"": ""ingestion_cloud_storage"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ingestion_pubsub"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ingestion_bigquery"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ingestion_dataflow_storage"", ""labels"": [ { ""text"": ""Dataflow (Batch)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ingestion_dataflow_pubsub"", ""labels"": [ { ""text"": ""Dataflow (Stream)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""ingestion_dataflow_bigquery"", ""labels"": [ { ""text"": ""Dataflow (Batch)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_storage_to_df"", ""labels"": [ { ""text"": ""batch processing"" } ] }, { ""id"": ""e_pubsub_to_df"", ""labels"": [ { ""text"": ""stream processing"" } ] }, { ""id"": ""e_bq_to_df"", ""labels"": [ { ""text"": ""batch processing"" } ] } ] }, { ""id"": ""non_conf_project"", ""labels"": [ { ""text"": ""non_conf_project"" } ], ""children"": [ { ""id"": ""non_conf_bigquery"", ""labels"": [ { ""text"": ""BigQuery (Encrypted/Masked)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""confidential_project"", ""labels"": [ { ""text"": ""confidential_project"" } ], ""children"": [ { ""id"": ""conf_dataflow"", ""labels"": [ { ""text"": ""Dataflow (Decrypt)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""conf_bigquery"", ""labels"": [ { ""text"": ""BigQuery (Plaintext)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_confdf_to_confBQ"", ""labels"": [ { ""text"": ""batch processing"" } ] } ] }, { ""id"": ""supporting_services"", ""labels"": [ { ""text"": ""supporting_services"" } ], ""children"": [ { ""id"": ""encryption_keys"", ""labels"": [ { ""text"": ""encryption_keys"" } ], ""children"": [ { ""id"": ""tink_key_sm"", ""labels"": [ { ""text"": ""Tink Key (Secret Manager)"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_kms"", ""labels"": [ { ""text"": ""Cloud KMS"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""governance"", ""labels"": [ { ""text"": ""Governance (Taxonomy & Masking)"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_ingest_to_nonconf"", ""labels"": [ { ""text"": ""batch loads"" } ] }, { ""id"": ""e_nonconf_to_confdf"", ""labels"": [ { ""text"": ""provides data"" } ] }, { ""id"": ""e_gov_to_nonconf"", ""labels"": [ { ""text"": ""governs"" } ] } ] } ], ""edges"": [ { ""id"": ""e_keys_user"", ""labels"": [ { ""text"": ""provides keys"" } ] }, { ""id"": ""e_keys_templates"", ""labels"": [ { ""text"": ""secures"" } ] }, { ""id"": ""e_user_to_storage"", ""labels"": [ { ""text"": ""uploads"" } ] }, { ""id"": ""e_decrypt_to_ingest"", ""labels"": [ { ""text"": ""applies"" } ] }, { ""id"": ""e_template_to_confdf"", ""labels"": [ { ""text"": ""applies"" } ] }, { ""id"": ""e_confBQ_full"", ""labels"": [ { ""text"": ""query"" } ] }, { ""id"": ""e_nonconfBQ_mixed"", ""labels"": [ { ""text"": ""query"" } ] }, { ""id"": ""e_nonconfBQ_limited"", ""labels"": [ { ""text"": ""query"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/big-data-analytics/data-warehouse,"Jump Start Solution: Data warehouse with BigQuery bookmark_border Last reviewed 2025-02-03 UTC This guide helps you understand, deploy, and use the Data warehouse with BigQuery Jump Start Solution. This solution demonstrates how you can build a data warehouse in Google Cloud using BigQuery as your data warehouse, with Looker Studio as a dashboard and visualization tool. The solution also uses the generative AI capabilities of Vertex AI to generate text that summarizes the analysis. Common use cases for building a data warehouse include the following: Aggregating and creating marketing analytics warehouses to improve revenue or other customer metrics. Building financial reports and analyses. Building operational dashboards to improve corporate performance. This document is intended for developers who have some background with data analysis and have used a database to perform an analysis. It assumes that you're familiar with basic cloud concepts, though not necessarily Google Cloud. Experience with Terraform is helpful but not required in order to deploy this solution through the console. Note: This solution helps you explore the capabilities of Google Cloud. The solution is not intended to be used as is for production environments. For information about designing and setting up production-grade environments in Google Cloud, see Landing zone design in Google Cloud and Google Cloud setup checklist. Objectives Learn how data flows into a cloud data warehouse, and how the data can be transformed using SQL. Build dashboards from the data to perform data analysis. Schedule SQL statements to update data on a common recurrence. Create a machine learning model to predict data values over time. Use generative AI to summarize the results of your machine learning model. Products used The solution uses the following Google Cloud products: BigQuery: A fully managed, highly scalable data warehouse with built-in machine learning capabilities. Cloud Storage: An enterprise-ready service that provides low-cost, no-limit object storage for diverse data types. Data is accessible from within and outside of Google Cloud and is replicated geo-redundantly. Looker Studio: Self-service business intelligence platform that helps you create and share data insights. Vertex AI: A machine learning (ML) platform that lets you train and deploy ML models and AI applications. The following Google Cloud products are used to stage data in the solution for first use: Workflows: A fully managed orchestration platform that executes services in a specified order as a workflow. Workflows can combine services, including custom services hosted on Cloud Run or Cloud Run functions, Google Cloud services such as BigQuery, and any HTTP-based API. Cloud Run functions: A serverless execution environment for building and connecting cloud services. Architecture The example warehouse that this solution deploys analyzes fictional ecommerce data from TheLook to understand company performance over time. The following diagram shows the architecture of the Google Cloud resources that the solution deploys. Architecture of the infrastructure for the data warehouse solution. Solution flow The architecture represents a common data flow to populate and transform data for a data warehouse: Data is sent to a Cloud Storage bucket. Workflows facilitates the data movement. Data is loaded into BigQuery as a BigLake table using a SQL stored procedure. Data is transformed in BigQuery by using a SQL stored procedure. Dashboards are created from the data to further analyze with Looker Studio. Data is analyzed using a k-means model built with BigQuery ML. The analysis identifies common patterns, which are summarized by using the generative AI capabilities from Vertex AI through BigQuery. Cloud Run functions creates Python notebooks with additional learning content. Cost For an estimate of the cost of the Google Cloud resources that the data warehouse with BigQuery solution uses, see the precalculated estimate in the Google Cloud Pricing Calculator. Use the estimate as a starting point to calculate the cost of your deployment. You can modify the estimate to reflect any configuration changes that you plan to make for the resources that are used in the solution. The precalculated estimate is based on assumptions for certain factors, including the following: The Google Cloud locations where the resources are deployed. The amount of time that the resources are used. The data region where the data is staged.","{ ""id"": ""root"", ""children"": [ { ""id"": ""data_sources"", ""labels"": [ { ""text"": ""data_sources"" } ], ""children"": [ { ""id"": ""data1"", ""labels"": [ { ""text"": ""Data"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data2"", ""labels"": [ { ""text"": ""Data"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data3"", ""labels"": [ { ""text"": ""Data"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data4"", ""labels"": [ { ""text"": ""Data"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_env"", ""labels"": [ { ""text"": ""gcp_env"" } ], ""children"": [ { ""id"": ""application"", ""labels"": [ { ""text"": ""application"" } ], ""children"": [ { ""id"": ""cloud_storage"", ""labels"": [ { ""text"": ""Cloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""workflows"", ""labels"": [ { ""text"": ""Workflows"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_analysis"", ""labels"": [ { ""text"": ""data_analysis"" } ], ""children"": [ { ""id"": ""bigquery"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""data_studio"", ""labels"": [ { ""text"": ""Data Studio"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_bigquery_to_datastudio"", ""labels"": [ { ""text"": ""visualizes"" } ] } ] }, { ""id"": ""ml_ops"", ""labels"": [ { ""text"": ""ml_ops"" } ], ""children"": [ { ""id"": ""vertex_ai"", ""labels"": [ { ""text"": ""Vertex AI"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""cloud_functions"", ""labels"": [ { ""text"": ""Cloud Functions"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_vertex_to_cf2"", ""labels"": [ { ""text"": ""deploys"" } ] } ] } ], ""edges"": [ { ""id"": ""e_storage_to_workflows"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_storage_to_bigquery"", ""labels"": [ { ""text"": ""loads"" } ] }, { ""id"": ""e_bigquery_to_vertexai"", ""labels"": [ { ""text"": ""trains"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_datasources_to_storage"", ""labels"": [ { ""text"": ""stores"" } ] } ] }"
gcp,big-data-analytics,big-data-analytics,https://cloud.google.com/architecture/scalable-bigquery-backup-automation,"Scalable BigQuery backup automation bookmark_border Release Notes Last reviewed 2024-09-17 UTC This architecture provides a framework and reference deployment to help you develop your BigQuery backup strategy. This recommended framework and its automation can help your organization do the following: Adhere to your organization's disaster recovery objectives. Recover data that was lost due to human errors. Comply with regulations. Improve operational efficiency. The scope of BigQuery data can include (or exclude) folders, projects, datasets, and tables. This recommended architecture shows you how to automate the recurrent backup operations at scale. You can use two backup methods for each table: BigQuery snapshots and BigQuery exports to Cloud Storage. This document is intended for cloud architects, engineers, and data governance officers who want to define and automate data policies in their organizations. Architecture The following diagram shows the automated backup architecture: Architecture for the automated backup solution. The workflow that's shown in the preceding diagram includes the following phases: Cloud Scheduler triggers a run to the dispatcher service through a Pub/Sub message, which contains the scope of the BigQuery data that's included and excluded. Runs are scheduled by using a cron expression. The dispatcher service, which is built on Cloud Run, uses the BigQuery API to list the tables that are within the BigQuery scope. The dispatcher service submits one request for each table to the configurator service through a Pub/Sub message. The Cloud Run configurator service computes the backup policy of the table from one of the following defined options: The table-level policy, which is defined by data owners. The fallback policy, which is defined by the data governance officer, for tables that don't have defined policies. For details about backup policies, see Backup policies. The configurator service submits one request for each table to the next service, based on the computed backup policy. Depending on the backup method, one of the following custom Cloud Run services submits a request to the BigQuery API and runs the backup process: The service for BigQuery snapshots backs up the table as a snapshot. The service for data exports backs up the table as a data export to Cloud Storage. When the backup method is a table data export, a Cloud Logging log sink listens to the export jobs completion events in order to enable the asynchronous execution of the next step. After the backup services complete their operations, Pub/Sub triggers the tagger service. For each table, the tagger service logs the results of the backup services and updates the backup state in the Cloud Storage metadata layer. Products used This reference architecture uses the following Google Cloud products: BigQuery: An enterprise data warehouse that helps you manage and analyze your data with built-in features like machine learning geospatial analysis, and business intelligence. Cloud Logging: A real-time log management system with storage, search, analysis, and alerting. Pub/Sub: An asynchronous and scalable messaging service that decouples services that produce messages from services that process those messages. Cloud Run: A serverless compute platform that lets you run containers directly on top of Google's scalable infrastructure. Cloud Storage: A low-cost, no-limit object store for diverse data types. Data can be accessed from within and outside Google Cloud, and it's replicated across locations for redundancy. Cloud Scheduler: A fully managed enterprise-grade cron job scheduler that lets you set up scheduled units of work to be executed at defined times or regular intervals. Datastore: A highly scalable NoSQL database for your web and mobile applications. Use cases This section provides examples of use cases for which you can use this architecture. Backup automation As an example, your company might operate in a regulated industry and use BigQuery as the main data warehouse. Even when your company follows best practices in software development, code review, and release engineering, there's still a risk of data loss or data corruption due to human errors. In a regulated industry, you need to minimize this risk as much as possible. Examples of these human errors include the following: Accidental deletion of tables. Data corruption due to erroneous data pipeline logic. These types of human errors can usually be resolved with the time travel feature, which lets you recover data from up to seven days ago. In addition, BigQuery also offers a fail-safe period, during which deleted data is retained in fail-safe storage for an additional seven days after the time travel window. That data is available for emergency recovery through Cloud Customer Care. However, if your company doesn't discover and fix such errors within this combined timeframe, the deleted data is no longer recoverable from its last stable state. To mitigate this, we recommend that you execute regular backups for any BigQuery tables that can't be reconstructed from source data (for example, historical records or KPIs with evolving business logic). Your company could use basic scripts to back up tens of tables. However, if you need to regularly back up hundreds or thousands of tables across the organization, you need a scalable automation solution that can do the following: Handle different Google Cloud API limits. Provide a standardized framework for defining backup policies. Provide transparency and monitoring capabilities for the backup operations. Backup policies Your company might also require that the backup policies be defined by the following groups of people: Data owners, who are most familiar with the tables and can set the appropriate table-level backup policies. Data governance team, who ensure that a fallback policy is in place to cover any tables that don't have a table-level policy. The fallback policy ensures that certain datasets, projects, and folders are backed up to comply with your company's data retention regulations. In the deployment for this reference architecture, there are two ways to define the backup policies for tables, and they can be used together: Data owner configuration (decentralized): a table-level backup policy, which is manually attached to a table. The data owner defines a table-level JSON file that's stored in a common bucket. Manual policies take precedence over fallback policies when the solution determines the backup policy of a table. For details in the deployment, see Set table-level backup policies. Organization default configuration (centralized): a fallback policy, which applies only to tables that don't have manually-attached policies. A data governance team defines a central JSON file in Terraform, as part of the solution. The fallback policy offers default backup strategies on folder, project, dataset, and table levels. For details in the deployment, see Define fallback backup policies. Backup versus replication A backup process makes a copy of the table data from a certain point in time, so that it can be restored if the data is lost or corrupted. Backups can be run as a one-time occurrence or recurrently (through a scheduled query or workflow). In BigQuery, point-in-time backups can be achieved with snapshots. You can use snapshots to keep copies of the data beyond the seven-day time travel period within the same storage location as the source data. BigQuery snapshots are particularly helpful for recovering data after human errors that lead to data loss or corruption, rather than recovering from regional failures. BigQuery offers a Service Level Objective (SLO) of 99.9% to 99.99%, depending on the edition. By contrast, replication is the continuous process of copying database changes to a secondary (or replica) database in a different location. In BigQuery, cross-region replication can help provide geo-redundancy by creating read-only copies of the data in secondary Google Cloud regions, which are different from the source data region. However, BigQuery cross-region replication isn't intended for use as a disaster recovery plan for total-region outage scenarios. For resilience against regional disasters, consider using BigQuery managed disaster recovery. BigQuery cross-region replication provides a synchronized read-only copy of the data in a region that is close to the data consumers. These data copies enable collocated joins and avoid cross-regional traffic and cost. However, in cases of data corruption due to human error, replication alone can't help with recovery, because the corrupted data is automatically copied to the replica. In such cases, point-in-time backups (snapshots) are a better choice. The following table shows a summarized comparison of backup methods and replication: Method	Frequency	Storage location	Use cases	Costs Backup (Snapshots or Cloud Storage export)	One-time or recurrently	Same as the source table data	Restore original data, beyond the time travel period	Snapshots incur storage charges for data changes in the snapshot only Exports can incur standard storage charges See Cost optimization Cross-region replication	Continuously	Remote	Create a replica in another region One-time migrations between regions	Incurs charges for storing data in the replica Incurs data replication costs Design considerations This section provides guidance for you to consider when you use this reference architecture to develop a topology that meets your specific requirements for security, reliability, cost optimization, operational efficiency, and performance. Security, privacy, and compliance The deployment incorporates the following security measures in its design and implementation: The network ingress setting for Cloud Run accepts only internal traffic, to restrict access from the internet. It also allows only authenticated users and service accounts to call the services. Each Cloud Run service and Pub/Sub subscription uses a separate service account, which has only the required permissions assigned to it. This mitigates the risks associated with using one service account for the system and follows the principle of least privilege. For privacy considerations, the solution doesn't collect or process personally identifiable information (PII). However, if the source tables have exposed PII, the backups taken of those tables also include this exposed data. The owner of the source data is responsible for protecting any PII in the source tables (for example, by applying column-level security, data masking, or redaction). The backups are secure only when the source data is secured. Another approach is to make sure that projects, datasets, or buckets that hold backup data with exposed PII have the required Identity and Access Management (IAM) policies that restrict access to only authorized users. As a general-purpose solution, the reference deployment doesn't necessarily comply with a particular industry's specific requirements. Reliability This section describes features and design considerations for reliability. Failure mitigation with granularity To take backups of thousands of tables, it's likely that you might reach API limits for the underlying Google Cloud products (for example, snapshot and export operation limits for each project). However, if the backup of one table fails due to misconfiguration or other transient issues, that shouldn't affect the overall execution and ability to back up other tables. To mitigate potential failures, the reference deployment decouples the processing steps by using granular Cloud Run services and connecting them through Pub/Sub. If a table backup request fails at the final tagger service step, Pub/Sub retries only this step and it doesn't retry the entire process. Breaking down the flow into multiple Cloud Run services, instead of multiple endpoints hosted under one Cloud Run service, helps provide granular control of each service configuration. The level of configuration depends on the service's capabilities and the APIs that it communicates with. For example, the dispatcher service executes once per run, but it requires a substantial amount of time to list all the tables within the BigQuery backup scope. Therefore, the dispatcher service requires higher time-out and memory settings. However, the Cloud Run service for BigQuery snapshots executes once per table in a single run, and completes in less time than the dispatcher service. Therefore, the Cloud Run service requires a different set of configurations at the service level. Data consistency Data consistency across tables and views is crucial for maintaining a reliable backup strategy. Because data is continuously updated and modified, backups taken at different times might capture different states of your dataset. These backups in different states can lead to inconsistencies when you restore data, particularly for tables that belong to the same functional dataset. For example, restoring a sales table to a point in time that's different from its corresponding inventory table could create a mismatch in available stock. Similarly, database views that aggregate data from multiple tables can be particularly sensitive to inconsistencies. Restoring these views without ensuring that the underlying tables are in a consistent state could lead to inaccurate or misleading results. Therefore, when you design your BigQuery backup policies and frequencies, it's imperative to consider this consistency and ensure that your restored data accurately reflects the real-world state of your dataset at a given point in time. For example, in the deployment for this reference architecture, data consistency is controlled through the following two configurations in the backup policies. These configurations compute the exact table snapshot time through time travel, without necessarily backing up all tables at the same time. backup_cron: Controls the frequency with which a table is backed up. The start timestamp of a run is used as a reference point for time travel calculation for all tables that are backed up in this run. backup_time_travel_offset_days: Controls how many days in the past should be subtracted from the reference point in time (run start time), to compute the exact time travel version of the table. Automated backup restoration Although this reference architecture focuses on backup automation at scale, you can consider restoring these backups in an automated way as well. This additional automation can provide similar benefits to those of the backup automation, including improved recovery efficiency and speed, with less downtime. Because the solution keeps track of all backup parameters and results through the tagger service, you could develop a similar architecture to apply the restoration operations at scale. For example, you could create a solution based on an on-demand trigger that sends a scope of BigQuery data to a dispatcher service, which dispatches one request per table to a configurator service. The configurator service could fetch the backup history that you want for a particular table. The configurator service could then pass it on to either a BigQuery snapshot restoration service or Cloud Storage restoration service to apply the restoration operation accordingly. Lastly, a tagger service could store the results of these operations in a state store. By doing so, the automated restoration framework can benefit from the same design objectives as the backup framework detailed in this document. Cost optimization The framework of this architecture provides backup policies that set the following parameters for overall cost optimization: Backup method: The framework offers the following two backup methods: BigQuery snapshots, which incur storage costs based on updated and deleted data compared to the base table. Therefore, snapshots are more cost effective for tables that are append-only or have limited updates. BigQuery exports to Cloud Storage, which incur standard storage charges. However, for large tables that follow a truncate and load approach, it's more cost effective to back them up as exports in less expensive storage classes. Snapshot expiration: The time to live (TTL) is set for a single table snapshot, to avoid incurring storage costs for the snapshot indefinitely. Storage costs can grow over time if tables have no expiration. Operational efficiency This section describes features and considerations for operational efficiency. Granular and scalable backup policies One of the goals of this framework is operational efficiency by scaling up business output while keeping business input relatively low and manageable. For example, the output is a high number of regularly backed up tables, while the input is a small number of maintained backup policies and configurations. In addition to allowing backup policies at the table level, the framework also allows for policies at the dataset, project, folder, and global level. This means that with a few configurations at higher levels (for example, the folder or project level), hundreds or thousands of tables can be backed up regularly, at scale. Observability With an automation framework, it's critical that you understand the statuses of the processes. For example, you should be able to find the information for the following common queries: The backup policy that is used by the system for each table. The backup history and backup locations of each table. The overall status of a single run (the number of processed tables and failed tables). The fatal errors that occurred in a single run, and the components or steps of the process in which they occurred. To provide this information, the deployment writes structured logs to Cloud Logging at each execution step that uses a Cloud Run service. The logs include the input, output, and errors, along with other progress checkpoints. A log sink routes these logs to a BigQuery table. You can run a number of queries to monitor runs and get reports for common observability use cases. For more information about logs and queries in BigQuery, see View logs routed to BigQuery. Performance optimization To handle thousands of tables at each run, the solution processes backup requests in parallel. The dispatcher service lists all of the tables that are included within the BigQuery backup scope and it generates one backup request per table at each run. This enables the application to process thousands of requests and tables in parallel, not sequentially. Some of these requests might initially fail for temporary reasons such as reaching the limits of the underlying Google Cloud APIs or experiencing network issues. Until the requests are completed, Pub/Sub automatically retries the requests with the exponential backoff retry policy. If there are fatal errors such as invalid backup destinations or missing permissions, the errors are logged and the execution of that particular table request is terminated without affecting the overall run. Limits The following quotas and limits apply to this architecture. For table snapshots, the following applies for each backup operation project that you specify: One project can run up to 100 concurrent table snapshot jobs. One project can run up to 50,000 table snapshot jobs per day. One project can run up to 50 table snapshot jobs per table per day. For details, see Table snapshots. For export jobs (exports to Cloud Storage), the following applies: You can export up to 50 TiB of data per day from a project for free, by using the shared slot pool. One project can run up to 100,000 exports per day. To extend this limit, create a slot reservation. For more information about extending these limits, see Export jobs. Regarding concurrency limits, this architecture uses Pub/Sub to automatically retry requests that fail due to these limits, until they're served by the API. However, for other limits on the number of operations per project per day, these could be mitigated by either a quota-increase request, or by spreading the backup operations (snapshots or exports) across multiple projects. To spread operations across projects, configure the backup policies as described in the following deployment sections: Define fallback backup policies Configure additional backup operation projects Set table-level backup policies Deployment To deploy this architecture, see Deploy scalable BigQuery backup automation. What's next Learn more about BigQuery: BigQuery table snapshots BigQuery table exports to Cloud Storage For more reference architectures, diagrams, and best practices, explore the Cloud Architecture Center.","{ ""id"": ""root"", ""children"": [ { ""id"": ""users"", ""labels"": [ { ""text"": ""users"" } ], ""children"": [ { ""id"": ""data_owners"", ""labels"": [ { ""text"": ""Data Owners"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""gcp_cloud"", ""labels"": [ { ""text"": ""gcp_cloud"" } ], ""children"": [ { ""id"": ""services_layer"", ""labels"": [ { ""text"": ""services_layer"" } ], ""children"": [ { ""id"": ""gateway"", ""labels"": [ { ""text"": ""gateway"" } ], ""children"": [ { ""id"": ""cloud_scheduler"", ""labels"": [ { ""text"": ""Cloud Scheduler"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""dispatcher"", ""labels"": [ { ""text"": ""Dispatcher\nCloud Run"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_scheduler_trigger"", ""labels"": [ { ""text"": ""triggers"" } ] } ] }, { ""id"": ""backend_services"", ""labels"": [ { ""text"": ""backend_services"" } ], ""children"": [ { ""id"": ""configurator"", ""labels"": [ { ""text"": ""Configurator\nCloud Run"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""table_policy"", ""labels"": [ { ""text"": ""Table-level Backup Policy"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""fallback_policy"", ""labels"": [ { ""text"": ""Fallback Backup Policy"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_cfg_read_policy1"", ""labels"": [ { ""text"": ""reads"" } ] }, { ""id"": ""e_cfg_read_policy2"", ""labels"": [ { ""text"": ""reads"" } ] } ] }, { ""id"": ""messaging"", ""labels"": [ { ""text"": ""messaging"" } ], ""children"": [ { ""id"": ""pubsub_dispatch"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_backup"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_snapshot"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_export"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""pubsub_logs"", ""labels"": [ { ""text"": ""Pub/Sub"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_dispatch_route_backup"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_backup_to_snapshot"", ""labels"": [ { ""text"": ""routes"" } ] }, { ""id"": ""e_backup_to_export"", ""labels"": [ { ""text"": ""routes"" } ] } ] }, { ""id"": ""data_layer"", ""labels"": [ { ""text"": ""data_layer"" } ], ""children"": [ { ""id"": ""bigquery"", ""labels"": [ { ""text"": ""BigQuery"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""bigquery_api"", ""labels"": [ { ""text"": ""BigQuery API"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""backup_storage_bucket"", ""labels"": [ { ""text"": ""Backup Storage\nCloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""backup_metadata_storage"", ""labels"": [ { ""text"": ""Backup Metadata\nCloud Storage"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [ { ""id"": ""e_api_to_bq"", ""labels"": [ { ""text"": ""reads/writes"" } ] } ] }, { ""id"": ""backup_services"", ""labels"": [ { ""text"": ""backup_services"" } ], ""children"": [ { ""id"": ""backup_snapshot_service"", ""labels"": [ { ""text"": ""Back up as Table Snapshot\nCloud Run"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""backup_export_service"", ""labels"": [ { ""text"": ""Back up as Export to Cloud Storage\nCloud Run"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""logging_services"", ""labels"": [ { ""text"": ""logging_services"" } ], ""children"": [ { ""id"": ""log_sink"", ""labels"": [ { ""text"": ""Log Sink\nCloud Logging"" } ], ""children"": [], ""edges"": [], ""data"": {} }, { ""id"": ""tagger"", ""labels"": [ { ""text"": ""Tagger\nCloud Run"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] }, { ""id"": ""state_service"", ""labels"": [ { ""text"": ""state_service"" } ], ""children"": [ { ""id"": ""backup_state_service"", ""labels"": [ { ""text"": ""Table-level Backup State Service"" } ], ""children"": [], ""edges"": [], ""data"": {} } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_dispatch_query"", ""labels"": [ { ""text"": ""queries"" } ] }, { ""id"": ""e_dispatch_pub"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_pubsub_deliver_cfg"", ""labels"": [ { ""text"": ""delivers"" } ] }, { ""id"": ""e_cfg_publish_backup"", ""labels"": [ { ""text"": ""publishes"" } ] }, { ""id"": ""e_snapshot_trigger"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_export_trigger"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_snapshot_call_api"", ""labels"": [ { ""text"": ""snapshot"" } ] }, { ""id"": ""e_export_call_api"", ""labels"": [ { ""text"": ""export"" } ] }, { ""id"": ""e_export_store_storage"", ""labels"": [ { ""text"": ""stores"" } ] }, { ""id"": ""e_api_log_sink"", ""labels"": [ { ""text"": ""logs"" } ] }, { ""id"": ""e_log_sink_to_pubsub"", ""labels"": [ { ""text"": ""exports"" } ] }, { ""id"": ""e_pubsub_logs_to_tagger"", ""labels"": [ { ""text"": ""triggers"" } ] }, { ""id"": ""e_tagger_tag_api"", ""labels"": [ { ""text"": ""tags"" } ] }, { ""id"": ""e_api_update_state"", ""labels"": [ { ""text"": ""updates"" } ] }, { ""id"": ""e_snapshot_write_state"", ""labels"": [ { ""text"": ""writes"" } ] }, { ""id"": ""e_export_write_state"", ""labels"": [ { ""text"": ""writes"" } ] } ] } ], ""edges"": [] } ], ""edges"": [ { ""id"": ""e_define_policies"", ""labels"": [ { ""text"": ""defines"" } ] } ] }"